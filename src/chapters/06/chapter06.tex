\documentclass[class=book, crop=false, oneside, 12pt]{standalone}
\usepackage{standalone}
\usepackage{../../style}
\graphicspath{{./assets/images/}}

\begin{document}

\chapter{Analisi lessicale e analisi sintattica}

\section{Analisi lessicale}
\subsection{Introduzione}
Dopo i precedenti due capitoli, nei quali abbiamo introdotto e approfondito un buon numero di concetti, algoritmi e modelli necessari alla comprensione di come l'impostazione teorica dei linguaggi formali sia fondamentale nella progettazione dei compilatori, andiamo a tuffarci in quella che è la prima fase di compilazione: l'analisi lessicale.


Ricordiamo che questa è la fase in cui vogliamo identificare quali parti del sorgente che abbiamo scritto corrispondono alle keyword, quali agli identificatori, alle costanti e via di questo passo; per essere più formali, questi elementi che vogliamo riconoscere portano il nome di \emph{lessemi}. 
Il nostro obiettivo in questa fase è trasformare quindi il sorgente in un flusso di tokens, i quali costituiscono i terminali della grammatica che genera il nostro linguaggio di programmazione.

Ricordiamo per l'ennesima volta che la grammatica di un linguaggio ci dirà quali sono le forme che un'espressione deve avere per essere considerata ben formata rispetto a quel linguaggio. Ad esempio, una grammatica ci può dire che la seguente forma denota un'espressione valida:\\
% begin minted
Identificatore      simbolo di assegnamento    numero\\
% end minted
e che quindi espressioni come la seguente sono grammaticali e ben formate secondo il linguaggio.\\
% begin minted
pippo = 2\\
% end minted8
Il mestiere dell'analizzatore lessicale è proprio quello di ricevere in input un \texttt{pippo} qualsiasi (che è un token) e, in output, determinare che è un \emph{identificatore}, ossia la categoria più astratta (lessema) di cui \texttt{pippo} è istanza.

% L’analizzatore lessicale lavora in tandem (pipeline) con l’analizzatore sintattico per non si sa bene come o perché
\subsection{Esempio: la grammatica di C99}
Andiamo a vedere da vicino la grammatica di un reale linguaggio, anzi, del linguaggio preferito di tutti noi, ossia il C (nella versione \(99\), scritta per il parser \emph{Bison}); il lettore interessato ad approfondire può trovare lo stesso file cliccando \url{http://www.quut.com/c/ANSI-C-grammar-y-1999.html}{qui}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth,keepaspectratio]{c99-ex1}
    \caption{}
    % \label{...}
\end{figure}
Notiamo subito alcune differenze rispetto alla notazione che abbiamo impiegato finora: 
\begin{itemize}
    \item la freccia ( \(\to\) ) è rappresentata da un altro simbolo, i due punti ( \(:\) );
    \item il pipe ( \(\mid\) ), invece, possiede l'abituale significato;
    \item i terminali possono essere indicati precisamente se inseriti tra singoli apici '\emph{terminale}';
    \item inoltre, la convenzione rispetto alla capitalizzazione è invertita: qui osserviamo che gli elementi in maiuscolo indicano i terminali, mentre invece quelli in minuscolo rappresentano non terminali; ad esempio, in figura sopra possiamo notare che il non-letterale \texttt{primary\_expression} ha una produzione per cui può risultare o in una serie di letterali (\texttt{IDENTIFIER}, \texttt{CONSTANT}), oppure in una forma \texttt{'(' expression ')'}, dove \texttt{expression} è un altro non-letterale, che a sua volta avrà altre produzioni.
\end{itemize}

Questo file, inoltre, è pensato per essere utilizzato in tandem con un analizzatore sintattico; per questo motivo, nell'intestazione dello stesso possiamo trovare le dichiarazione di quelli che sono i token (vale a dire, lo ripetiamo, i terminali della grammatica descritta). Saranno espressi nella forma rappresentata in Fig.\ref{token_c99}:
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{c99-ex2}
    \caption{}
    \label{token_c99}
\end{figure}

Un'altra cosa che possiamo osservare è la dichiarazione di quello che è lo starting symbol della grammatica:\\
% begin minted
\% start translation\_unit\\
% end minted
Questa dichiarazione in realtà potrebbe non essere presente su alcuni file contenenti grammatiche, dal momento che, in sua assenza, non otteniamo degli errori, bensì viene preso il non-letterale della prima produzione listata a seguito come starting symbol.

Il ruolo dell’analizzatore lessicale è quindi analizzare il sorgente e decidere, di volta in volta, quale derivazione può essere applicata a ciascuna delle righe del sorgente analizzato. Una volta completato l'albero di derivazione e quindi arrivato presso un terminale (ad esempio \texttt{IDENTIFIER}), l'analizzatore lessicale andrà anche ad associargli informazioni come valre e tipo, dal momento che è necessario distinguere quell'\texttt{IDENTIFIER} da un altro. Nel programma potremmo appunto avere due \texttt{IDENTIFIER} di diverso tipo, ma in ogni caso dovremo conservare delle informazioni aggiuntive di qualche tipo (nome, tipo, scope e altro ancora). Tutte queste informazioni vengono conservate in una \emph{symbol table}.

Infine, le ultime rihe del file contengono informazioni utili al particolare analizzatore sintattico utilizzato; avremo modo di parlarne nel dettaglio in futuro.

\subsection{Classi di tokens}
Si potrebbe a ragione considerare superfluo specificare che ogni diversa grammatica (e quindi ogni linguaggio) presenta diverse categorie di tokens; banalmente, il lettore è probabilmente ben cosciente che linguaggi diversi possiedono generalmente keyword diverse. Ad ogni modo, a seguito presentiamo alcune tra le scelte più ricorrenti:
\begin{itemize}
    \item un token per ogni keyword, quindi un token per ogni nome di base già presente nel linguaggio (\texttt{if}, \texttt{while}, \texttt{for} e via dicendo);
    \item un token per ogni operatore (o anche per classe di operatori), quindi in C ne avremo uno per \texttt{+} ma anche uno dedicato per \texttt{++}; 
    \item un unico token per gli identificatori, valido per tutti quanti;
    \item un token per ogni simbolo di punteggiatura. 
\end{itemize}

\subsection{Il compito dell'analizzatore lessicale}
L'obiettivo dell'analalizzatore lessicale è quindi riconoscere i cosiddetti \emph{lessemi}, ossia quelle parti del programma che corrispondono ai token, e ritornarli. Ciascuno di questi, di solito, viene ritornato sotto forma di coppia \texttt{<token-name>: <token-value>}, dove:
\begin{itemize}
    \item \texttt{<token-name>} è il nome scelto per denotare quel preciso token; seguendo l'esempio della grammatica precedente, \texttt{IDENTIFIER} è un \texttt{<token-name>};
    \item \texttt{<token-value>} è tipicamente un puntatore a una entry della symbol table, in cui si va a salvare tutte le informazioni relative a quel preciso token di tipo\footnote{In questo caso la parola "tipo" è chiaramente impropria, ma è molto utile per rendere la natura astratta di classe di token.} \texttt{<token-name>}.
\end{itemize}

\subsection{Lessemi e espressioni regolari}
Andiamo quindi a capire in che modo la teoria studiata nei due capitoli precedenti entra prepotentemente nell'analisi lessicale.

I lessemi sono estremamente facili da descrivere utilizzando le espressioni regolari: ad esempio, in un linguaggio che prevede un lessema identificatore, il quale è costituito di qualsiasi combinazione di lettere maiuscole e minuscole, quest'ultimo può essere facilmente denotato da un'espressione regolare del tipo:
\begin{equation*}
    (a \mid b \mid \ldots \mid z \mid A \mid B \mid \ldots \mid Z)^*
\end{equation*}
Questo è molto interessante: se è vero che i lessemi sono perfettamente descrivibili con dei linguaggi regolari, allora vuol dire che possiamo utilizzare quelli in sede di analisi lessicale, senza dover tirare in ballo i più potenti ma decisamente più complessi linguaggi liberi.

Quindi questi lessemi, essendo descritti da delle espressioni regolari, possono anche essere riconosciuti agevolmente da una macchina a stati, come ad esempio quella in figura sotto, che descrive la classe \texttt{relop} di token per gli operatori relazionali.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{lec-14-1}
    \caption{}
    % \label{dfa-b_odd-and-a_even}
\end{figure}
È molto semplice intuire graficamente che ciascun percorso che termini in uno stato finale ritorna un token, sotto forma di coppia, in cui il \texttt{token-name} è appunto quello della classe \texttt{relop}, mentre il \texttt{token-value} indica qual è l'elemento della classe da ritornare.

\paragraph{Retract}
Possiamo notare che i due stati finali il cui arco entrante è marcato come \(other\) hanno anche un asterisco: questo indica che, quando consumo quella transizione, devo ricordarmi di tornare indietro di un simbolo, perché quest'ultimo simbolo \(other\) (cioè che non fa parti di alcun elemento appartenente alla classe \texttt{relop}) potrebbe essere un elemento di un token successivo, e se non compiano quanto detto sopra rischieremmo di saltarla nell'analisi e ottenere un flusso di token incorretto. Questa operazione è detta \(retract\).

L'operazione di retract è uno dei tanti elementi legati alla gestione dell'input e del buffer che ci fanno pensare che la macchina a stati è un modello molto vicino al più noto, per noi, automa a stati finiti, ma quanto è profondo questo legame?

\subsection{Pattern matching basato su NFA}
Immaginiamo di avere delle espressioni regolari che denotano il linguaggio dei lessemi che siamo interessati a riconoscere. Si pensi ad esempio all'espressione regolare che denota il lessema \texttt{IDENTIFIER}, ossia l'espressione regolare che denota tutte le possibili combinazioni di lettere maiuscole e minuscole (con le dovute peculiarità di ciascun linguaggio); oppure, si pensi a un'espressione regolare che denoti il lessema degli operatori relazionali, o ancora un'altra che denoti la scrittura dei floating numbers. In sostanza, per ciascuna categoria sintattica abbiamo un'espressione regolare che la denota.

Sappiamo anche che, avendo delle espressioni regolari, per ciascuna di queste possiamo costruire un NFA che riconosce il linguaggio denotato dall'espressione considerata; possiamo a questo punto immaginare di far collidere questi NFA inserendo uno stato iniziale extra e collegandolo a ciascun NFA tramite una \(\varepsilon\)-transizione, come mostrato in Fig.\ref{nfa_for_grammar_regular_expressions} sotto.
Da notare che nel caso d'uso del compilatore ci sono delle azioni associate agli stati finali (se l'analizzatore legge un assegnamento compie, appunto, le azioni per registrare l'assegnamento).
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth,keepaspectratio]{lec-14-2}
    \caption{}
    \label{nfa_for_grammar_regular_expressions}
\end{figure}
Una struttura di questo tipo può riconoscere i linguaggi denotati da tutte le espressioni regolari per cui abbiamo costruito degli NFA. 

Adesso dobbiamo pensare come ottimizzare l'uso di una struttura simile per l'analisi lessicale. Potremmo trovarci infatti a gestire problemi di diversa natura, in particolare relativi all'input buffering: ad esempio, come posso fare a sapere se i due caratteri \texttt{i} e \texttt{f} che ho appena letto stanno a indicare la keyword \texttt{if} e non un ipotetico identificatore \texttt{iffoff}? Detto in maniera informale, come decido quando fermarmi, quando tornare indietro, quando e se ho letto più di quanto mi serviva?

Il procedimento che seguiamo è il seguente:
\begin{enumerate}
    \item innanzitutto simuliamo l'NFA creato come sopra descritto;
    \item nel caso di ambiguità, ossia di situazioni come quella descritta sopra tra \texttt{if} e \texttt{iffoff}, la convenzione è di preoseguire la simulazione finché nessun’altra transizione è possibile, solitamente quando incontriamo uno spazio o un \texttt{\(\backslash\)n}, privilegiando quindi il match più lungo (si parla di cercare il \emph{longest match});
    \item se nell'insieme di stati che abbiamo raggiunto ci sono delle azioni disponibili, allora andremo ad eseguire quella appartenente al longest match, in caso di pareggio ciascuna azione avrà una priorità, e noi eseguiremo quella con più alta priorità;
    \item se nessun'azione è disponibile, invece dobbiamo torniamo indietro nella sequenza di stati percorsi, fermarci nel primo insieme di stati che presenta almeno uno stato finale e delle azioni associate e cercare di nuovo quella prioritaria; per ognuno di questi passi all'indietro, dobbiamo ricordari di aggiornare il puntatore all'input buffer.
\end{enumerate}

Tutto questo funzionerebbe in maniera analoga se utilizzassimo un DFA: dovremmo semplicemente costruire il DFA a partire dal NFA, dimodoché l'insieme degli stati del DFA sia un sottoinsieme di quello dell'NFA corrispondente.

\subsection{Generatori di analizzatori lessicali}
Andiamo a parlare quindi di questi generatori. L'idea di creare un generatore di analizzatori lessicali è nata in concomitanza con la prima definizione e implementazione del compilatore del C, e l'idea era di evitare di dover scrivere ogni volta, per ogni programma, un analizzatore lessicale e uno sintattico a mano.

\emph{Flex} è il primo della sua storia\footnote{o meglio, una sua versione più moderna: il primo vero e proprio si chiamava \emph{lex} e quella \emph{f}, aggiunta successivamente, sta per fast.} ed è solitamente compreso nelle distribuzioni di C; l'idea è risultata talmente valida che oggiggiorno ogni linguaggio possiede un proprio generatore di analizzatore lessiale e sintattico.

Il funzionamento è il seguente: 
\begin{itemize}
    \item andremo a creare un file del tipo \texttt{file.l}, il quale sarà l'input del generatore e in cui scriveremo quali sono i pattern che vogliamo riconoscere e quali sono le azioni da compiere in corrispondenza dei vari matches;
    \item a quel punto, possiamo comiplare \texttt{file.l} utilizzando il comando \texttt{Flex}, e questo ci restituirà un file di nome \texttt{lex.yy.c}\footnote{La ragione di questo nome è puramente convenzionale e deriva dal fatto che, storicamente, il generatore Flex è stato usato in coppia all'analizzatore \emph{Yacc}};
    \item compilando questo \texttt{lex.yy.c} con il compilatore \texttt{gcc} otteniamo il lexer, cioè quel signore che si occupa di gestire l'input buffering, di fare le operazioni di retract e altro ancora.
\end{itemize} 
% begin minted
Flex file.l
gcc lex.yy.c -lfl
./a
% end minted
Si tenga bene a mente che queste tre righe descrivono come utilizzare Flex da solo; tuttavia, quest'ultima è un'eventualità piuttosto rara, dal momento che Flex è nato per essere usato in pipeline con un generatore di analizzatore di analisi sintattica, ma poiché questi sono elementi ancora sconosciuti per noi, al momento non ce ne preoccupiamo.

\subsection{Struttura del \texttt{file.l}}
I file con estensione \texttt{.l} che diamo da fagocitare a Flex hanno la seguente struttura:
% begin minted
...(Preambolo)
% { code
% } %
shorthand for patterns
%%
(Parte centrale)
pattern-1 {action-1};
pattern-2 {action-2};
...
%%
(Epilogo)
user rountes
% end minted
Il contenuto di queste tre macrosezioni è ben determinato, andiamo a vederlo più da vicino:
\begin{labeling}{Parte centrale}
    \item[Preambolo] qui ci andrà del codice C,in particolare le inizializzazioni di variabili, o delle abbreviazioni per indicare dei particolari pattern nelle espressioni regolari che andremo a utilizzare sotto;
    \item[Parte centrale] è il cuore del file, e si tratta di una lista pattern-azione, dove pattern è un'espressione regolare, e l'azione sarà ciò che dobbiamo compiere qualora dovessimo riconoscere il pattern corrispondente;
    \item[Epilogo] questa sezione è in realtà facoltativa, il lexer funziona anche senza che vi sia scritto alcunché; tuttavia, quello che potremmo eventualmente trovare e/o scrivere sono delle routine definite dall'utente, le quali verranno copiate e incollate dentro al genituro \texttt{lex.yy.c}.
\end{labeling}

\subsection{Linguaggio delle espressioni regolari in Flex}
Dal momento che tutte le coppie \texttt{pattern \{azione\}} sono scritte in linguaggio \texttt{lex}, a differenza di altre parti di \texttt{file.l}, è opportuno conoscere questo linguaggio. Di seguito presentiamo i \emph{metacaratteri} di Flex, ossia dei caratteri riservati:
\begin{equation*}
    \slash \quad \backslash \quad - \quad * \quad + \quad >\quad " \quad \{ \quad \} \quad . \quad \$ \quad ( \quad ) \quad \mid \quad \% \quad [ \quad ] \quad ^\wedge
\end{equation*}
\noindent Andiamo a vedere più da vicino quali sono le regole di matching dei metacaratteri:
\begin{labeling}{a | b}
    \item[\texttt{.}] qualsiasi carattere, eccetto il newline;
    \item[\texttt{\(\backslash\)n}] il newline;
    \item[\texttt{*}] zero o più copie di un elemento;
    \item[\texttt{+}] una o più copie di un elemento;
    \item[\texttt{?}] zero o una copia di un elemento;
    \item[\texttt{[]}] denota le classi di caratteri: al posto di \(a \mid b \mid \ldots \mid z\), posso scrivere \texttt{[a-z]};
    \item[\texttt{\(^\wedge\)}] inizio di riga, negazione se usato in una classe di caratteri;
    \item[\texttt{\$}] end of line;
    \item[\texttt{a|b}] pipe, semantica consueta;
    \item[\texttt{()}] raggruppamenti;
    \item[\texttt{"+"}] somma letterale di due elementi
    \item[\texttt{\{\}}] espressioni regolari scritte nel preambolo
\end{labeling}

\subsection{Esempi di file per Flex}
Facciamo un breve riepilogo di quanto abbiamo visto: tutti i file predisposti per Flex si compongono da tre sezioni, ciascuna separata da coppie di \%. Nel preambolo si inseriscono delle definizioni come le \texttt{define} che servono e i pattern (espressioni regolari per i lessemi che vogliamo inserire). I pattern sono inseriti insieme all'azione nella parte centrale. In fondo si inseriscono le routine dell'utente che vengono copiate nel file \texttt{lex.yy.c}: un testo minimale è l'invocazione della routine \texttt{yylex()} che se non viene inserita viene chiamata in automatico. I comandi, qualora non si usi Flex con Bison (analizzatore sintattico in pipeline con Flex), sono i seguenti:
\begin{itemize}
    \item Flex file.l
    \item gcc lex.yy.c lfl
    \item ./a
\end{itemize}

\subsubsection{file0.l}
        
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file0.l.png}
    \caption{file0.l}
    \label{file0.l}
\end{figure}

In qusto primo esempio vediamo che nel preambolo non sono presenti né delle \texttt{define} né del codice; dal momento che non sono presenti routine inserite dall'utente, verrà invocata la procedura \texttt{yylex()} all'esecuzione. Basandoci sulla sintassi di Flex vista precedentemente, andiamo ad analizzare la parte centrale costituita da una sola coppia pattern-azione:
\begin{itemize}
    \item il pattern è dato dalla regular expression composta solo da \texttt{.}, il che significa che l'azione seguente farà riferimento a tutti i caratteri incontrati, tranne quello di \texttt{\(\backslash\)n} (\emph{new line});
    \item l'azione è quel \texttt{printf("hello world!")}, il che consiste banalmente nello stampare la stringa hello world ogni volta che viene eseguito un match sul pattern.
\end{itemize}
In sostanza, questo file ci farà sì che, nella lettura, qualsiasi carattere incontriamo, ad eccezione del new line, verrà sostituito da un'occorrenza della stringa \texttt{"hello world!"}; quindi, ad esempio:
\begin{itemize}
    \item \(a\) diventerà \texttt{hello world!};
    \item \(aa\) diventerà \texttt{hello world!hello world!}.
\end{itemize}

Possiamo notare anche un'altra cosa molto interessante: solitamente, per poter utilizzare lo standard output nei programmi C, è necessario includere all'interno del file la libreria stdout \texttt{stdio.h}, ma nel \texttt{file0.l} non vi è nemmeno l'ombra di un inlcude: è lo stesso Flex a occuparsi di aggiungere in un secondo momento tutti i dettagli necessari per la gestione dell'output.

\subsubsection{file1.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file1.l.png}
    \caption{file1.l}
    \label{file1.l}
\end{figure}


\paragraph{Preambolo}
Diversamente dall'esempio precedente, in questo \texttt{file1.l} abbiamo del contenuto nel preambolo. Andiamo a vederlo da vicino cosa ci troviamo:

\begin{enumerate}
    \item la prima riga (in questo caso vuota) è identificata dal simbolo di apertura \texttt{\%\{} e cui può seguire del codice C;
    \item la seconda riga, invece, è successiva al simbolo di chiusura \texttt{\%\}} e contiene quelli che sono gli shorthands per i pattern, vale a dire degli alias per delle regular expression.
\end{enumerate}

Rifacendoci sempre alla sintassi di Flex, possiamo analizzare l'espressione regolare definita nel preambolo e arrivare alla conclusione che faccia riferimento a quell'insieme di caratteri (\texttt{[ ]}) che si ripetono \(0\) o più volte (*) e che sono diversi (\^{}) da '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}'; questo significa che comporteranno un match tutte quelle sequenze di caratteri che sono separate da spazi, tab o new line; più semplicemente, saranno le parolea causare un match.

\paragraph{Parte centrale}
Come descritto precedentemente, nella parte centrale i pattern sono associati alle relative azioni; in questo caso, le coppie che troviamo sono le seguenti:
\begin{itemize}
    \item \texttt{non\_white} comporta l'esecuzione della macro \texttt{echo} di Flex, che è equivalente a \texttt{printf("\%s", yytext)} in linguaggio C, e stamperà a video esattamente la sequenza di caratteri che ha causato il match;
    \item un qualunque carattere diverso da \texttt{\textbackslash n (.)} verrà eliminato;
    \item new line (\texttt{\textbackslash n}) verrà anch'esso eliminato
\end{itemize}

Complessivamente, il risultato ottenuto dall'esecuzione di un programma compilato a partire dal file discusso porterà all'eliminazione di '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}' dal testo passato in input e ci farà ottenere ad una sequenza continua di caratteri (ad esempio, una stringa del tipo "\texttt{ ab c d e \hspace{.8cm} f g}" diventerà "\texttt{abcdefg}").

\subsubsection{file2.l}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file2.l.png}
    \caption{file2.l}
    \label{file2.l}
\end{figure}

\paragraph{Preambolo}
In questo esempio, invece, troviamo finalmente del codice C nel preambolo; in particolare, troviamo la dichiarazione di un intero chiamato \texttt{lineno} ed inizializzato al valore \(1\). 

\paragraph{Parte centrale}
A seguire abbiamo la parte centrale del file, dove: 

\begin{itemize}
    \item all'espressione regolare \texttt{\^{}.*\textbackslash n}, che identifica tutte quelle sequenze di caratteri che iniziano per un carattere differente da new line ripetuto zero o più volte e finiscono per new line o, più semplicemente, le parole sulla stessa riga
    \item  viene associata l'azione \texttt{printf("\%4d)\textbackslash t\%s", lineno++, yytext)}, che stampa il valore dell'intero lineno seguito dal simbolo ) e dal contenuto del buffer yytext (che contiene l'input fino ad ora riconosciuto) ed infine esegue il postincremento di lineno.
\end{itemize}

\noindent Quindi, il nostro programma si occuperà di stampare tutte le parole sulla stessa riga anteponendo il numero della riga a cui fa riferimento; in altre parole, se avessimo un testo come il seguente:
\begin{itemize}[noitemsep]
    \item[] \emph{Autem quo ea. Voluptatum saepe porro. Quibusdam illo eum.}
    \item[] \emph{Quia aperiam nesciunt. Qui est voluptate. Aut temporibus perspiciatis.}
    \item[] \emph{Repudiandae delectus omnis. Modi earum doloribus. Quis eaque quidem.}
\end{itemize}
Allora avremmo che verrebbe riscritto come segue:
\begin{enumerate}[noitemsep]
    \item[\texttt{1}] \emph{Autem quo ea. Voluptatum saepe porro. Quibusdam illo eum.}
    \item[\texttt{2}] \emph{Quia aperiam nesciunt. Qui est voluptate. Aut temporibus perspiciatis.}
    \item[\texttt{3}] \emph{Repudiandae delectus omnis. Modi earum doloribus. Quis eaque quidem.}
\end{enumerate}

\subsubsection{file3.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file3.l.jpg}
    \caption{file3.l}
    \label{file3.l}
\end{figure}
In questo quarto esercizio andiamo a combinare quanto visto precedentemente, aggiungendo anche una routine custom scrivendo del codice ad hoc nell'epilogo del file.

\paragraph{Preambolo}
Nel preambolo abbiamo la dichiarazione di tre variabili che ci serviranno nelle routine come contatori; inoltre, abbiamo anche una shorthand, chiamata \texttt{word}, la cui espressione regolare include tutte quelle sequenze di uno o più caratteri (\texttt{+}) che non contengono '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}.

\paragraph{Parte centrale}
Nella parte centrale del file abbiamo invece le seguenti associazioni pattern-azione:

\begin{itemize}
    \item \texttt{word} comporta l'esecuzione del seguente pezzo di codice \texttt{\{wordCount++; charCount += yyleng; \}}, dove la variabile \texttt{wordCount} viene postincrementata, mentre alla variabile \texttt{charCount} viene sommato un numero pari a \texttt{yyleng}, che contiene la lunghezza della stringa riconosciuta e, per via dell'espressione regolare alla base della shorthand, equivalente a quella della parola;
    \item il secondo pattern è \texttt{\textbackslash n}, cioè nel caso in cui si trovi una sequenza di caratteri composta solamente da una new line; l'azione legata è il blocco \texttt{\{charCount++; lineCount++; \}}, dove si postincrementano sia \texttt{charCount} che \texttt{lineCount};
    \item infine, l'ultimo pattern, il punto (\texttt{.}), comporta solamente il postincremento della variabile \texttt{charCount}.
\end{itemize}

In sostanza, il programma che stiamo descrivendo con questi pattern si occupa di contare complessivamente quanti caratteri, parole e righe sono state individuate in un dato testo; qualora avessimo un match con una parola, allora avviene un incremento proporzionale al numero di caratteri che costituiscono quella stessa parola e, inoltre, si aggiorna il numero di parole totali che sono contenute nel testo; ogni volta che si trova una new line si incrementano di un'unità il numero di righe e di caratteri e, infine, ogni volta che si trova uno spazio o un tab viene incrementato solamente il numero di caratteri. 

\subsubsection{file4.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file4.l.jpg}
    \caption{file4.l}
    \label{file4.l}
\end{figure}
L'ultimo esempio proposto fa uso di quanto discusso fino ad ora per analizzare un \texttt{csv} e restituire i dati sotto forma di una tabella in formato \texttt{html}.

\paragraph{Preambolo}
Nel preambolo viene dichiarata una variabile intera \texttt{comma}, che è inizializzata a \(0\); servirà per controllare le entry vuote nel \texttt{csv} passato in input date da virgole consecutive.

\paragraph{Parte centrale}
Andiamo a vedere da vicino le coppie pattern-azione che troviamo nella parte centrale.
\begin{itemize}
    \item La regular expression \texttt{[a-zA-Z0-9]+}, che indica la sequenza di uno o più caratteri alfanumerici, comporta l'esecuzione di 
    
    \{printf("\(<\)td\(>\) "); ECHO; comma = 0;\} 
        
    dove viene creata una nuova colonna per la tabella, viene inserita la sequenza appena letta e viene impostato il valore dell'intero \texttt{comma} a \(0\).
    \item La seconda espressione è "\texttt{,}", che vuol dire che bisogna trovare il carattere che corrisponde alla virgola e che nel \texttt{csv} serve per separare i dati, comporta l'esecuzione del blocco di codice
        % \begin{minted}[linenos, breaklines, tabsize = 2]{c}
        % {            
        % if (comma) {
        %     printf("<td> </td>"); 
        % } else { 
        %     printf("</td>");
        % }       
        % comma = 1;
        % }
        % \end{minted}
        
    dove viene verificato il numero di virgole incontrate fino a quel punto: se si sono incontrate 0 virgole, allora si chiude semplicemente la colonna precedentemente aperta; se invece si è già incontrata \(1\) virgola e, consecutivamente, se ne legge un'altra (o comunque le due occorrenze risultano essere separate da sequenze non alfanumeriche), viene creata una colonna vuota, in quanto nel \texttt{csv} non vi sono dati rilevanti. In ogni caso alla fine del blocco viene impostato il valore di comma ad \(1\).
    \item In ultimo, \texttt{\textbackslash n}, cioè per ogni new line viene eseguito il blocco di codice \{printf("\(<\)/tr\(>\) \textbackslash n \(<\)tr\(>\)"); comma = 0;\} dove viene chiusa la riga precedente ed aperta quella successiva.
\end{itemize}

\paragraph{Epilogo}
Nell'epilogo troviamo una routine custom dove viene creato l'html di base per creare la tabella e, tra le varie cose, viene anche aperta la prima riga. Dopo aver stampato lo scheletro della struttura viene invocata la procedura \texttt{yylex()}, la quale cerca i pattern nel testo; infine, la tabella viene chiusa.

\subsection{Ulteriori informazioni su Flex}
Quando abbiamo discusso del funzionamento dell'analisi lessicale basata sull'utilizzo di NFA o DFA abbiamo anche parlato del fatto che, quando si tenta di riconoscere il pattern di una determinata parola, è possibile che avvengano dei match su più stati finali dell'automa e che quindi si possa avere il dubbio su quale azione eseguire: come si riflette tutto ciò in Flex? 

Per risolvere a tale problematica si utilizza la regola del \emph{longest match}: dalla lista di pattern si va a recuperare quello più lungo che ha portato ad un match.

Cosa succede invece se ci sono più pattern che a parità di lunghezza comportano un match (ovvero i due pattern hanno la stessa lunghezza)? In questo caso si sceglie sempre il primo della lista. Da qui si intuisce dunque che anche l'ordine in cui si scrive la sequenza dei pattern ha una sua importanza: un medesimo \texttt{file.l} potrebbe non generare lo stesso risultato se si eseguisse uno shuffle dei pattern.

\section{Parsing}
Il parsing (o analisi sintattica) è quel processo in cui, data una grammatica \(\mathcal{G} = (V, T, S, \mathcal{P})\) e una parola \(w\), dobbiamo dire se \(w \in \mathcal{L(G)}\) e, se così fosse, fornire il suo albero di derivazione. Solitamente gli approcci al parsing che vengono presi in considerazione nell'ambito dei linguaggi di programmazione sono due: 
\begin{itemize}
    \item \textbf{Top-Down}: consiste nella costruzione di una derivazione leftmost da uno start symbol della grammatica e quindi procede dalla radice verso le foglie dell'albero di derivazione; a prima vista, si direbbe che sia l'approccio più intuitivo;
    \item \textbf{Bottom-Up}: consiste invece nella costruzione di una derivazione rightmost (in ordine inverso) della stringa dalle foglie alla radice.
\end{itemize}
A questo punto è necessario aggiungere che il parsing non si limita a questi due approcci, ma esiste anche in forma più generale utilizzando delle tattiche che vengono impiegate nel caso dei linguaggi naturali. Gli approcci descritti non permettono di considerare tutti i possibili linguaggi liberi, ma solamente delle sottoclassi: per questo si ha un'analisi sintattica estremamente efficiente dal punto di vista computazionale.

\subsection{Top-Down Parsing}
\subsubsection{Esempio 1}
Sia \(w\) = \(bd\) e sia \(\mathcal{G}\): 
\begin{align*}
    \mathcal{G}: S &\rightarrow Ad \mid Bd \\
    A &\rightarrow a \\
    B &\rightarrow b
\end{align*}

Per verificare se \(w \in \mathcal{L(G)}\) con un approccio top-down dobbiamo ottenere una derivazione leftmost a partire dallo starting symbol. Ovviamente, si noterà subito che non è possibile scegliere \(Ad\) come derivazione iniziale dello starting symbol, perché a quel punto l'unica parola ottenibile sarebbe parola \(ad\); dobbiamo invece optare per la seconda derivazione. La derivazione completa ci porta a
\begin{align*}
    S \Rightarrow Bd \Rightarrow bd
\end{align*}
Visto che abbiamo dimostrato che \(w \in \mathcal{L(G)}\) e che tale derivazione esiste, allora possiamo fornire il suo derivation tree che, per questo esempio, risulta davvero molto semplice.

\begin{figure}[H]
    \centering
    \includegraphics[width=.15\textwidth,keepaspectratio]{par-td-es1.png}
    \caption{Albero di derivazione per esercizio 1}
    \label{par-td-es1}
\end{figure}

\subsubsection{Esempio 2}
Sia \(w\) = \(id + id * id\) e sia \(\mathcal{G}\):
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon \\
    F &\rightarrow (E) \mid id
\end{align*}

Qui non è così intuitivo riuscire a dire se esiste una derivazione leftmost: che cosa mi conviene espandere? Avremo modo di parlare meglio di questo esempio non appena introdurremo il \textbf{Predictive top-down parsing}.

\subsubsection{Esempio 3}
Sia \(w\) = \(cad\) e sia \(\mathcal{G}\):
\begin{align*}
    S &\rightarrow cAd \\
    A &\rightarrow ab \mid a
\end{align*}

Nonostante questo esempio sia molto intuitivo, dobbiamo sforzarci di ragionare nei panni dell'algoritmo di parsing: dopo la prima derivazione, infatti, entrambe le opzioni che vengono proposte per poter derivare il non-terminale \(A\) sono apparentemente valide in quanto iniziano entrambe per \(a\). Quale delle due dovrei dunque scegliere? Quanto devo continuare l'esecuzione prima di accorgermi se ho fatto una scelta corretta oppure no? Il nostro algoritmo potrebbe anche trovarsi nel caso di dover fare \emph{backtrack}, perché semplicemente non c'era un modo semplice e generale per capire cosa dover scegliere: ovviamente questo tipo di tecnica funziona, ma vogliamo un approccio efficiente e il backtrack, come sappiamo, in questo non ci aiuta.

\subsection{Predictive Top-Down Parsing}
Nel caso del Predictive top-down parsing non è mai necessario applicare la tecnica del backtrack, poiché questo fa riferimento a una classe particolare di grammatiche, le quali vengono definite \textbf{LL(1) grammars}; vengono così chiamate per via della procedura impiegata per analizzarle, in cui:
\begin{itemize}
    \item guardiamo le parole da sinistra a destra;
    \item eseguiamo una produzione leftmost;
    \item guardiamo un solo simbolo (non-terminale).
\end{itemize}
Tali grammatiche prevedono una tipologia di parsing per cui non è necessario backtrack, e per di più è \textbf{completamente deterministica}.

Queste grammatiche vengono classificate a seconda del grado di determinismo che consentono; all'inizio del corso abbiamo visto la differenza tra le grammatiche senza contesto e contestuali, ma adesso le classi che incontreremo da oggi in poi si differenzieranno esclusivamente per il tipo di analizzatore con cui possiamo riconoscere le parole generate da queste grammatiche. 

In questo caso il parsing si basa sul fatto che possiamo costruire una tabella di parsing che ci guida nell'analisi della parola che ci viene data in input; quessta strategia ci permette molto efficacemente di dire se la parola appartenga oppure no al linguaggio e, in caso di esito positivo, di costruire la derivazione leftmost richiesta e il conseguente albero di derivazione.

Prendiamo come esempio il caso che abbiamo lasciato in sospeso precedentemente:
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon \\
    F &\rightarrow (E) \mid id
\end{align*}
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{ptdp-example.tex}
    \caption{Tabella del parsing top-down}
    \label{ptdp-example}
\end{table} 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.7\textwidth,keepaspectratio]{parsingTopDownTable.jpg}
%     \caption{parsingTopDownTable}
%     \label{parsingTopDownTable}
% \end{figure}

La tabella è così costruita:
\begin{itemize}
    \item si ha una \textbf{riga} per ogni non-terminale della grammatica;
    \item ed una \textbf{colonna} per ogni terminale della grammatica, a cui si aggiunge il simbolo \$ alla parola fornita in input e utilizzato come terminatore
    \item le entry vuote all'interno della tabella identificano i casi di errore.
\end{itemize}

Avendo a disposizione la tabella di parsing, la cui costruzione verrà trattata successivamente, è possibile utilizzarla per il parsing top-down predittivo.

\subsubsection{Algoritmo per il Predictive Top-Down Parsing}
\begin{itemize}
    \item Input: Una stringa \(w\), una tabella \(M\) di parsing top-down per la grammatica \(\mathcal{G} = (V, T, S, \mathcal{P})\);
    \item Output: La derivazione leftmost della stringa \(w \iff w \in \mathcal{L(G)}\), altrimenti \emph{error()}.
\end{itemize}
Inizializziamo la procedura posizionando \(w\$\) nell'input buffer e, nella pila che viene utilizzata per inserire e analizzare gli elementi parziali, inseriamo il simbolo \$ con lo starting symbol \(S\) in cima.
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{PredictiveTopDownAlgorithm.png}
    \caption{PredictiveTopDownAlgorithm}
    \label{PredictiveTopDownAlgorithm}
\end{figure}

Nell'algoritmo di parsing si utilizza la variabile \(b\) come primo simbolo delle parola w\$ e si inizializza la variabile X con la cima dello stack (il caso base corrisponde ad avere \(X = S\)). Finché \(X \neq \$\) (sostanzialmente finché non ho svuotato completamente la pila), sono dati i seguenti casi:

\begin{enumerate}
    \item se \(X = b\), allora tolgo l'elemento dalla cima della pila e imposto \(b\) al simbolo successivo nell'input buffer. Questo corrisponde al caso in cui nella derivazione della costruzione parziale ho ottenuto un match con un terminale nella parola;
    \item se invece X è comunque un terminale, allora deve essere che \(X \neq b\), ma ciò produce un errore;
    \item se invece X è un non-terminale, allora è necessario verificare la tabella di top-down parsing in posizione \(M[X, b]\) e possono valere le seguenti:
    \begin{itemize}
        \item \(M[X, b]\) = \texttt{error} e allora viene restituito \texttt{error()};
        \item \(M[X, b] = X \rightarrow Y_1...Y_k\) e quindi è una produzione che verrà utilizzata per continuare la derivazione: questa viene stampata in output, viene rimosso l'elemento \(X\) dalla testa della pila e infine viene inserito il body della produzione in ordine inverso (cioè in modo che \(Y_1\) sia l'elemento in cima allo stack).
    \end{itemize}
\end{enumerate}

Infine, come ultima operazione \(X\) viene assegnato all'elemento in cima alla pila e si ripete. A seguire un esempio che fa uso dell'algoritmo appena descritto.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{ptdp-ex1.tex}
    \caption{Tabella delle strutture a ogni passo}
    \label{ptdp-ex1}
\end{table} 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.7\textwidth,keepaspectratio]{TopDownExercise.png}
%     \caption{TopDownExercise}
%     \label{TopDownExercise}
% \end{figure}

\subsection{Parsing Table}

L'algoritmo di Parsing Predittivo ha una complessità lineare ma come costruiamo le Parsing Table? In quale posizione dobbiamo mettere le produzioni della grammatica per fare sì che l'algoritmo funzioni?

Ricordiamo che la cella \(M[A, b]\) della Parsing Table viene consultata per espandere il non-terminale \(A\) sapendo che il prossimo terminale nell'input buffer è \(b\).

Andiamo dunque a valorizzare la cella \(M[A, b]\) = \(A \rightarrow \alpha\) se

\begin{itemize}
    \item il body della nostra produzione che ha driver \(A\) è tale per cui esiste una derivazione del tipo \(\alpha \Rightarrow^* b \beta\), cioè partendo da \(\alpha\) si riesce, con zero o più passi, ad ottenere una stringa che inizia per \(b\).
    \item oppure \(\alpha \Rightarrow^* \epsilon\) ed è possibile avere \(S \Rightarrow^* w A \gamma\) (ottenuta da una derivazione di tipo leftmost) con \(\gamma \Rightarrow^* b \beta\)
\end{itemize}

Le celle per cui non è possibile inserire una produzione (cioè quelle vuote) verranno valorizzate a \emph{error()}.

\subsubsection{Es 1}

Sia data la seguente grammatica
\begin{itemize}
    \item[] \(S \rightarrow aA \mid bB\)
    \item[] \(A \rightarrow c\)
    \item[] \(B \rightarrow c\)
\end{itemize}

Come riempiamo la tabella? Il linguaggio denotato dalla seguente grammatica include semplicemente due parole che sono \(ac\) oppure \(bc\). Se applichiamo la definizione precedente abbiamo che \(M[S, a] = S \rightarrow aA\) in quanto solamente utilizzando quella produzione riusciremmo ad avere una stringa che comincia per \(a\) (i.e. \(aA \Rightarrow ac\)). Lo stesso ragionamento può essere applicato per \(M[S, b] = S \rightarrow bB\) e, visto che non è possibile creare delle stringhe che cominciano per il terminale \(c\), \(M[S, c] = error()\). A questo punto è possibile passare agli altri due non-terminali della grammatica: nel caso di \(A\) è possibile notare che esiste una sola produzione che permette di ottenere soltanto \(c\) e quindi avremo che \(M[A, c] = A \rightarrow c\); nel secondo caso è possibile applicare la stessa logica e quindi si ha che \(M[B, c] = B \rightarrow c\).

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{filling-parsing-table1.tex}
    \caption{Riempimento Parsing Table - Es 1}
    \label{filling-parsing-table1}
\end{table} 

Ricordiamoci che non è possibile che più produzioni possano essere inserite all'interno della stessa cella in quanto ci stiamo occupando delle grammatiche LL1 in cui quindi è possibile applicare un parsing di tipo deterministico. 

Quando si arriva ad una error entry vuol dire che sulla testa della pila c'è un non-terminale che, indipendentemente da come decida di espanderlo, non mi porterà mai alla stringa che sto cercando di ottenere. 

\subsubsection{Es 2}
Sia data la seguente grammatica
\begin{itemize}
    \item[] \(S \rightarrow aAb\)
    \item[] \(A \rightarrow \epsilon\)
\end{itemize}

In questo esempio invece il linguaggio denotato dalla grammatica precedentemente citata è costituito solamente dalla parola \(ab\). Per la definizione di \(\alpha\) l'unica cella che ha come non terminale (nonchè starting symbol) \(S\) è \(M[S, a] = S \rightarrow aAb\) perchè l'unica stringa che è possibile ottenere inizia per \(a\). Da qui è possibile intuire anche che per ottenere la parola \(ab\) è necessario che \(M[A, b] = S \rightarrow \epsilon\); in tutti gli altri casi verrà ritornato un errore in quanto tale parola non appartiene al linguaggio denotato dalla grammatica. 

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{filling-parsing-table2.tex}
    \caption{Riempimento Parsing Table - Es 2}
    \label{filling-parsing-table2}
\end{table}

\subsection{First(\(\alpha\))}

Insieme di terminali che sono posti all'inizio delle stringhe derivate da \(\alpha\)

Inoltre, se \(\alpha \Rightarrow^* \epsilon\) allora \(\epsilon \in\) first(\(\alpha\)): ciò vuol dire che \(\alpha\) è un non-terminale annullabile (nullable) e quindi che dopo una serie di passi sarà pari a \(\epsilon\).

Il concetto di first può essere applicato come segue

\begin{itemize}
    \item first(\(\epsilon\)) = \{\(\epsilon\)\}
    \item first(\(a\)) = \{\(a\)\}
    \item first(\(A\)) = \(\cup_{A \rightarrow \alpha}\)first(\(\alpha\))
\end{itemize}

Nell'ultimo caso, quello ricorsivo, si ha che first(\(A\)) è dato dall'unione di tutti i first(\(\alpha\)) con \(\alpha\) body delle produzioni della grammatica che hanno \(A\) come driver.

\subsubsection{Algoritmo first(\(\alpha\))}

Mostriamo ora un algoritmo che ci permette di calcolare first(\(Y_1...Y_n\)).

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{first-algorithm.png}
    \caption{Algoritmo per calcolare first(\(\alpha\))}
    \label{first-algorithm}
\end{figure}

L'idea è la seguente: finchè non abbiamo esaminato tutti gli \(Y_j : 1 \leq j \leq n\), si aggiunge first(\(Y_j\)) \(\setminus\) \{\(\epsilon\)\} ai first(\(Y_1...Y_n\)) e poi si controlla se \(\epsilon \in\) first(\(Y_j\)). Se il controllo restituisce true allora vuol dire che è necessario continuare la ricerca dei first perchè è possibile che in almeno un caso \(Y_j = \epsilon\) e quindi è possibile che nessuno dei first(\(Y_j\)) \(\in\) first(\(Y_1...Y_n\)). Se il controllo restituisce invece false allora possiamo fermarci in quanto abbiamo trovato tutti i first(\(Y_1...Y_n\)). L'ultimo controllo serve per verificare se \(\forall\) \( Y_j \in (Y_1...Y_n)\) non sono in realtà tutti dei nullable e quindi è necessario aggiungere anche \(\epsilon\) ai first(\(Y_1...Y_n\)) in quanto anche \((Y_1...Y_n)\) è nullable.

\subsubsection{Training}

Sia data la seguente grammatica

\begin{itemize}
    \item[] \(E \rightarrow TE'\)
    \item[] \(E' \rightarrow +TE' \mid \epsilon\)
    \item[] \(T \rightarrow FT'\)
    \item[] \(T' \rightarrow *FT' \mid \epsilon\)
    \item[] \(F \rightarrow (E) \mid id\)
\end{itemize}

L'idea generale è quella dunque di partire dai casi base e quindi da quelle produzioni che hanno come body un terminale oppure \(\epsilon\). Nel nostro caso dunque, secondo la definizione espressa precedentemente, possiamo affermare con certezza che \{\(\epsilon\)\} \(\in\) first(\(E'\)), \{\(\epsilon\)\} \(\in\) first(\(T'\)) e \{\(id\)\} \(\in\) first(\(F\)). A questo punto dobbiamo applicare l'algoritmo per ogni produzione: ad esempio, per calcolare first(\(F\)) dobbiamo fare l'unione di tutte le produzioni del tipo \(F \rightarrow \alpha\), nel nostro caso abbiamo \(F \rightarrow (E) \cup F \rightarrow id\). Nel primo caso sappiamo che \((E) = Y_1Y_2Y_3\) e dunque dobbiamo analizzare first(\(Y_1\)), tuttavia \(Y_1\) = ( è un terminale per cui first(\(Y_1\)) = \{\(Y_1\)\} e \{\(Y_1\} \neq \epsilon\) per cui possiamo aggiungerlo a first(F) e possiamo fermare subito l'algoritmo. Nel secondo caso invece ci troviamo di fronte al caso base first(\(id\)) = \(\{id\}\) e dunque possiamo aggiungerlo a first(F).

In questo caso il risultato finale è dunque first(F) = \(\{(\} \cup \{id\} = \{(, id\}\).

Il miglior modo per procedere nell'algoritmo è, in mancanza di casi base, trovare una produzione del tipo \(A \rightarrow F\alpha\) per cui è possibile sfruttare la conoscenza appena ottenuta relativamente a first(F). Applicando dunque l'algoritmo per first(T) è necessario analizzare \(T \rightarrow FT' = Y_1Y_2\) per cui è necessario controllare per primo first(\(Y_1\)). Essendo che però abbiamo già calcolato first(F) e, essendo che \(\epsilon \notin\) first(F), possiamo direttamente arrestare l'algoritmo e affermare che first(T) = first(F) = \(\{(, id\}\).

Svolgendo dunque l'esercizio nella sua interezza è possibile ottenere il seguente risultato

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{computing-first.tex}
    \caption{Esercizio sui first}
    \label{computing-first}
\end{table}

Cosa sarebbe successo se la produzione di T fosse stata T -> T’F ?
Allora, considerando tutto il resto invariato andiamo a rivedere i first di T:
In questo caso abbiamo tutti i first di T’, ottengo { “eps”, “*” }, quindi eps è contenuto! L’algoritmo non si ferma (anche se non aggiunge ancora “eps” ai first di T ) e procedo ad aggiungere ai first di T anche i first di F { “id”, “(“ }; a questo punto i first non contengono epsilon e quindi termino riportando come first di T { “*”, “id”, “(“ }


\subsection{Follow(\(A\))}

A differenza dei first, che sono definiti per stringhe generiche di terminali e non terminali, i \emph{follow} sono computati solamente per i non terminali infatti scriveremo follow(\(A\)).

Con follow(\(A\)) indichiamo l'insieme dei terminali che possono seguire A in qualche derivazione.

I first evidenziano quali sono i terminali per cui iniziano le stringhe derivabili da certi elementi (stringhe o non terminali), i follow invece indicano quali sono i terminali che possono seguire.

\subsubsection{Algoritmo Follow(\(A\))}

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{follow-algorithm.png}
    \caption{Algoritmo per calcolare follow(\(A\))}
    \label{follow-algorithm}
\end{figure}

L'algoritmo comincia impostando follow(\(S\)) = \$ perchè tutte le perole inziano partendo dallo starting symbol e dunque viene direttamente inserito il terminatore. %not so sure

Negli altri casi invece \(\forall\) \(A : A\) è un non terminale \(\Rightarrow\) follow(\(A\)) = \(\emptyset\).

A questo punto è necessario sottoloneare che ci interessano le produzioni della grammatica per cui il simbolo A non compare come driver della grammatica ma come body: dato dunque \(B \rightarrow \alpha A \beta\), si seseguono le sequenti operazioni: 

\begin{itemize}
    \item se \(\beta \neq \epsilon\) allora aggiungiamo first(\(\beta\)) \(\setminus \epsilon\) a follow(\(A\)), questo perchè siamo interessati a sapere quali possano essere i possibili terminali che seguono \(A\)
    \item se \(\beta = \epsilon\) or \(\epsilon \in\) first(\(\beta\)) allora aggiungiamo follow(\(B\)) a follow(\(A\)) %il perchè giuro non l'ho capito
\end{itemize}

%Se si immagina un albero di derivazione qusto è formato da un sottoalbero per alpha, poi abbiamo un sottoalbero di A e poi quello di beta che contiene tutto quello che derivad a beta: tutti i terminali che sono diversi da epsilon e derivano da beta allora li mettiamo in A. Se invece beta è uguale a epsilon oppure epsilon appartiene ai first di beta allora dobbiamo aggiungere i follow di B ai follow di A perchè ciò che segue B potrà anche seguire A perchè se noi andiamo a considerare i sottoalberi che vengono generati andiamo a scoprire che A è la radice dell'utlimo sottoalbero della B e quindi quello che può seguire questa particolare occorrenza di B ... Si procede aggiungendo degli elementi ogni volta che andiamo a considerare delle partciolari istanze nella nostra grammatica e ci fermiamo fino a quando non possiamo inserire altro. 

\subsubsection{Training}

Utilizziamo sempre la grammatica dell'esempio precedente e definiamo i follow

\begin{enumerate}
    \item Dall'algoritmo sappiamo follow(E) = \$
    \item Iniziamo guardando la prima produzione, cioè \(E \rightarrow TE'\): se poniamo \(A = E'\), allora \(\beta = \epsilon\) dunque devo aggiungere i follow(\(E\)) (quelli del driver) a follow(\(E'\)) (quelli del body)
    \item Sempre soffermandoci sulla stessa produzione poniamo \(A = T\), questo vuol dire che \(\beta = E' \neq \epsilon\) e quindi, ricadendo nel primo caso, aggiungo first(\(E'\)) \(\setminus \{\epsilon\} = \{+\}\) a follow(\(T\)), tuttavia, essendo che \(\epsilon \in\) first(\(E'\)) allora ricadiamo anche nel secondo caso per cui aggiungiamo follow(\(E\)) a follow(\(T\))
    \item Passiamo ora alla produzione \(E' \rightarrow +TE' \mid \epsilon\): poniamo quindi \(A = E'\) e, essendo che \(\beta = \epsilon\) ricadiamo nel secondo caso dunque dobbiamo aggiungere follow(\(E'\)) a follow(\(E'\)): dato che questo non fornisce informazioni aggiuntive scartiamo questa informazione.
    \item Possiamo ora esaminare il caso per la produzione precedente in cui \(A = T\) e quindi \(\beta = E'\) ma tale caso è già stato esaminato al punto 3
    \item L'ultimo elemento che non abbiamo esaminato nella produzione è terminale \(+\) ma, in quanto terminale, non possiamo calcolare follow(\(+\))
    \item Continuiamo ad applicare l'algoritmo come visto nei punti precedenti per computare i follow delle produzioni rimanenti...
    \item Un caso che potrebbe risultare interessante riguarda la produzione \(F \rightarrow (E)\). Poniamo come fatto precedentemente \(A = E\) e, essendo che \(\beta = (\) \(\neq \epsilon\), è necessario aggiungere first(\(\beta\)) \(\setminus \{\epsilon\} = \{)\}\) a follow(\(E\)) e, visto che la seconda condizione non è vera, possiamo fermarci ed affermare che follow(\(E\)) = \{\$, )\}
\end{enumerate}

Il risultato finale verrà rappresentato come segue:

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{computing-follow.tex}
    \caption{Esercizio sui follow - step intermedio}
    \label{computing-follow}
\end{table}

A questo punto è possibile eliminare mano a mano le dipendenze e le varie ripetizioni presenti all'interno delle computazioni dei follow e ottenere quanto segue: 

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{follow.tex}
    \caption{Esercizio sui follow - risultato finale}
    \label{follow}
\end{table}

\end{document}