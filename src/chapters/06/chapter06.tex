\documentclass[class=book, crop=false, oneside, 12pt]{standalone}
\usepackage{standalone}
\usepackage{../../style}
\usepackage{csquotes}
\graphicspath{{./assets/images/}}

\begin{document}
\chapter{Analisi lessicale e analisi sintattica}

\section{Analisi lessicale}
\subsection{Introduzione}
Dopo i precedenti due capitoli, nei quali abbiamo introdotto e approfondito un buon numero di concetti, algoritmi e modelli necessari alla comprensione di come l'impostazione teorica dei linguaggi formali sia fondamentale nella progettazione dei compilatori, andiamo a tuffarci in quella che è la prima fase di compilazione: l'analisi lessicale.


Ricordiamo che questa è la fase in cui vogliamo identificare quali parti del sorgente che abbiamo scritto corrispondono alle keyword, quali agli identificatori, alle costanti e via di questo passo; per essere più formali, questi elementi che vogliamo riconoscere portano il nome di \emph{lessemi}. 
Il nostro obiettivo in questa fase è trasformare quindi il sorgente in un flusso di tokens, i quali costituiscono i terminali della grammatica che genera il nostro linguaggio di programmazione.

Ricordiamo per l'ennesima volta che la grammatica di un linguaggio ci dirà quali sono le forme che un'espressione deve avere per essere considerata ben formata rispetto a quel linguaggio. Ad esempio, una grammatica ci può dire che la seguente forma denota un'espressione valida:\\
% begin minted
Identificatore      simbolo di assegnamento    numero\\
% end minted
e che quindi espressioni come la seguente sono grammaticali e ben formate secondo il linguaggio.\\
% begin minted
pippo = 2\\
% end minted8
Il mestiere dell'analizzatore lessicale è proprio quello di ricevere in input un \texttt{pippo} qualsiasi (che è un token) e, in output, determinare che è un \emph{identificatore}, ossia la categoria più astratta (lessema) di cui \texttt{pippo} è istanza.

% L’analizzatore lessicale lavora in tandem (pipeline) con l’analizzatore sintattico per non si sa bene come o perché
\subsection{Esempio: la grammatica di C99}
Andiamo a vedere da vicino la grammatica di un reale linguaggio, anzi, del linguaggio preferito di tutti noi, ossia il C (nella versione \(99\), scritta per il parser \emph{Bison}); il lettore interessato ad approfondire può trovare lo stesso file cliccando \url{http://www.quut.com/c/ANSI-C-grammar-y-1999.html}{qui}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth,keepaspectratio]{c99-ex1}
    \caption{}
    % \label{...}
\end{figure}
Notiamo subito alcune differenze rispetto alla notazione che abbiamo impiegato finora: 
\begin{itemize}
    \item la freccia ( \(\to\) ) è rappresentata da un altro simbolo, i due punti ( \(:\) );
    \item il pipe ( \(\mid\) ), invece, possiede l'abituale significato;
    \item i terminali possono essere indicati precisamente se inseriti tra singoli apici '\emph{terminale}';
    \item inoltre, la convenzione rispetto alla capitalizzazione è invertita: qui osserviamo che gli elementi in maiuscolo indicano i terminali, mentre invece quelli in minuscolo rappresentano non terminali; ad esempio, in figura sopra possiamo notare che il non-letterale \texttt{primary\_expression} ha una produzione per cui può risultare o in una serie di letterali (\texttt{IDENTIFIER}, \texttt{CONSTANT}), oppure in una forma \texttt{'(' expression ')'}, dove \texttt{expression} è un altro non-letterale, che a sua volta avrà altre produzioni.
\end{itemize}

Questo file, inoltre, è pensato per essere utilizzato in tandem con un analizzatore sintattico; per questo motivo, nell'intestazione dello stesso possiamo trovare le dichiarazione di quelli che sono i token (vale a dire, lo ripetiamo, i terminali della grammatica descritta). Saranno espressi nella forma rappresentata in Fig.\ref{token_c99}:
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{c99-ex2}
    \caption{}
    \label{token_c99}
\end{figure}

Un'altra cosa che possiamo osservare è la dichiarazione di quello che è lo starting symbol della grammatica:\\
% begin minted
\% start translation\_unit\\
% end minted
Questa dichiarazione in realtà potrebbe non essere presente su alcuni file contenenti grammatiche, dal momento che, in sua assenza, non otteniamo degli errori, bensì viene preso il non-letterale della prima produzione listata a seguito come starting symbol.

Il ruolo dell’analizzatore lessicale è quindi analizzare il sorgente e decidere, di volta in volta, quale derivazione può essere applicata a ciascuna delle righe del sorgente analizzato. Una volta completato l'albero di derivazione e quindi arrivato presso un terminale (ad esempio \texttt{IDENTIFIER}), l'analizzatore lessicale andrà anche ad associargli informazioni come valre e tipo, dal momento che è necessario distinguere quell'\texttt{IDENTIFIER} da un altro. Nel programma potremmo appunto avere due \texttt{IDENTIFIER} di diverso tipo, ma in ogni caso dovremo conservare delle informazioni aggiuntive di qualche tipo (nome, tipo, scope e altro ancora). Tutte queste informazioni vengono conservate in una \emph{symbol table}.

Infine, le ultime rihe del file contengono informazioni utili al particolare analizzatore sintattico utilizzato; avremo modo di parlarne nel dettaglio in futuro.

\subsection{Classi di tokens}
Si potrebbe a ragione considerare superfluo specificare che ogni diversa grammatica (e quindi ogni linguaggio) presenta diverse categorie di tokens; banalmente, il lettore è probabilmente ben cosciente che linguaggi diversi possiedono generalmente keyword diverse. Ad ogni modo, a seguito presentiamo alcune tra le scelte più ricorrenti:
\begin{itemize}
    \item un token per ogni keyword, quindi un token per ogni nome di base già presente nel linguaggio (\texttt{if}, \texttt{while}, \texttt{for} e via dicendo);
    \item un token per ogni operatore (o anche per classe di operatori), quindi in C ne avremo uno per \texttt{+} ma anche uno dedicato per \texttt{++}; 
    \item un unico token per gli identificatori, valido per tutti quanti;
    \item un token per ogni simbolo di punteggiatura. 
\end{itemize}

\subsection{Il compito dell'analizzatore lessicale}
L'obiettivo dell'analalizzatore lessicale è quindi riconoscere i cosiddetti \emph{lessemi}, ossia quelle parti del programma che corrispondono ai token, e ritornarli. Ciascuno di questi, di solito, viene ritornato sotto forma di coppia \texttt{<token-name>: <token-value>}, dove:
\begin{itemize}
    \item \texttt{<token-name>} è il nome scelto per denotare quel preciso token; seguendo l'esempio della grammatica precedente, \texttt{IDENTIFIER} è un \texttt{<token-name>};
    \item \texttt{<token-value>} è tipicamente un puntatore a una entry della symbol table, in cui si va a salvare tutte le informazioni relative a quel preciso token di tipo\footnote{In questo caso la parola "tipo" è chiaramente impropria, ma è molto utile per rendere la natura astratta di classe di token.} \texttt{<token-name>}.
\end{itemize}

\subsection{Lessemi e espressioni regolari}
Andiamo quindi a capire in che modo la teoria studiata nei due capitoli precedenti entra prepotentemente nell'analisi lessicale.

I lessemi sono estremamente facili da descrivere utilizzando le espressioni regolari: ad esempio, in un linguaggio che prevede un lessema identificatore, il quale è costituito di qualsiasi combinazione di lettere maiuscole e minuscole, quest'ultimo può essere facilmente denotato da un'espressione regolare del tipo:
\begin{equation*}
    (a \mid b \mid \ldots \mid z \mid A \mid B \mid \ldots \mid Z)^*
\end{equation*}
Questo è molto interessante: se è vero che i lessemi sono perfettamente descrivibili con dei linguaggi regolari, allora vuol dire che possiamo utilizzare quelli in sede di analisi lessicale, senza dover tirare in ballo i più potenti ma decisamente più complessi linguaggi liberi.

Quindi questi lessemi, essendo descritti da delle espressioni regolari, possono anche essere riconosciuti agevolmente da una macchina a stati, come ad esempio quella in figura sotto, che descrive la classe \texttt{relop} di token per gli operatori relazionali.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{lec-14-1}
    \caption{}
    % \label{dfa-b_odd-and-a_even}
\end{figure}
È molto semplice intuire graficamente che ciascun percorso che termini in uno stato finale ritorna un token, sotto forma di coppia, in cui il \texttt{token-name} è appunto quello della classe \texttt{relop}, mentre il \texttt{token-value} indica qual è l'elemento della classe da ritornare.

\paragraph{Retract}
Possiamo notare che i due stati finali il cui arco entrante è marcato come \(other\) hanno anche un asterisco: questo indica che, quando consumo quella transizione, devo ricordarmi di tornare indietro di un simbolo, perché quest'ultimo simbolo \(other\) (cioè che non fa parti di alcun elemento appartenente alla classe \texttt{relop}) potrebbe essere un elemento di un token successivo, e se non compiano quanto detto sopra rischieremmo di saltarla nell'analisi e ottenere un flusso di token incorretto. Questa operazione è detta \(retract\).

L'operazione di retract è uno dei tanti elementi legati alla gestione dell'input e del buffer che ci fanno pensare che la macchina a stati è un modello molto vicino al più noto, per noi, automa a stati finiti, ma quanto è profondo questo legame?

\subsection{Pattern matching basato su NFA}
Immaginiamo di avere delle espressioni regolari che denotano il linguaggio dei lessemi che siamo interessati a riconoscere. Si pensi ad esempio all'espressione regolare che denota il lessema \texttt{IDENTIFIER}, ossia l'espressione regolare che denota tutte le possibili combinazioni di lettere maiuscole e minuscole (con le dovute peculiarità di ciascun linguaggio); oppure, si pensi a un'espressione regolare che denoti il lessema degli operatori relazionali, o ancora un'altra che denoti la scrittura dei floating numbers. In sostanza, per ciascuna categoria sintattica abbiamo un'espressione regolare che la denota.

Sappiamo anche che, avendo delle espressioni regolari, per ciascuna di queste possiamo costruire un NFA che riconosce il linguaggio denotato dall'espressione considerata; possiamo a questo punto immaginare di far collidere questi NFA inserendo uno stato iniziale extra e collegandolo a ciascun NFA tramite una \(\varepsilon\)-transizione, come mostrato in Fig.\ref{nfa_for_grammar_regular_expressions} sotto.
Da notare che nel caso d'uso del compilatore ci sono delle azioni associate agli stati finali (se l'analizzatore legge un assegnamento compie, appunto, le azioni per registrare l'assegnamento).
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth,keepaspectratio]{lec-14-2}
    \caption{}
    \label{nfa_for_grammar_regular_expressions}
\end{figure}
Una struttura di questo tipo può riconoscere i linguaggi denotati da tutte le espressioni regolari per cui abbiamo costruito degli NFA. 

Adesso dobbiamo pensare come ottimizzare l'uso di una struttura simile per l'analisi lessicale. Potremmo trovarci infatti a gestire problemi di diversa natura, in particolare relativi all'input buffering: ad esempio, come posso fare a sapere se i due caratteri \texttt{i} e \texttt{f} che ho appena letto stanno a indicare la keyword \texttt{if} e non un ipotetico identificatore \texttt{iffoff}? Detto in maniera informale, come decido quando fermarmi, quando tornare indietro, quando e se ho letto più di quanto mi serviva?

Il procedimento che seguiamo è il seguente:
\begin{enumerate}
    \item innanzitutto simuliamo l'NFA creato come sopra descritto;
    \item nel caso di ambiguità, ossia di situazioni come quella descritta sopra tra \texttt{if} e \texttt{iffoff}, la convenzione è di preoseguire la simulazione finché nessun’altra transizione è possibile, solitamente quando incontriamo uno spazio o un \texttt{\(\backslash\)n}, privilegiando quindi il match più lungo (si parla di cercare il \emph{longest match});
    \item se nell'insieme di stati che abbiamo raggiunto ci sono delle azioni disponibili, allora andremo ad eseguire quella appartenente al longest match, in caso di pareggio ciascuna azione avrà una priorità, e noi eseguiremo quella con più alta priorità;
    \item se nessun'azione è disponibile, invece dobbiamo torniamo indietro nella sequenza di stati percorsi, fermarci nel primo insieme di stati che presenta almeno uno stato finale e delle azioni associate e cercare di nuovo quella prioritaria; per ognuno di questi passi all'indietro, dobbiamo ricordari di aggiornare il puntatore all'input buffer.
\end{enumerate}

Tutto questo funzionerebbe in maniera analoga se utilizzassimo un DFA: dovremmo semplicemente costruire il DFA a partire dal NFA, dimodoché l'insieme degli stati del DFA sia un sottoinsieme di quello dell'NFA corrispondente.

\subsection{Generatori di analizzatori lessicali}
Andiamo a parlare quindi di questi generatori. L'idea di creare un generatore di analizzatori lessicali è nata in concomitanza con la prima definizione e implementazione del compilatore del C, e l'idea era di evitare di dover scrivere ogni volta, per ogni programma, un analizzatore lessicale e uno sintattico a mano.

\emph{Flex} è il primo della sua storia\footnote{o meglio, una sua versione più moderna: il primo vero e proprio si chiamava \emph{lex} e quella \emph{f}, aggiunta successivamente, sta per fast.} ed è solitamente compreso nelle distribuzioni di C; l'idea è risultata talmente valida che oggiggiorno ogni linguaggio possiede un proprio generatore di analizzatore lessiale e sintattico.

Il funzionamento è il seguente: 
\begin{itemize}
    \item andremo a creare un file del tipo \texttt{file.l}, il quale sarà l'input del generatore e in cui scriveremo quali sono i pattern che vogliamo riconoscere e quali sono le azioni da compiere in corrispondenza dei vari matches;
    \item a quel punto, possiamo comiplare \texttt{file.l} utilizzando il comando \texttt{Flex}, e questo ci restituirà un file di nome \texttt{lex.yy.c}\footnote{La ragione di questo nome è puramente convenzionale e deriva dal fatto che, storicamente, il generatore Flex è stato usato in coppia all'analizzatore \emph{Yacc}};
    \item compilando questo \texttt{lex.yy.c} con il compilatore \texttt{gcc} otteniamo il lexer, cioè quel signore che si occupa di gestire l'input buffering, di fare le operazioni di retract e altro ancora.
\end{itemize} 
% begin minted
Flex file.l
gcc lex.yy.c -lfl
./a
% end minted
Si tenga bene a mente che queste tre righe descrivono come utilizzare Flex da solo; tuttavia, quest'ultima è un'eventualità piuttosto rara, dal momento che Flex è nato per essere usato in pipeline con un generatore di analizzatore di analisi sintattica, ma poiché questi sono elementi ancora sconosciuti per noi, al momento non ce ne preoccupiamo.

\subsection{Struttura del \texttt{file.l}}
I file con estensione \texttt{.l} che diamo da fagocitare a Flex hanno la seguente struttura:
% begin minted
...(Preambolo)
% { code
% } %
shorthand for patterns
%%
(Parte centrale)
pattern-1 {action-1};
pattern-2 {action-2};
...
%%
(Epilogo)
user rountes
% end minted
Il contenuto di queste tre macrosezioni è ben determinato, andiamo a vederlo più da vicino:
\begin{labeling}{Parte centrale}
    \item[Preambolo] qui ci andrà del codice C,in particolare le inizializzazioni di variabili, o delle abbreviazioni per indicare dei particolari pattern nelle espressioni regolari che andremo a utilizzare sotto;
    \item[Parte centrale] è il cuore del file, e si tratta di una lista pattern-azione, dove pattern è un'espressione regolare, e l'azione sarà ciò che dobbiamo compiere qualora dovessimo riconoscere il pattern corrispondente;
    \item[Epilogo] questa sezione è in realtà facoltativa, il lexer funziona anche senza che vi sia scritto alcunché; tuttavia, quello che potremmo eventualmente trovare e/o scrivere sono delle routine definite dall'utente, le quali verranno copiate e incollate dentro al genituro \texttt{lex.yy.c}.
\end{labeling}

\subsection{Linguaggio delle espressioni regolari in Flex}
Dal momento che tutte le coppie \texttt{pattern \{azione\}} sono scritte in linguaggio \texttt{lex}, a differenza di altre parti di \texttt{file.l}, è opportuno conoscere questo linguaggio. Di seguito presentiamo i \emph{metacaratteri} di Flex, ossia dei caratteri riservati:
\begin{equation*}
    \slash \quad \backslash \quad - \quad * \quad + \quad >\quad " \quad \{ \quad \} \quad . \quad \$ \quad ( \quad ) \quad \mid \quad \% \quad [ \quad ] \quad ^\wedge
\end{equation*}
\noindent Andiamo a vedere più da vicino quali sono le regole di matching dei metacaratteri:
\begin{labeling}{a | b}
    \item[\texttt{.}] qualsiasi carattere, eccetto il newline;
    \item[\texttt{\(\backslash\)n}] il newline;
    \item[\texttt{*}] zero o più copie di un elemento;
    \item[\texttt{+}] una o più copie di un elemento;
    \item[\texttt{?}] zero o una copia di un elemento;
    \item[\texttt{[]}] denota le classi di caratteri: al posto di \(a \mid b \mid \ldots \mid z\), posso scrivere \texttt{[a-z]};
    \item[\texttt{\(^\wedge\)}] inizio di riga, negazione se usato in una classe di caratteri;
    \item[\texttt{\$}] end of line;
    \item[\texttt{a|b}] pipe, semantica consueta;
    \item[\texttt{()}] raggruppamenti;
    \item[\texttt{"+"}] somma letterale di due elementi
    \item[\texttt{\{\}}] espressioni regolari scritte nel preambolo
\end{labeling}

\subsection{Esempi di file per Flex}
Facciamo un breve riepilogo di quanto abbiamo visto: tutti i file predisposti per Flex si compongono da tre sezioni, ciascuna separata da coppie di \%. Nel preambolo si inseriscono delle definizioni come le \texttt{define} che servono e i pattern (espressioni regolari per i lessemi che vogliamo inserire). I pattern sono inseriti insieme all'azione nella parte centrale. In fondo si inseriscono le routine dell'utente che vengono copiate nel file \texttt{lex.yy.c}: un testo minimale è l'invocazione della routine \texttt{yylex()} che se non viene inserita viene chiamata in automatico. I comandi, qualora non si usi Flex con Bison (analizzatore sintattico in pipeline con Flex), sono i seguenti:
\begin{itemize}
    \item Flex file.l
    \item gcc lex.yy.c lfl
    \item ./a
\end{itemize}

\subsubsection{file0.l}
        
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file0.l.png}
    \caption{file0.l}
    \label{file0.l}
\end{figure}

In qusto primo esempio vediamo che nel preambolo non sono presenti né delle \texttt{define} né del codice; dal momento che non sono presenti routine inserite dall'utente, verrà invocata la procedura \texttt{yylex()} all'esecuzione. Basandoci sulla sintassi di Flex vista precedentemente, andiamo ad analizzare la parte centrale costituita da una sola coppia pattern-azione:
\begin{itemize}
    \item il pattern è dato dalla regular expression composta solo da \texttt{.}, il che significa che l'azione seguente farà riferimento a tutti i caratteri incontrati, tranne quello di \texttt{\(\backslash\)n} (\emph{new line});
    \item l'azione è quel \texttt{printf("hello world!")}, il che consiste banalmente nello stampare la stringa hello world ogni volta che viene eseguito un match sul pattern.
\end{itemize}
In sostanza, questo file ci farà sì che, nella lettura, qualsiasi carattere incontriamo, ad eccezione del new line, verrà sostituito da un'occorrenza della stringa \texttt{"hello world!"}; quindi, ad esempio:
\begin{itemize}
    \item \(a\) diventerà \texttt{hello world!};
    \item \(aa\) diventerà \texttt{hello world!hello world!}.
\end{itemize}

Possiamo notare anche un'altra cosa molto interessante: solitamente, per poter utilizzare lo standard output nei programmi C, è necessario includere all'interno del file la libreria stdout \texttt{stdio.h}, ma nel \texttt{file0.l} non vi è nemmeno l'ombra di un inlcude: è lo stesso Flex a occuparsi di aggiungere in un secondo momento tutti i dettagli necessari per la gestione dell'output.

\subsubsection{file1.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file1.l.png}
    \caption{file1.l}
    \label{file1.l}
\end{figure}


\paragraph{Preambolo}
Diversamente dall'esempio precedente, in questo \texttt{file1.l} abbiamo del contenuto nel preambolo. Andiamo a vederlo da vicino cosa ci troviamo:

\begin{enumerate}
    \item la prima riga (in questo caso vuota) è identificata dal simbolo di apertura \texttt{\%\{} e cui può seguire del codice C;
    \item la seconda riga, invece, è successiva al simbolo di chiusura \texttt{\%\}} e contiene quelli che sono gli shorthands per i pattern, vale a dire degli alias per delle regular expression.
\end{enumerate}

Rifacendoci sempre alla sintassi di Flex, possiamo analizzare l'espressione regolare definita nel preambolo e arrivare alla conclusione che faccia riferimento a quell'insieme di caratteri (\texttt{[ ]}) che si ripetono \(0\) o più volte (*) e che sono diversi (\^{}) da '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}'; questo significa che comporteranno un match tutte quelle sequenze di caratteri che sono separate da spazi, tab o new line; più semplicemente, saranno le parolea causare un match.

\paragraph{Parte centrale}
Come descritto precedentemente, nella parte centrale i pattern sono associati alle relative azioni; in questo caso, le coppie che troviamo sono le seguenti:
\begin{itemize}
    \item \texttt{non\_white} comporta l'esecuzione della macro \texttt{echo} di Flex, che è equivalente a \texttt{printf("\%s", yytext)} in linguaggio C, e stamperà a video esattamente la sequenza di caratteri che ha causato il match;
    \item un qualunque carattere diverso da \texttt{\textbackslash n (.)} verrà eliminato;
    \item new line (\texttt{\textbackslash n}) verrà anch'esso eliminato
\end{itemize}

Complessivamente, il risultato ottenuto dall'esecuzione di un programma compilato a partire dal file discusso porterà all'eliminazione di '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}' dal testo passato in input e ci farà ottenere ad una sequenza continua di caratteri (ad esempio, una stringa del tipo "\texttt{ ab c d e \hspace{.8cm} f g}" diventerà "\texttt{abcdefg}").

\subsubsection{file2.l}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file2.l.png}
    \caption{file2.l}
    \label{file2.l}
\end{figure}

\paragraph{Preambolo}
In questo esempio, invece, troviamo finalmente del codice C nel preambolo; in particolare, troviamo la dichiarazione di un intero chiamato \texttt{lineno} ed inizializzato al valore \(1\). 

\paragraph{Parte centrale}
A seguire abbiamo la parte centrale del file, dove: 

\begin{itemize}
    \item all'espressione regolare \texttt{\^{}.*\textbackslash n}, che identifica tutte quelle sequenze di caratteri che iniziano per un carattere differente da new line ripetuto zero o più volte e finiscono per new line o, più semplicemente, le parole sulla stessa riga
    \item  viene associata l'azione \texttt{printf("\%4d)\textbackslash t\%s", lineno++, yytext)}, che stampa il valore dell'intero lineno seguito dal simbolo ) e dal contenuto del buffer yytext (che contiene l'input fino ad ora riconosciuto) ed infine esegue il postincremento di lineno.
\end{itemize}

\noindent Quindi, il nostro programma si occuperà di stampare tutte le parole sulla stessa riga anteponendo il numero della riga a cui fa riferimento; in altre parole, se avessimo un testo come il seguente:
\begin{itemize}[noitemsep]
    \item[] \emph{Autem quo ea. Voluptatum saepe porro. Quibusdam illo eum.}
    \item[] \emph{Quia aperiam nesciunt. Qui est voluptate. Aut temporibus perspiciatis.}
    \item[] \emph{Repudiandae delectus omnis. Modi earum doloribus. Quis eaque quidem.}
\end{itemize}
Allora avremmo che verrebbe riscritto come segue:
\begin{enumerate}[noitemsep]
    \item[\texttt{1}] \emph{Autem quo ea. Voluptatum saepe porro. Quibusdam illo eum.}
    \item[\texttt{2}] \emph{Quia aperiam nesciunt. Qui est voluptate. Aut temporibus perspiciatis.}
    \item[\texttt{3}] \emph{Repudiandae delectus omnis. Modi earum doloribus. Quis eaque quidem.}
\end{enumerate}

\subsubsection{file3.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file3.l.jpg}
    \caption{file3.l}
    \label{file3.l}
\end{figure}
In questo quarto esercizio andiamo a combinare quanto visto precedentemente, aggiungendo anche una routine custom scrivendo del codice ad hoc nell'epilogo del file.

\paragraph{Preambolo}
Nel preambolo abbiamo la dichiarazione di tre variabili che ci serviranno nelle routine come contatori; inoltre, abbiamo anche una shorthand, chiamata \texttt{word}, la cui espressione regolare include tutte quelle sequenze di uno o più caratteri (\texttt{+}) che non contengono '\texttt{ }', '\texttt{\textbackslash t}' e '\texttt{\textbackslash n}.

\paragraph{Parte centrale}
Nella parte centrale del file abbiamo invece le seguenti associazioni pattern-azione:

\begin{itemize}
    \item \texttt{word} comporta l'esecuzione del seguente pezzo di codice \texttt{\{wordCount++; charCount += yyleng; \}}, dove la variabile \texttt{wordCount} viene postincrementata, mentre alla variabile \texttt{charCount} viene sommato un numero pari a \texttt{yyleng}, che contiene la lunghezza della stringa riconosciuta e, per via dell'espressione regolare alla base della shorthand, equivalente a quella della parola;
    \item il secondo pattern è \texttt{\textbackslash n}, cioè nel caso in cui si trovi una sequenza di caratteri composta solamente da una new line; l'azione legata è il blocco \texttt{\{charCount++; lineCount++; \}}, dove si postincrementano sia \texttt{charCount} che \texttt{lineCount};
    \item infine, l'ultimo pattern, il punto (\texttt{.}), comporta solamente il postincremento della variabile \texttt{charCount}.
\end{itemize}

In sostanza, il programma che stiamo descrivendo con questi pattern si occupa di contare complessivamente quanti caratteri, parole e righe sono state individuate in un dato testo; qualora avessimo un match con una parola, allora avviene un incremento proporzionale al numero di caratteri che costituiscono quella stessa parola e, inoltre, si aggiorna il numero di parole totali che sono contenute nel testo; ogni volta che si trova una new line si incrementano di un'unità il numero di righe e di caratteri e, infine, ogni volta che si trova uno spazio o un tab viene incrementato solamente il numero di caratteri. 

\subsubsection{file4.l}
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{file4.l.jpg}
    \caption{file4.l}
    \label{file4.l}
\end{figure}
L'ultimo esempio proposto fa uso di quanto discusso fino ad ora per analizzare un \texttt{csv} e restituire i dati sotto forma di una tabella in formato \texttt{html}.

\paragraph{Preambolo}
Nel preambolo viene dichiarata una variabile intera \texttt{comma}, che è inizializzata a \(0\); servirà per controllare le entry vuote nel \texttt{csv} passato in input date da virgole consecutive.

\paragraph{Parte centrale}
Andiamo a vedere da vicino le coppie pattern-azione che troviamo nella parte centrale.
\begin{itemize}
    \item La regular expression \texttt{[a-zA-Z0-9]+}, che indica la sequenza di uno o più caratteri alfanumerici, comporta l'esecuzione di 
    
    \{printf("\(<\)td\(>\) "); ECHO; comma = 0;\} 
        
    dove viene creata una nuova colonna per la tabella, viene inserita la sequenza appena letta e viene impostato il valore dell'intero \texttt{comma} a \(0\).
    \item La seconda espressione è "\texttt{,}", che vuol dire che bisogna trovare il carattere che corrisponde alla virgola e che nel \texttt{csv} serve per separare i dati, comporta l'esecuzione del blocco di codice
        % \begin{minted}[linenos, breaklines, tabsize = 2]{c}
        % {            
        % if (comma) {
        %     printf("<td> </td>"); 
        % } else { 
        %     printf("</td>");
        % }       
        % comma = 1;
        % }
        % \end{minted}
        
    dove viene verificato il numero di virgole incontrate fino a quel punto: se si sono incontrate 0 virgole, allora si chiude semplicemente la colonna precedentemente aperta; se invece si è già incontrata \(1\) virgola e, consecutivamente, se ne legge un'altra (o comunque le due occorrenze risultano essere separate da sequenze non alfanumeriche), viene creata una colonna vuota, in quanto nel \texttt{csv} non vi sono dati rilevanti. In ogni caso alla fine del blocco viene impostato il valore di comma ad \(1\).
    \item In ultimo, \texttt{\textbackslash n}, cioè per ogni new line viene eseguito il blocco di codice \{printf("\(<\)/tr\(>\) \textbackslash n \(<\)tr\(>\)"); comma = 0;\} dove viene chiusa la riga precedente ed aperta quella successiva.
\end{itemize}

\paragraph{Epilogo}
Nell'epilogo troviamo una routine custom dove viene creato l'html di base per creare la tabella e, tra le varie cose, viene anche aperta la prima riga. Dopo aver stampato lo scheletro della struttura viene invocata la procedura \texttt{yylex()}, la quale cerca i pattern nel testo; infine, la tabella viene chiusa.

\subsection{Ulteriori informazioni su Flex}
Quando abbiamo discusso del funzionamento dell'analisi lessicale basata sull'utilizzo di NFA o DFA abbiamo anche parlato del fatto che, quando si tenta di riconoscere il pattern di una determinata parola, è possibile che avvengano dei match su più stati finali dell'automa e che quindi si possa avere il dubbio su quale azione eseguire: come si riflette tutto ciò in Flex? 

Per risolvere a tale problematica si utilizza la regola del \emph{longest match}: dalla lista di pattern si va a recuperare quello più lungo che ha portato ad un match.

Cosa succede invece se ci sono più pattern che a parità di lunghezza comportano un match (ovvero i due pattern hanno la stessa lunghezza)? In questo caso si sceglie sempre il primo della lista. Da qui si intuisce dunque che anche l'ordine in cui si scrive la sequenza dei pattern ha una sua importanza: un medesimo \texttt{file.l} potrebbe non generare lo stesso risultato se si eseguisse uno shuffle dei pattern.

\section{Parsing}
Il parsing (o analisi sintattica) è quel processo in cui, data una grammatica \(\mathcal{G} = (V, T, S, \mathcal{P})\) e una parola \(w\), dobbiamo dire se \(w \in \mathcal{L(G)}\) e, se così fosse, fornire il suo albero di derivazione. Solitamente gli approcci al parsing che vengono presi in considerazione nell'ambito dei linguaggi di programmazione sono due: 
\begin{itemize}
    \item \textbf{Top-Down}: consiste nella costruzione di una derivazione leftmost da uno start symbol della grammatica e quindi procede dalla radice verso le foglie dell'albero di derivazione; a prima vista, si direbbe che sia l'approccio più intuitivo;
    \item \textbf{Bottom-Up}: consiste invece nella costruzione di una derivazione rightmost (in ordine inverso) della stringa dalle foglie alla radice.
\end{itemize}
A questo punto è necessario aggiungere che il parsing non si limita a questi due approcci, ma esiste anche in forma più generale utilizzando delle tattiche che vengono impiegate nel caso dei linguaggi naturali. Gli approcci descritti non permettono di considerare tutti i possibili linguaggi liberi, ma solamente delle sottoclassi: per questo si ha un'analisi sintattica estremamente efficiente dal punto di vista computazionale.

\subsection{Top-Down Parsing}
\subsubsection{Esempio 1}
Sia \(w\) = \(bd\) e sia \(\mathcal{G}\): 
\begin{align*}
    \mathcal{G}: S &\rightarrow Ad \mid Bd \\
    A &\rightarrow a \\
    B &\rightarrow b
\end{align*}

Per verificare se \(w \in \mathcal{L(G)}\) con un approccio top-down dobbiamo ottenere una derivazione leftmost a partire dallo starting symbol. Ovviamente, si noterà subito che non è possibile scegliere \(Ad\) come derivazione iniziale dello starting symbol, perché a quel punto l'unica parola ottenibile sarebbe parola \(ad\); dobbiamo invece optare per la seconda derivazione. La derivazione completa ci porta a
\begin{align*}
    S \Rightarrow Bd \Rightarrow bd
\end{align*}
Visto che abbiamo dimostrato che \(w \in \mathcal{L(G)}\) e che tale derivazione esiste, allora possiamo fornire il suo derivation tree che, per questo esempio, risulta davvero molto semplice.

\begin{figure}[H]
    \centering
    \includegraphics[width=.15\textwidth,keepaspectratio]{par-td-es1.png}
    \caption{Albero di derivazione per esercizio 1}
    \label{par-td-es1}
\end{figure}

\subsubsection{Esempio 2}
Sia \(w\) = \(id + id * id\) e sia \(\mathcal{G}\):
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon \\
    F &\rightarrow (E) \mid id
\end{align*}

Qui non è così intuitivo riuscire a dire se esiste una derivazione leftmost: che cosa mi conviene espandere? Avremo modo di parlare meglio di questo esempio non appena introdurremo il \textbf{Predictive top-down parsing}.

\subsubsection{Esempio 3}
Sia \(w\) = \(cad\) e sia \(\mathcal{G}\):
\begin{align*}
    S &\rightarrow cAd \\
    A &\rightarrow ab \mid a
\end{align*}

Nonostante questo esempio sia molto intuitivo, dobbiamo sforzarci di ragionare nei panni dell'algoritmo di parsing: dopo la prima derivazione, infatti, entrambe le opzioni che vengono proposte per poter derivare il non-terminale \(A\) sono apparentemente valide in quanto iniziano entrambe per \(a\). Quale delle due dovrei dunque scegliere? Quanto devo continuare l'esecuzione prima di accorgermi se ho fatto una scelta corretta oppure no? Il nostro algoritmo potrebbe anche trovarsi nel caso di dover fare \emph{backtrack}, perché semplicemente non c'era un modo semplice e generale per capire cosa dover scegliere: ovviamente questo tipo di tecnica funziona, ma vogliamo un approccio efficiente e il backtrack, come sappiamo, in questo non ci aiuta.

\subsection{Predictive Top-Down Parsing}
Nel caso del Predictive top-down parsing non è mai necessario applicare la tecnica del backtrack, poiché questo fa riferimento a una classe particolare di grammatiche, le quali vengono definite \textbf{LL(1) grammars}; vengono così chiamate per via della procedura impiegata per analizzarle, in cui:
\begin{itemize}
    \item guardiamo le parole da sinistra a destra;
    \item eseguiamo una produzione leftmost;
    \item guardiamo un solo simbolo (non-terminale).
\end{itemize}
Tali grammatiche prevedono una tipologia di parsing per cui non è necessario backtrack, e per di più è \textbf{completamente deterministica}.

Queste grammatiche vengono classificate a seconda del grado di determinismo che consentono; all'inizio del corso abbiamo visto la differenza tra le grammatiche senza contesto e contestuali, ma adesso le classi che incontreremo da oggi in poi si differenzieranno esclusivamente per il tipo di analizzatore con cui possiamo riconoscere le parole generate da queste grammatiche. 

In questo caso il parsing si basa sul fatto che possiamo costruire una tabella di parsing che ci guida nell'analisi della parola che ci viene data in input; quessta strategia ci permette molto efficacemente di dire se la parola appartenga oppure no al linguaggio e, in caso di esito positivo, di costruire la derivazione leftmost richiesta e il conseguente albero di derivazione.

Prendiamo come esempio il caso che abbiamo lasciato in sospeso precedentemente:
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon \\
    F &\rightarrow (E) \mid id
\end{align*}
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{ptdp-example.tex}
    \caption{Tabella del parsing top-down}
    \label{ptdp-example}
\end{table} 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.7\textwidth,keepaspectratio]{parsingTopDownTable.jpg}
%     \caption{parsingTopDownTable}
%     \label{parsingTopDownTable}
% \end{figure}

La tabella è così costruita:
\begin{itemize}
    \item si ha una \textbf{riga} per ogni non-terminale della grammatica;
    \item ed una \textbf{colonna} per ogni terminale della grammatica, a cui si aggiunge il simbolo \$ alla parola fornita in input e utilizzato come terminatore
    \item le entry vuote all'interno della tabella identificano i casi di errore.
\end{itemize}

Avendo a disposizione la tabella di parsing, la cui costruzione verrà trattata successivamente, è possibile utilizzarla per il parsing top-down predittivo.

\subsubsection{Algoritmo per il Predictive Top-Down Parsing}
\begin{itemize}
    \item Input: Una stringa \(w\), una tabella \(M\) di parsing top-down per la grammatica \(\mathcal{G} = (V, T, S, \mathcal{P})\);
    \item Output: La derivazione leftmost della stringa \(w \iff w \in \mathcal{L(G)}\), altrimenti \emph{error()}.
\end{itemize}
Inizializziamo la procedura posizionando \(w\$\) nell'input buffer e, nella pila che viene utilizzata per inserire e analizzare gli elementi parziali, inseriamo il simbolo \$ con lo starting symbol \(S\) in cima.
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{PredictiveTopDownAlgorithm.png}
    \caption{PredictiveTopDownAlgorithm}
    \label{PredictiveTopDownAlgorithm}
\end{figure}

Nell'algoritmo di parsing si utilizza la variabile \(b\) come primo simbolo delle parola w\$ e si inizializza la variabile X con la cima dello stack (il caso base corrisponde ad avere \(X = S\)). Finché \(X \neq \$\) (sostanzialmente finché non ho svuotato completamente la pila), sono dati i seguenti casi:

\begin{enumerate}
    \item se \(X = b\), allora tolgo l'elemento dalla cima della pila e imposto \(b\) al simbolo successivo nell'input buffer. Questo corrisponde al caso in cui nella derivazione della costruzione parziale ho ottenuto un match con un terminale nella parola;
    \item se invece X è comunque un terminale, allora deve essere che \(X \neq b\), ma ciò produce un errore;
    \item se invece X è un non-terminale, allora è necessario verificare la tabella di top-down parsing in posizione \(M[X, b]\) e possono valere le seguenti:
    \begin{itemize}
        \item \(M[X, b]\) = \texttt{error} e allora viene restituito \texttt{error()};
        \item \(M[X, b] = X \rightarrow Y_1...Y_k\) e quindi è una produzione che verrà utilizzata per continuare la derivazione: questa viene stampata in output, viene rimosso l'elemento \(X\) dalla testa della pila e infine viene inserito il body della produzione in ordine inverso (cioè in modo che \(Y_1\) sia l'elemento in cima allo stack).
    \end{itemize}
\end{enumerate}

Infine, come ultima operazione \(X\) viene assegnato all'elemento in cima alla pila e si ripete. A seguire un esempio che fa uso dell'algoritmo appena descritto.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{ptdp-ex1.tex}
    \caption{Tabella delle strutture a ogni passo}
    \label{ptdp-ex1}
\end{table} 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.7\textwidth,keepaspectratio]{TopDownExercise.png}
%     \caption{TopDownExercise}
%     \label{TopDownExercise}
% \end{figure}

\subsection{Parsing Table}

L'algoritmo di Parsing predittivo ha una complessità lineare. Ci chiediamo quindi: come costruiamo le Parsing Table? In quale posizione dobbiamo mettere le produzioni della grammatica per fare sì che l'algoritmo funzioni?

Ricordiamo che la cella \(M[A, b]\) della parsing table viene consultata per espandere il non-terminale \(A\) sapendo che il prossimo terminale nell'input buffer è \(b\). Andiamo dunque a valorizzare la cella: \(M[A, b]\) = \(A \rightarrow \alpha\) se:

\begin{itemize}
    \item il body della nostra produzione di driver \(A\) è tale per cui esiste una derivazione del tipo \(\alpha \Rightarrow^* b \beta\), ossia partendo da \(\alpha\) si riesce, con zero o più passi, a ottenere una stringa che inizia per \(b\);
    \item oppure \(\alpha \Rightarrow^* \varepsilon\) ed è possibile avere \(S \Rightarrow^* w A \gamma\) (ottenuta da una derivazione di tipo leftmost) con \(\gamma \Rightarrow^* b \beta\)
\end{itemize}

Le celle per cui non è possibile inserire una produzione (cioè quelle vuote) verranno valorizzate a \texttt{error()}.

\subsubsection{Esercizio 1}

Sia data la seguente grammatica:
\begin{align*}
    \mathcal{G}: S &\rightarrow aA \mid bB \\
    A &\rightarrow c \\
    B &\rightarrow c
\end{align*}

Andiamo a vedere come costruire la tabella, aiutandoci dal fatto che è molto semplice ricavare a vista il linguaggio denotato dalla seguente grammatica, che include semplicemente le due parole \(ac\) oppure \(bc\). Partiamo dalle produzioni di \(S\):
\begin{itemize}
    \item se applichiamo la definizione precedente abbiamo che \(M[S, a] = S \rightarrow aA\), in quanto solamente utilizzando quella produzione riusciremmo ad avere una stringa che comincia per \(a\) (\(aA \Rightarrow ac\));
    \item lo stesso ragionamento può essere applicato per \(M[S, b] = S \rightarrow bB\);
    \item dal momento che non è possibile creare delle stringhe che cominciano per il terminale \(c\), allora avremo che \(M[S, c] = error()\);
\end{itemize}
A questo punto è possibile passare agli altri due non-terminali della grammatica: 
\begin{itemize}
    \item nel caso di \(A\) è possibile notare che esiste una sola produzione che permette di ottenere soltanto \(c\), quindi avremo che \(M[A, c] = A \rightarrow c\);
    \item nel secondo caso è possibile applicare la stessa logica, per cui avremo che \(M[B, c] = B \to c\).
\end{itemize} 
La tabella finale sarà quindi costruita in questo modo:

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{filling-parsing-table1.tex}
    \caption{Parsing table per esercizio 1}
    \label{filling-parsing-table1}
\end{table} 

Ricordiamoci che non è possibile che più produzioni possano essere inserite all'interno della stessa cella, in quanto ci stiamo occupando delle grammatiche LL1, nelle quali è possibile applicare un parsing di tipo deterministico. 

Quando si arriva ad una error entry vuol dire che sulla testa della pila c'è un non-terminale che, indipendentemente da come decida di espanderlo, non mi porterà mai alla stringa che sto cercando di ottenere. 

\subsubsection{Esercizio 2}
Sia data la seguente grammatica:
\begin{align*}
    \mathcal{G}: S &\rightarrow aAb \\
    A &\rightarrow \varepsilon
\end{align*}

In questo esempio, invece, il linguaggio denotato dalla nostra grammatica è composto solamente dalla parola \(ab\). Per la definizione di \(\alpha\), l'unica cella che ha come non terminale (nonché starting symbol) \(S\) è la cella \(M[S, a] = S \rightarrow aAb\), perché l'unica stringa che è possibile ottenere inizia per \(a\). Da qui è possibile intuire anche che, per ottenere la parola \(ab\), è necessario che \(M[A, b] = S \rightarrow \varepsilon\); in tutti gli altri casi verrà ritornato un errore, in quanto tale parola non apparterrebbe al linguaggio denotato dalla grammatica. 

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{filling-parsing-table2.tex}
    \caption{Parsing table per esercizio 2}
    \label{filling-parsing-table2}
\end{table}

\section{First(\(\alpha\))}
\subsection{Definizione}
\begin{definition}
    Chiamiamo \(first(\alpha)\) quell'insieme di terminali che sono posti all'inizio delle stringhe derivate da \(\alpha\).
    
    Inoltre, se \(\alpha \Rightarrow^* \varepsilon\), allora \(\varepsilon \in \textrm{first}(\alpha)\): ciò vuol dire che \(\alpha\) è un non-terminale annullabile (nullable) e quindi, dopo una serie di passi, sarà pari a \(\varepsilon\).
\end{definition}

Il concetto di first può essere definito ricorsivamente come segue:

\begin{labeling}{step}
    \item[base] casi base:
    \begin{itemize}
        \item first(\(\varepsilon\)) = \{\(\varepsilon\)\}
        \item first(\(a\)) = \{\(a\)\}
    \end{itemize}
    \item[step] first(\(A\)) = \(\cup_{A \rightarrow \alpha}\) first(\(\alpha\))
\end{labeling}

    Nell'ultimo caso, quello ricorsivo, si ha che first(\(A\)) è dato dall'unione di tutti i first(\(\alpha\)), dove \(\alpha\) è il body delle produzioni della grammatica che hanno \(A\) come driver.

\subsection{Algoritmo di calcolo di first(\(\alpha\))}

Mostriamo ora un algoritmo che ci permette di calcolare first(\(Y_1 \ldots Y_n\)) (con \(Y_i \in V\)), cioè l'insieme dei first per una certa parola considerata.

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{first-algorithm.png}
    \caption{Algoritmo per calcolare first(\(\alpha\))}
    \label{first-algorithm}
\end{figure}

Andiamo a vedere più da vicino quale idea stiamo seguendo in questo algoritmo. Dopo aver inizializzato l'insieme dei first(\(Y_1 \ldots Y_n\)), vado a iterare su tutti gli elementi \(Y_j \in V\) della parola: 
\begin{itemize}
    \item finché non abbiamo esaminato tutti gli \(Y_j : 1 \leq j \leq n\), si aggiunge first(\(Y_j\)) \(\setminus\{\varepsilon\}\) ai first(\(Y_1 \ldots Y_n\)) e poi si controlla se \(\varepsilon \in\) first(\(Y_j\));
    \item se il controllo restituisce \texttt{true}, allora vuol dire che è necessario continuare la ricerca dei first, perché è possibile che in almeno un caso \(Y_j = \varepsilon\) e quindi è possibile che nessuno dei first(\(Y_j\)) \(\in\) first(\(Y_, \ldots Y_n\));
    \item se il controllo restituisce invece \texttt{false}, allora possiamo fermarci, in quanto abbiamo trovato tutti i first(\(Y_1, \ldots, Y_n\)).
\end{itemize} 
L'ultimo controllo serve per verificare se, \(\forall\) \( Y_j \in (Y_1 \ldots Y_n)\), non sono in realtà tutti annullabili; in tal caso, sarebbe necessario aggiungere anche \(\varepsilon\) ai first(\(Y_1 \ldots Y_n\)), in quanto \((Y_1 \ldots Y_n)\) sarebbe annullabile.

\subsection{Training}
Sia data la seguente grammatica:
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon\\
    F &\rightarrow (E) \mid id
\end{align*}

L'idea generale è quella dunque di partire dai casi base, vale a dire da quelle produzioni che hanno come body o un terminale oppure \(\varepsilon\). Nel nostro caso dunque, secondo la definizione espressa precedentemente, possiamo risolvere immediatamente la procedura per i seguenti casi: 
\begin{align*}
    \{\varepsilon\} &\in first(E')\textrm{,} &\textrm{da}\; E' &\to \varepsilon \\
    \{\varepsilon\} &\in first(T')\textrm{,} &\textrm{da}\; T' &\to \varepsilon \\
    \{id\} &\in first(F)\textrm{,} &\textrm{da } F\; &\to id
\end{align*}

A questo punto dobbiamo applicare l'algoritmo per ogni produzione: ad esempio, per calcolare first(\(F\)) dobbiamo fare l'unione di tutte le produzioni di \(F\); nel nostro caso saranno quindi \(F \rightarrow (E) \cup F \rightarrow id\):
\begin{itemize}
    \item nel primo caso abbiamo \((E)\), che è una parola composta di tre elementi (\(Y_1 = '(', Y_2 = E, Y_3 = ')'\), il lettore perdoni l'abuso di notazione nell'utilizzo degli apici) e dunque per prima cosa dobbiamo analizzare first(\(Y_1\)); tuttavia, \(Y_1 = '('\) è un terminale, da cui avremo che il suo first sarà first(\(Y_1\)) = \{\(Y_1\)\}, il che significa che \(\{Y_1\} \neq \varepsilon\), per cui possiamo aggiungerlo a first(\(F\)) e fermiamo subito l'algoritmo;
    \item nel secondo caso, invece, ci troviamo di fronte al caso base first(\(id\)) = \(\{id\}\) e dunque possiamo subito aggiungerlo a first(\(F\)).
\end{itemize}
In questo caso il risultato finale è dunque first(\(F\)) = \(\{(\} \cup \{id\} = \{(, id\}\).

A questo punto, il miglior modo per procedere nell'algoritmo è, in mancanza di casi base, trovare una produzione del tipo \(A \rightarrow F\alpha\); in questo modo possiamo sfruttare la conoscenza appena ottenuta relativamente a first(\(F\)). Applicando dunque l'algoritmo per first(\(T\)) è necessario analizzare \(T \rightarrow FT' = Y_1Y_2\), quindi andiamo a controllare per primo first(\(Y_1\)). Dal momento che che abbiamo già calcolato first(\(F\)) e poiché \(\varepsilon \notin\) first(\(F\)), possiamo direttamente arrestare l'algoritmo e affermare che first(\(T\)) = first(\(T\)) = \(\{(, id\}\).

Svolgendo dunque l'esercizio nella sua interezza è possibile ottenere il seguente risultato:

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{computing-first.tex}
    \caption{Esercizio sui first}
    \label{computing-first}
\end{table}

Cosa sarebbe successo se invece la produzione di \(T\) fosse stata \(T \to T’F\)? Se tutto ill resto rimane invariato, dobbiamo andare a rivedere l'insieme first(\(T\)). Al posto dei first(\(F\)), in questo caso avremo i first(\(T’\)), per cui otterremo \(\{\varepsilon, \ast\}\); dal momento che \(\varepsilon\) è contenuto nell'insieme, l’algoritmo non si ferma (e non aggiunge ancora \(\varepsilon\) ai first(\(T\)) e procediamo quindi ad aggiungere ai first(\(T\)) anche i first(\(F\)), che sono \(\{id, )\}\); a questo punto i first non contengono \(\varepsilon\) e quindi termino riportando come first(\(T\)) \(\{\ast, id, )\}\)

\section{Follow(\(A\))}
\subsection{Definizione}
A differenza dei first, che sono definiti per stringhe generiche di terminali e non terminali, i \emph{follow} sono computati solamente per i non terminali infatti scriveremo follow(\(A\)).
\begin{definition}
    Con follow(\(A\)) indichiamo l'insieme dei terminali che possono seguire A in qualche derivazione.    
\end{definition}
I first evidenziano quali sono i terminali per cui iniziano le stringhe derivabili da certi elementi (stringhe o non terminali), i follow invece indicano quali sono i terminali che possono seguire.

\subsection{Algoritmo per il calcolo dei follow(\(A\))}

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth,keepaspectratio]{follow-algorithm.png}
    \caption{Algoritmo per calcolare follow(\(A\))}
    \label{follow-algorithm}
    % PER CHI RISCRIVERà L'ALGORITMO: lasciate la label invariata o fate un refactoring perché viene referenziata giù dabbasso <3
\end{figure}

L'algoritmo comincia impostando follow(\(S\)) = \$, dal momento che lo starting symbol genera tutte le possibili parole, e per qualsiasi parola possiamo aspettarci di trovare il terminatore di stringa \$. Negli altri casi, invece, \(\forall A\) tale che \(A\) è un non terminale, inizializziamo in questo modo: follow(\(A\)) = \(\emptyset\).

A questo punto è necessario sottoloneare che ci interessano solamente le produzioni della grammatica per cui il simbolo \(A\) non compare come driver della grammatica, ma come body; dunque, per ogni \(B \rightarrow \alpha A \beta\), si eseguono le sequenti operazioni: 

\begin{itemize}
    \item se \(\beta \neq \varepsilon\), allora aggiungiamo first(\(\beta\)) \(\setminus \varepsilon\) a follow(\(A\));
    \item se \(\beta = \varepsilon\) or \(\varepsilon \in\) first(\(\beta\)), allora aggiungiamo follow(\(B\)) a follow(\(A\)), perché quello che segue \(B\) potrà seguire anche \(A\) (si tenga presente che se \(\beta = \varepsilon\), allora \(A\) è la radice dell'ultimo sottoalbero generato da \(B\).
\end{itemize}   
% se si immagina un albero di derivazione qusto è formato da un sottoalbero per alpha, poi abbiamo un sottoalbero di A e poi quello di beta che contiene tutto quello che derivad a beta: tutti i terminali che sono diversi da epsilon e derivano da beta allora li mettiamo in A. Se invece beta è uguale a epsilon oppure epsilon appartiene ai first di beta allora dobbiamo aggiungere i follow di B ai follow di A perchè ciò che segue B potrà anche seguire A perchè se noi andiamo a considerare i sottoalberi che vengono generati andiamo a scoprire che A è la radice dell'utlimo sottoalbero della B e quindi quello che può seguire questa particolare occorrenza di B ... Si procede aggiungendo degli elementi ogni volta che andiamo a considerare delle partciolari istanze nella nostra grammatica e ci fermiamo fino a quando non possiamo inserire altro. 

\subsection{Esercizio first/follow 1}
Utilizziamo sempre la grammatica dell'esempio precedente, che qui riportiamo per comodità del lettore, e andiamone a definire i follow.
\begin{align*}
    \mathcal{G}: E &\rightarrow TE' \\
    E' &\rightarrow +TE' \mid \varepsilon \\
    T &\rightarrow FT' \\
    T' &\rightarrow *FT' \mid \varepsilon\\
    F &\rightarrow (E) \mid id
\end{align*}
\begin{enumerate}
    \item Dall'algoritmo sappiamo follow(\(E\)) = \$;
    \item iniziamo guardando la prima produzione, cioè \(E \rightarrow TE'\): se poniamo \(A = E'\), allora \(\beta = \varepsilon\) dunque devo aggiungere i follow(\(E\)) (quelli del driver) a follow(\(E'\)) (quelli del body);
    \item Sempre soffermandoci sulla stessa produzione poniamo \(A = T\), questo vuol dire che \(\beta = E' \neq \varepsilon\) e quindi, ricadendo nel primo caso, aggiungo first(\(E'\)) \(\setminus \{\varepsilon\} = \{+\}\) a follow(\(T\)), tuttavia, essendo che \(\varepsilon \in\) first(\(E'\)) allora ricadiamo anche nel secondo caso per cui aggiungiamo follow(\(E\)) a follow(\(T\));
    \item Passiamo ora alla produzione \(E' \rightarrow +TE' \mid \varepsilon\): poniamo quindi \(A = E'\) e, essendo che \(\beta = \varepsilon\) ricadiamo nel secondo caso dunque dobbiamo aggiungere follow(\(E'\)) a follow(\(E'\)): dato che questo non fornisce informazioni aggiuntive scartiamo questa informazione;
    \item Possiamo ora esaminare il caso per la produzione precedente, in cui \(A = T\) e quindi \(\beta = E'\), ma tale caso è già stato esaminato al punto 3;
    \item L'ultimo elemento che non abbiamo esaminato nella produzione è terminale \(+\) ma, in quanto terminale, non possiamo calcolarne i follow(\(+\));
    \item Continuiamo ad applicare l'algoritmo come visto nei punti precedenti per computare i follow delle produzioni rimanenti;
    \item Un caso che potrebbe risultare interessante riguarda la produzione \(F \rightarrow (E)\). Poniamo come fatto precedentemente \(A = E\) e, essendo che \(\beta = (\) \(\neq \varepsilon\), è necessario aggiungere first(\(\beta\)) \(\setminus \{\varepsilon\} = \{)\}\) a follow(\(E\)) e, visto che la seconda condizione non è vera, possiamo fermarci ed affermare che follow(\(E\)) = \{\$, )\}.
\end{enumerate}
Il risultato finale verrà rappresentato come segue:
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{computing-follow.tex}
    \caption{Esercizio sui follow, step intermedio}
    \label{computing-follow}
\end{table}

A questo punto è possibile eliminare mano a mano le dipendenze e le varie ripetizioni presenti all'interno delle computazioni dei follow, in modo da ottenere quanto segue: 

\begin{table}[H]
	\centering
	\subimport{assets/tables/}{follow.tex}
    \caption{Esercizio sui follow, risultato finale}
    \label{follow}
\end{table}

\subsection{Esercizio first/follow 2}
\label{first-folllow-ex-2}
Prendiamo ora in analisi la seguente grammatica:
\begin{align*}
       \mathcal{G}: S &\to aABb \\
       A &\to Ac \mid d \\
       B &\to CD \\
       C &\to e \mid \varepsilon \\
       D &\to f \mid \varepsilon
\end{align*}
Questa volta ripassiamo anche il calcolo dei first.
\begin{enumerate}
    \item Partiamo da \(S\): per calcolare i first di un non-terminale andiamo a vedere i first di tutte le sue produzioni; in particolare, so che qualunque stringa derivata da \(S\) inizierà con \(a\), e siccome \(a\) è un terminale ed è diverso da \(\varepsilon\), non proseguo oltre nella ricerca di first per \(S\);
    \item per calcolare i first di \(A\), invece, ho due produzioni da vagliare: la prima (che derivo da \(A \to Ac\)) mi dice che i first di \(A\) contengono anche i first dello stesso \(A\) (non aggiunge informazioni), la seconda mi dice che \(d\) può essere un first di \(A\), quindi scrivo \(\{d\}\);
    \item per calcolare i first di \(B\) devo conoscere quelli di \(C\) e \(D\), e dunque parto da \(C\): ottengo semplicemente che i first sono \(\{e\}\) e possibilmente \(\{\varepsilon\}\), quindi per \(C\) scrivo \(\{e, \varepsilon\}\);
    \item in modo simile a \(C\) posso facilmente ricavare che i first di \(D\) sono \(\{f, \varepsilon\}\);
    \item tornando ora ad analizzare \(B\) posso dire che i suoi first sono first(\(C\)) \( \setminus \{\varepsilon\}\), ma siccome \(C\) contiene \(\varepsilon\) tra i suoi first, allora devo aggiungere first(\(D\)) a first(\(B\));
    \item infine, notando che anche first(\(D\)) contiene \(\varepsilon\), posso annoverare in first(\(B\)) anche \(\varepsilon\) stesso.    
\end{enumerate}
Ora che abbiamo terminato la nostra ricerca dei first possiamo ricavare la tabella risolutiva dell'esercizio, che si presenta come in Tab. \ref{first-follow-ex-2_step-1}.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-2_step-1.tex}
    \caption{Esercizio \ref{first-folllow-ex-2} su first/follow, step 1}
    \label{first-follow-ex-2_step-1}
\end{table}
Passiamo ora gaiamente al prossimo step, ovvero il calcolo dei vari follow seguendo l'algoritmo indicato in \ref{follow-algorithm}.

Per comodità notazionale, dato che nelle produzioni dell'esercizio compare il non-terminale \(A\), scriveremo \(X\) per indicare la \(A\) dell'algoritmo per il calcolo del follow (assumiamo quindi che ogni \(A\) nell'algoritmo venga sostituita da \(X\); quindi, ad esempio, \(\alpha A \beta\) diventa \(\alpha X \beta\)).
\begin{enumerate}
    \item la fase di inizializzazione del calcolo dei follow vuole che lo start symbol abbia come follow il terminatore di stringa, per le ragioni che abbiamo già visto, quindi inseriamo subito \(\$\) per \(S\);
    \item cominciamo poi l'analisi dalle produzioni di \(S\):
    \begin{enumerate}
        \item osservando l'algoritmo, cerchiamo un match possibile per \(\alpha X \beta\); partiamo con il considerare \(X\) = \(A\) e quindi consideriamo \(\beta\) = \(Bb\);
        \item in questo caso abbiamo che \(\beta \neq \varepsilon\), quindi dobbiamo aggiungere i first(\(\beta\)) ai follow(\(A\));
        \item i first di \(\beta\) in questo caso sono i first di \(Bb\), quindi \(\{e, f, b\}\) (nota bene che non sono first(\(B\)) ma first(\(Bb\))); come indicato nella procedura, li aggiungo a follow(\(A\));
        \item \(\beta\) è diverso da \(\varepsilon\) ed il suo first non contiene \(\varepsilon\), quindi il secondo \texttt{if} non si applica e terminiamo questo branch;
        \item ora dobbiamo analizzare il caso in cui, studiando la produzione di \(S\), assumiamo \(X = B\) e quindi \(\beta = b\);
        \item in questo caso ho che devo aggiungere ai follow di \(B\) i first(\(b\)), ovvero \(\{b\}\);
        \item \(\beta\) è diverso da \(\varepsilon\) ed inoltre \(\varepsilon\) non appartiene ai first di \(\beta\), quindi termino anche questo branch.
    \end{enumerate}
    \item Ho concluso quindi l’analisi delle produzioni di \(S\) e passo quindi ad analizzare la produzione di \(A\):
    \begin{enumerate}
        \item parto da \(A \to Ac\): in questo caso non ho altra scelta che considerare \(X = A\), quindi \(\beta = c\) e cado quindi nel primo \texttt{if} dell'algoritmo, il quale mi dice di aggiungere first(\(c\)) \(\setminus \{\varepsilon\}\) (ovvero \(\{c\}\)) ai follow(\(A\));
        \item chiudo quindi il branch dato che il secondo \texttt{if} non si applica;
        \item considero ora la seconda produzione di \(A\), ovvero \(A \to d\); tuttavia, questa non è nella forma \(\alpha X \beta\) e quindi non ci dà informazioni, per cui l'algoritmo mi dice che posso passare oltre; ho terminato le produzioni di \(A\).
    \end{enumerate}
    \item Passiamo all'analisi della produzione \(B \to CD\):
    \begin{enumerate}
        \item considero \(X = C\) e ottengo quindi \(\beta = D\), e dato che \(\beta \neq \varepsilon\) aggiungo ai follow(\(C\)) i first(\(D\))\( \setminus \{\varepsilon\} = \{f\}\);
        \item però, dal momento che \(\varepsilon \in \) first(\(\beta\)), cado nel secondo \texttt{if} e devo quindi aggiungere i follow(\(B\)) (inteso come \(B\) dell'algoritmo) ai follow(\(X\)), il che significa aggiungere i follow(\(B\)) ai follow(\(C\)) nel nostro esercizio; lascio quindi un placeholder nella tabella da riscrivere quando avrò finito di calcolare follow(\(B\)), avendo raggiunto la fine del ciclo \texttt{while} chiudo qui il branch;
        \item ora considero la seconda opzione, ovvero \(X = D\), quindi \(\beta = \varepsilon\);
        \item in questo caso andiamo direttamente nel secondo \texttt{if} che ci dice di aggiungere i follow(\(B\)) ai follow(\(D\)); nella tabella, quindi, indico un altro placeholder;
        \item abbiamo terminato le possibili interpretazioni delle produzioni per la derivazione \(B \to CD\).
    \end{enumerate}
    \item Le produzioni di \(C\), come quelle di \(D\), non sono del tipo \(\alpha X \beta\), dato che non contengono caratteri non-terminali; queste produzioni non ci danno informazioni sui follow e l’algoritmo ci dice pertanto di ignorarle;
    \item a questo punto abbiamo terminato le produzioni da analizzare, non ci resta altro che risolvere i placeholder che abbiamo lasciato nella tabella durante i passi precedenti.
\end{enumerate}
Osserviamo in Tab.\ref{first-follow-ex-2_step-2} come risulta la tabella prima della sostituzione dei placeholder
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-2_step-2.tex}
    \caption{Esercizio \ref{first-folllow-ex-2} su first/follow con i placeholder}
    \label{first-follow-ex-2_step-2}
\end{table}
Mentre in Tab.\ref{first-follow-ex-2_step-3} si può osservare il risultato finale, con sostituzione dei placeholder.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-2_step-3.tex}
    \caption{Esercizio \ref{first-folllow-ex-2} su first/follow una volta sostituiti i placeholder}
    \label{first-follow-ex-2_step-3}
\end{table}

\subsection{Esercizio first/follow 3}
\label{first-folllow-ex-3}
Passiamo ora ad un'altro esercizio dello stesso tipo, sviluppato a partire dalla seguente grammatica:
\begin{align*}
    \mathcal{G}: S &\to aA \mid bBc \\
    A &\to Bd \mid Cc \\
    B &\to e \mid \varepsilon \\
    C &\to f \mid \varepsilon
\end{align*}
Questa volta saremo più spigliati con la risoluzione dei first, ma li scriveremo comunque, dato che dobbiamo tener ben presente questa massima:
\begin{displayquote}
    "I first ed i follow li dovete sapere bene perché noi, con questi, ci faremo gli spaghetti."
    ~Paola Quaglia
\end{displayquote}

\noindent Andiamo quindi, senza ulteriori indugi, a risolvere i first dei vari non-terminali.

Per comodità notazionale, dato che nelle produzioni dell'esercizio compare il non-terminale \(A\), anche in quest'esercizio scriveremo \(X\) per indicare la \(A\) dell'algoritmo per il calcolo del follow, come nel caso precedente.
\begin{enumerate}
    \item Per \(S\) inseriamo \(\{a, b\}\) e terminiamo subito, dato che entrambi sono terminali;
    \item per \(A\) dobbiamo conoscere i first di \(B\) e \(C\);
    \item per \(B\) abbiamo che i first sono \(\{e, \varepsilon\}\);
    \item per \(C\) abbiamo che i first sono \(\{f, \varepsilon\}\);
    \item infine torniamo a risolvere \(A\):
    \begin{itemize}
        \item analizziamo \(A \to Bd\): inseriamo first(\(B\)) \(\setminus \{\varepsilon\}\), ovvero inseriamo \(\{e\}\);
        \item siccome first(\(B\)) contiene \(\varepsilon\), aggiungiamo anche i first di \(b\), che sono proprio \(\{b\}\);
        \item analizziam poi \(A \to Cc\);
        \item esattamente come per \(A \to Bd\), abbiamo che i first sono \(\{f, c\}\);
    \end{itemize}
\end{enumerate}
Una volta inseriti i first nella tabella ci troveremo nella situazione rappresentata in Tab.\ref{first-follow-ex-3_step-1}.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-3_step-1.tex}
    \caption{Esercizio \ref{first-folllow-ex-2} su first/follow, step 1}
    \label{first-follow-ex-3_step-1}
\end{table}
Ora è il momento di tuffarci a capofitto nel calcolo dei follow; anche in questo caso tenterò di matentere la spiegazione concisa ma completa. Prima di tuffarci nei calcoli, è interessante osservare più da vicino qual è l’idea sottesa all'inserimento di \(\$\) in follow(\(S\)) come primo passo.

Questa azione risulta intuitiva se si pensa che follow(\(Z\)) rappresenta quello che io mi aspetto di poter derivare da un certo simbolo non-terminale \(Z\), con un certo numero di passi; quindi si capisce che, essendo \(S\) l’origine di tutte le parole generate da una certa grammatica, mi aspetto che, una volta analizzato tutto ciò che è generato da \(S\), troverò il terminatore di stringa, ovvero proprio \(\$\).

\noindent Bene, ora siamo pronti a fare a pugni coi calcoli.
\begin{enumerate}
    \item Partiamo con l’analizzare le produzioni di \(S\), iniziando con \(S \to aA\);
    \begin{enumerate}
        \item possiamo considerare solo \(X = A\), quindi per forza di cose avremo che \(\beta = \varepsilon\);
        \item passiamo subito al secondo \texttt{if}, che ci dice di ricordare di aggiungere i follow(\(S\)) a follow(\(A\)); lasciamo quindi un placeholder nella tabella e proseguiamo.
    \end{enumerate}
    \item Analizziamo ora \(S \to bBc\);
    \begin{enumerate}
        \item in questo caso siamo obbligati a scegliere \(X = B\) e quindi avremo che \(\beta = c\);
        \item passiamo dentro al primo \texttt{if} e aggiungiamo first(\(c\)) (\(=\{c\}\)) a follow(\(B\));
        \item il secondo if non si applica, quindi terminiamo.
    \end{enumerate}
    \item Analizziamo ora \(A \to Bb\);
    \begin{enumerate}
        \item dobbiamo scegliere \(X = B\), quindi abbiamo che \(\beta = d\);
        \item anche questa volta ci ritroviamo a soddisfare il primo \texttt{if} ed aggiungere first(\(d\)) (\(=\{d\}\)) a follow(\(B\));
        \item il secondo if non si applica, terminiamo il branch.
    \end{enumerate}
    \item Analizziamo \(A \to Cc\);
    \begin{enumerate}
        \item in questo caso \(X = C\), quindi troviamo che \(\beta = c\);
        \item come per il caso appena visto, aggiungiamo first(\(c\)) (\(=\{c\}\)) a follow(\(C\)) e chiudiamo il branch.
    \end{enumerate}
    \item Infine, tutte le rimanenti produzioni non sono nella forma \(\alpha X \beta\), quindi possiamo ragionevolmente diredi aver terminato l’analisi;
    \item l'ultimo passo che rimane da svolgere è la sostituzione dei placeholder.
\end{enumerate}
Osserviamo quindi in Tab.\ref{first-follow-ex-3_step-2}, come risulta la tabella dell' Es.\ref{first-folllow-ex-3} prima della sostituzione dei placeholder.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-3_step-2.tex}
    \caption{Esercizio \ref{first-folllow-ex-3} su first/follow con i placeholder}
    \label{first-follow-ex-3_step-2}
\end{table}
Quindi, in Tab.\ref{first-follow-ex-3_step-3}, ammiriamo la soluzione finale dell' Es.\ref{first-folllow-ex-3}, con sostituzione dei placeholder.
\begin{table}[H]
	\centering
	\subimport{assets/tables/}{first-follow-ex-3_step-3.tex}
    \caption{Esercizio \ref{first-folllow-ex-3} su first/follow una volta sostituiti i placeholder}
    \label{first-follow-ex-3_step-3}
\end{table}

\section{Costruire una parsing table}
Ora che ci siamo esercitati con i meccanismi necessari, possiamo tornare ad occuparci della nostra preoccupazione principale, ovvero come costruire una tabella di parsing.

\subsection{Algoritmo per la costruzione di una parsing table}
Rappresentato in Fig.\ref{algoritmo_parsing-table} si può osservare l'algoritmo che, data in input una grammatica \(\mathcal{G}\), ritorna la parsing table propria di quella grammatica. 
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\textwidth]{algoritmo_parsing-table.png}
    \caption{Algoritmo di costruzione dei una parsing table}
    \label{algoritmo_parsing-table}
\end{figure}
Ricordiamo brevemente che le tabelle di parsing ci servono per verificare se una certa parola può essere derivata tramite derivazione canonica da una data grammatica.

L'algoritmo prevede di scorrere tutte le produzioni \(A \to \alpha\) presenti nella grammatica e, per ognuna di queste:
\begin{enumerate}
    \item aggiungere \(A \to \alpha\) in \(M[A, b]\) per ogni \(b\) in first(\(\alpha\));
    \item se  \(\varepsilon \in\) first(\(\alpha\)), aggiungere \(A \to \alpha\) a \(M[A, x]\) per tutti gli \(x\) in follow(\(A\)). 
\end{enumerate}
Una volta terminato questo ciclo, si va a settare il valore \(error()\) in tutte le entry della tabella ancora vuote.

Osserva che follow(\(A\)) può contenere il simbolo \(\$\), ed è questo il motivo per cui nel punto due usiamo \(x\) invece che \(b\); infatti, mentre \(b\) indica terminali, \(x\) serve proprio a far notare che potrebbe esserci \(\$\), che non è un terminale della grammatica, ma il carattere segnalatore della terminazione di una parola.

Alcune note interessanti:
\begin{itemize}
    \item nel nostro caso le caselle di errore saranno lasciate vuote, mentre nell’applicazione effettiva le caselle vuote puntano tutte a routine di errore;
    \item ripetiamo che è possibile finire con l'avere con due entry in una stessa cella; tuttavia, le grammatiche che compongono tabelle con questa forma non sono deterministiche e non appartengono alla classe LL(1), che è la lasse di cui ci interessiamo noi;
    \item le entry multiple nella tabella di parsing si dicono multiply-defined.
\end{itemize}

\subsection{Applicazioni}
Viene proposta ora come esempio al lettore questa grammatica.
\begin{align*}
    \label{non-ll1_grammar}
    \mathcal{G}: E &\to E+T \mid T \\
    T &\to T*F \mid F \nonumber \\
    F &\to (E) \mid id \nonumber 
\end{align*}
Saprebbe dire il lettore se la grammatica qui presentata appartiene alla classe LL(1)?

Mentre il lettore riflette su questo quesito può leggere un'altra importante citazione del nostro punto di riferimento, Paola Quaglia, la quale riflette sul senso di incomunicabilità che attraversa i tempi moderni, quel conflitto generazionale di cui tutti, in certo momento della nostra vita e in un certo schieramento, siamo stati attori.
\begin{displayquote}
    "NWY5YTliMmVjMmRhNiwyOS8xMC8yMDIwIDExOjUxLGthbHQxMA==  kaltura ci parla cosi' :-("
\end{displayquote}

Ebbene, per rispondere alla domanda posta in precedenza la grammatica in \ref{non-ll1_grammar} non appartiene alla classe LL(1) e lo si può dimostrare provando a crearne la parsing table.

Si può notare di fatto come first(\(E\)) = first(\(T\)) = first(\(F\)) \(= \{(, id\}\).
Di conseguenza, se seguiamo i passi dell'algoritmo di creazione della parsing table, otteniamo una situazione in cui la casella \(M[E, id]\) contiene sia \(E \to E+T\) che \(E \to T\).

Questa grammatica presenta anche una proprietà (o difetto) molto interessante, chiamata ricorsione sinistra:
\begin{definition}
    Una grammatica presenta ricorsione sinistra (left recursive grammar) se, per qualche \(A\) e \(\alpha\), \(A \Rightarrow^* A\alpha\)
\end{definition}

Se osserviamo la nostra grammatica, ci rendiamo conto immediatamente che presenta ricorsione sinistra, dato che possiamo espandere un numero indefinito di volte la produzione \(E \to E+t\) ed ottenere una sequenza di derivazioni come la seguente:
\begin{equation*}
    E \Rightarrow E+T \Rightarrow E+T+T \Rightarrow E+T+T+T \Rightarrow \dots
\end{equation*}
La grammatica che abbiamo analizzato presenta, per la precisione, la caratteristica di ricorsione immediata sinistra (\emph{immediately left recursive grammar}), ovvero presenta una produzione in forma \(A \to A\alpha\).

Presentiamo ora un lemma sulle grammatiche con ricorsione sinistra.
\begin{lemma}
    Se una grammatica \(\mathcal{G}\) presenta ricorsione sinistra, immediata o no che sia, allora la tal grammatica \(\mathcal{G}\) \emph{non} appartiene alla classe LL(1).
\end{lemma}
Una volta venuti a conoscenza di questo lemma, è lecito chiedersi se tali grammatiche proprio non siano riducibili a grammatiche di tipo LL(1) per fare in modo da poterle analizzare in maniera deterministica (e di conseguenza più efficiente).

In molti casi è effettivamente possibile eliminare la ricorsione sinistra. 
Proviamo a capire l'intuizione da seguire per ottenere una grammatica LL(1) da una con ricorsione sinistra; per farlo, aiutiamoci tentando di risolvere tale problema per la seguente grammatica d'esempio:
\begin{equation*}
    \label{left-recursive_grammar}
    A \to A \alpha \mid \beta \textrm{  con  } \alpha \neq \varepsilon \textrm{  e  } \beta \neq A \gamma
\end{equation*}
Di fatto se valesse \(\alpha = \varepsilon\) la grammatica sarebbe solo scritta male e non sarebbe quindi left recursive.

Lasciamo al lettore un po' di tempo per pensare ad una grammatica LL(1), o quantomeno non left recursive, che generi lo stesso linguaggio della grammatica in Eq.\ref{left-recursive_grammar}.



\end{document}