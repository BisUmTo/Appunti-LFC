\documentclass[class=book, crop=false, oneside, 12pt]{standalone}
\usepackage{standalone}
\usepackage{../../style}
\usepackage[normalem]{ulem}
\graphicspath{{./assets/images/}}

% arara: pdflatex: { synctex: yes, shell: yes }
% arara: latexmk: { clean: partial }
\begin{document}
\chapter{Analisi sintattica: bottom-up parsing}
Iniziamo la trattazione del parsing di tipo \emph{bottom-up}: come suggerisce il nome stesso, consiste nel ricostruire le derivazioni di una parola in ordine inverso, partendo dall'ultima produzione usata per derivare la parola considerata, fino ad arrivare allo start symbol; a livello visivo, possiamo pensare che la nostra intenzione è di costruire un albero di derivazione partendo dalle foglie e arrivando alla radice.

\section{Classi di grammatiche}
Possiamo subito anticipare che anche per questo approccio possiamo identificare diverse classi di grammatiche, ciascuna delle quali ci permetterà di utilizzare, di volta in volta, diverse tecniche di parsing; in ogni caso, ciascuna di queste classi condivide le seguenti caratterstiche:
\begin{itemize}
    \item per qualsiasi grammatica \(\G\) considerata, andremo sempre a espandere il suo insieme \(\P\) in \(\P'\) aggiungendo la produzione \(S \to S'\), dove \(S'\) è un non-terminale \emph{fresh};
    \item utilizzano i medesimi algoritmi \emph{shift} e \emph{reduce} (ne parleremo più avanti);
    \item hanno sempre un automa a stati finiti, detto \emph{automa caratteristico}, il cui ruolo è di supervisionare il funzionamento dell'algoritmo di parsing.
\end{itemize}
Dipendentemente dalla classe di grammatica considerata, avremo automi caratteristici che rappresentano le informazioni in maniera più o meno dettagliata. Maggiore è il livello di dettaglio dell'informazione, più diventa grande e complesso l'automa caratteristico, ma anche più potente diventa il nostro parsing, inteso come numero di diverse grammatiche che può analizzare. Impariamo a conoscere qual è il significato delle abbreviazioni che costituiscono quei nomi un po' criptici delle classi di grammatiche:
\begin{labeling}{LA}
    \item[L] left: leggiamo l'input da sinistra;
    \item[R] ricostruiamo una derivazione rightmost;
    \item[1] andiamo a considerare un simbolo alla volta;
    \item[LA] sta per look ahead;
    \item[S] sta per simple;  
\end{labeling}
Noi diciamo che una grammatica \(\G\) appartiene a una certa classe di grammatiche, ad esempio LR(1), se siamo capaci di costruire una tabella di parsing di tipo LR(1) per \(\G\) (o di qualsiasi altro tipo stiamo considerando).

Tra quelli presentati, il meccanismo più potente è LR(1), complementare di LL(1); per questo motivo noi cercheremo sempre di costruire una tabella di parsing deterministico che rientri nei vincoli di LR(1). Se questo non sarà possibile andremo a scalare in complessità con LALR e SRL.

\section{Un primo esempio di applicazione}
\subsection{Mosse di shift e reduce}
Andiamo a introdurre l'algoritmo che utilizzeremo per verificare se una certa parola appartenga o meno al linguaggio denotato da una certa grammatica, rappresentata dal suo automa caratteristico; questo è detto algoritmo di shift/reduce, dal nome delle due mosse che andremo a utilizzare. Come prima cosa, prendiamo familiriatà con le due pile che utilizzeremo nella procedura:
\begin{itemize}
    \item nella prima inseriamo gli stati verso cui ci muoviamo;
    \item nella seconda conserviamo la derivazione parziale a cui siamo arrivati sinora.
\end{itemize}
Si tenga presente che in realtà potremmo farci bastare anche una sola pila, ma andrebbe a complicare sensibilmente la gestione della procedura.

Adesso che conosciamo le strutture necessarie alla procedura, andiamo a vedere le due mosse sopra menzionate:
\begin{enumerate}
    \item la mossa di \emph{shift} è quella che compiamo quando passiamo da un nodo (stato) all'altro, inserendo nella pila delle derivazioni parziali il terminale che marca l'arco attraversato e nella pila degli stati il nodo di destinazione;
    \item la mossa di \emph{reduce} è quella che eseguiamo quando raggiungiamo un nodo etichettato da una formula di riduzione (capiremo nell'esempio quale forma hanno) e che ci porta a eliminare dei terminali dalla pila delle derivazioni parziali e degli stati dalla pila degli stati, coerentemente alla struttura dell'automa caratteristico.
\end{enumerate}
Consideriamo come esempio una delle prime grammatiche che abbiamo visto, quella che genera due occorrenze bilanciate:
\begin{equation}
    \label{balanced}
    \G: S \to aSb \mid ab
\end{equation}
\subsection{Esempio di automa}
L'automa caratterisco di tipo LR(1) per questa grammatica è il seguente:
\begin{figure}[H]
    \centering
	\subimport{assets/figures/}{automa_LR_8-1.tex}
    \caption{Automa caratteristico LR(1) per Eq. \ref{balanced}}
    \label{balanced-char_aut-lr1}
\end{figure}
Lo utilizzeremo come guida per determinare, di volta in volta, quali mosse di shift e reduce applicare per verificare se una certa parola appartiene o no al linguaggio generato da \(\G\). 

\subsection{Procedura}
Consideriamo ad esempio la parola \(w = aaabbb\). Come prima cosa le applichiamo il carattere terminatore di stringa \(aaabbb\$\). 
\begin{itemize}
    \item Partiamo dallo stato \(0\) e inseriamolo nella pila degli stati;
    \item il primo simbolo che leggiamo in \(w\) è \(a\); vediamo che l'automa presenta una \(a\)-transizione verso lo stato \(2\), per cui la seguiamo, inseriamo lo stato \(2\) nella pila, passiamo oltre al simbolo \(a\) appena "consumato" e passiamo al prossimo simbolo;
    \item il prossimo simbolo è ancora \(a\); di nuovo, seguiamo la \(a\)-transizione verso lo stato \(5\), lo inseriamo nella pila degli stati, passiamo oltre al simbolo consumato e andiamo avanti;
    \item abbiamo una terza occorrenza di \(a\) e abbiamo una \(a\)-transizione in forma di self loop in \(5\), che andiamo ad eseguire, reinserendo \(5\) nella pila degli stati passando oltre alla nostra terza \(a\);
    \item troviamo quindi una \(b\), per cui ci spostiamo allo stato \(8\), il quale ha un'etichetta rossa che riporta un passo di riduzione in forma \(S \to ab, \{b\}\); questo sta a indicare che, se in lettura troviamo \(b\), contenuto nel set \(\{b\}\), possiamo ritornare indietro di due passi, eliminando i due precedenti stati dalla pila e spostarci direttamento dal primo \(5\) a \(7\), dal momento che i due stati sono collegati da una \(S\)-transizione:
    \begin{align*}
        \textrm{pila degli stati prima:} &\quad 02558 \\
        \textrm{pila degli stati dopo:} &\quad 0257 
    \end{align*}
    inoltre dobbiamo anche rimuovere gli ultimi due simboli \(ab\) - il body della produzione della riduzione - dalla pila della derivazione e sostituirli con \(S\):
    \begin{align*}
        \textrm{pila di derivazione prima:} &\quad \#aaab \\
        \textrm{pila di derivazione dopo:} &\quad \#aaS 
    \end{align*}
    \item leggiamo un'altra \(b\) e avanziamo allo stato \(9\), e anche qui operiamo un passo di riduzione (reduce), nello specifico abbiamo che \(R: S \to aSb, \{b\}\); questo ci dice che dobbiamo tornare indietro di tre passi, eliminando i tre elementi precedenti sia nella pila degli stati e muovendoci verso \(3\), sostituendo nella pila delle derivazioni il body della riduzione con il driver; si osservi attentamente il cambiamento delle pile per capire cosa succede:
    \begin{align*}
        \textrm{pila degli stati prima:} &\quad 02579 & \textrm{pila di derivazione prima:} &\quad \#aaSb \\
        \textrm{pila degli stati dopo:} &\quad 023 & \textrm{pila di derivazione dopo:} &\quad \#aS
    \end{align*}
    \item proseguiamo quindi con la lettura e incontriamo una terza \(b\), ci muoviamo verso \(6\) e incontriamo una terza riduzione \(R: S \to aSb, \{\$\}\); di nuovo, torniamo indietro di tre stati e contestualmente sostituiamo gli elementi nelle pile: 
    \begin{align*}
        \textrm{pila degli stati prima:} &\quad 0236 & \textrm{pila di derivazione prima:} &\quad \#aSb \\
        \textrm{pila degli stati dopo:} &\quad 0 & \textrm{pila di derivazione dopo:} &\quad \#S
    \end{align*}
    \item abbiamo terminato: ci troviamo nello stato \(0\) e troviamo solamente il nostro start symbol \(S\), che ci permette  muoverci verso lo stato \(1\), e l'endmaker \$; la presenza della keyword \(Accept\) nello stato in cui abbiamo terminato ci indica che la parola è stata riconosciuta dall'automa.
\end{itemize}

\subsection{Riassumendo}
Questo è un esempio del procedimento dell'algoritmo di shift/reduce; vediamo quali regole generali possiamo dedurne:
\begin{itemize}
    \item partendo dallo stato iniziale, iniziamo a leggere la parola data attraversando gli archi marchiati dalle \(symbol\)-transizioni che incontriamo di volta in volta;
    \item quando arriviamo in un nodo in cui si dovrà effettuare un passo di riduzione, questo sarà marcato da un'etichetta che avrà la forma \(A \to B, \{l\}\); a questo punto, dovremo:
    \begin{itemize}
        \item eliminare dalla cima della pila della derivazione il body della riduzione;
        \item mettere al suo posto il driver della riduzione
        \item eliminare dalla pila degli stati tanti stati quanti i caratteri nel body della derivazione;
        \item ritornare nello stato che si trova ora in cima alla pila degli stati;
        \item da qui, effettuare una \(A\) transizione;
        \item inserire lo stato in cui siamo giunti tramite la \(A\) transizione nella pila degli stati. 
    \end{itemize}
\end{itemize}
Gli automi caratteristici sono una rappresentazione utile, ma si tenga presente che la stessa funzione può essere ottemperata anche da una tabella.

\section{Costruzione dell'automa}
Dopo aver visto questo esempio, non possiamo fare a meno di domandarci com'è possibile per noi calcolare questo automa caratteristico e la relativa tabella di parsing.

\subsection{Stati}
Andiamo per prima cosa a indagare gli stati. Gli stati sono degli insiemi di items, dove gli items sono oggetti di diverso tipo dipendentemente dalla tecnica di parsing utilizzata:
\begin{labeling}{LR(1)-items}
    \item[LR(0)-items] \(A \to \alpha \cdot \beta\)
    \item[LR(1)-items] [\(A \to \alpha \cdot \beta, L\)], dove \(L \subseteq T \cup \{\$\}\)\footnote{Ricordiamo che \(T\) è l'insieme dei terminali della grammatica considerata.}
\end{labeling}
Da qui capiamo finalmente cosa intendevamo quando prima abbiamo detto che gli LR(1)-items sono più ricchi dei loro corrispondenti LR(0), e ci permettono quindi di riconoscere grammatiche in modo più efficace. Coerentemente alla definzione sopra riportata, avremo che gli automi caratteristici LR(0) sono quelli i cui stati sono LR(0)-items, e analogamente per gli automi carattersitici LR(1).

Andiamo a fare subito un esempio per aiutarci a capire di cosa stiamo parlando. Consideriamo un LR(0)-item \(S' \to \cdot S\): il significato intuitivo è che, se siamo all'inizio della procedura di parsing, siamo in una posizione in cui vogliamo conoscere quali sono le parole derivabili a partire da \(S\), per cui è logico pensare che il nostro item \(S' \to \cdot S\) debba stare nello stato iniziale dell'automa, che chiamiamo \(P_0\).

Ma non sarà l'unico item a risiedere in \(P_0\). Per aiutarci, riprendiamo la nostra fidata grammatica in Eq. \ref{balanced}: in questo caso, analizzare la parola vuol dire aspettarsi qualcosa che derivi da \(aSb\) oppure \(ab\), per cui gli LR(0)-items di \(S\) saranno:
\begin{itemize}
    \item \(S \to \cdot aSb\);
    \item \(S \to \cdot ab\).
\end{itemize}
E verranno inseriti in \(P_0\).

\subsection{Chiusura di un insieme di LR(0)-items}
Andiamo ora a vedere il concetto di chiusura di un insieme di LR(0)-items:

\begin{definition}
    Sia \(P\) un insieme di LR(0)-items; allora, \(closure_0(P)\) è il più piccolo insieme che soddisfa la seguente equazione:
    \begin{equation}
        closure_0(P) = P \cup \{B \to \cdot \gamma \; \mid \; A \to \alpha \cdot B \beta \in closure_0(P) \; \textrm{e} \; B \to \gamma \in P'\}
    \end{equation}
\end{definition}

La chiusura consiste sostanzialmente nell'aggiungere, per tutti quegli items che hanno un punto davanti ad un non-terminale, tutte le derivazioni possibili di quel non-terminale; questo va applicato ricorsivamente fino a che non sono presenti tutte le chiusure delle produizioni con un punto davanti ad un non-terminale. Più facile a farsi che a dirsi, credetemi.

\subsection{Esempio di calcolo della chiusura}
Prendiamo ora ad esempio la seguente grammatica:
\begin{align*}
    E &\to E+T \mid T\\
    T &\to T*F \mid F\\
    F &\to (E) \mid id
\end{align*}
Come primo passo, consideriamo l'item \(E \to \cdot E\) e calcoliamone la chiusura \(closure_0(\{E' \to \cdot E\})\). Ecco il procedimento passo per passo:
\begin{enumerate}
    \item inizializziamo \(closure_0(\{E' \to \cdot E\})\) = \(\{E' \to \cdot E\}\); 
    \item andiamo a vedere se questo insieme contiene qualche marker davanti ad un non-terminale: effettivamente, c'è un marker prima di \(E\);
    \item aggiungiamo quindi le due produzioni di \(E\) alla chiusura \(\{E \to \cdot E+T\}\) e \(\{E \to \cdot T\}\);
    \item una volta aggiunte queste produzioni, vediamo ricorsivamente se si presentano altre situazioni con \(\cdot\) prima di un non-terminale;
    \item nel primo caso troviamo ancora \(\cdot E\), però abbiamo già analizzato la produzione \(E\), per cui possiamo passare oltre;
    \item nel secondo caso invece abbiamo \(\cdot T\) e non abbiamo ancora analizzato tutte le produzioni di \(T\), quindi andiamo ad aggiungere all'insieme le produzioni di \(T\);
    \item aggiungamo \(\{T \to \cdot T * F\}\) e \(\{T \to \cdot F\}\);
    \item ci troviamo di nuovo in un caso in cui abbiamo due nuove derivazioni con \(\cdot\) davanti
    a un non-terminale, ma \(\cdot T\) è già analizzato, per cui analizziamo solo \(\cdot F\);
    \item aggiungiamo le produzioni di \(F\): \(\{F \to \cdot (E)\}\) e \(\{F \to \cdot id \}\);
    \item siamo arrivati alla conclusione, e qui sotto è riportato l'insieme chiusura che abbiamo trovato:
\end{enumerate}
\begin{align*}
    E' &\to \cdot E \\
    E  &\to \cdot E+T \\
    E  &\to \cdot T \\
    T  &\to \cdot T * F \\
    T  &\to \cdot F \\
    F  &\to \cdot (E) \\
    F  &\to \cdot id
\end{align*}

Ora che abbiamo visto un applicazione otteniamo lo pseudocodice dell'algoritmo per la computazione della chiusura:
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{bottom-up-parsing_closure_algorithm.png}
    \caption{Algoritmo per il calcolo di \(closure_0(P)\)}
\end{figure}
\subimport{assets/pseudocode/}{lr0-closure.tex}

\subsection{Costruire un automa caratteristico per il parsing LR(0)}
Vedremo ora come costruire un automa caratteristico per il parsing di tipo LR(0) data una certa grammatica. La tecnica di costruzione è incrementale: andiamo a popolare un set di stati definendo mano a mano la funzione di transizione, fino a saturazione; il lettore si accorgerà che tale tecnica è poi utilizzata per costruire anche altri automi LR.

\paragraph{Inizio}
In primis, definiamo il kernel dello stato iniziale come \(P_0 = \{S' \to \cdot S\}\), dove \(S'\) è un carattere inserito da noi, mentre \(S\) è lo start symbol della nostra grammatica.

\paragraph{Svolgimento}
Fino a che non esauriamo gli stati ancora da visitare andiamo a prenderli uno alla volta e li analizziamo nel segiente modo:
\begin{enumerate}
    \item calcoliamo il la chiusura del kernel dello stato, questo insieme rappresenta tutte le produzioni che si possono compiere da un certo stato per transitare verso altri stati;
    \item una volta calcolata la chiusura del kernel, le produzioni (gli item) che abbiamo già collezionato avranno forma \(A \to \alpha \cdot x \beta\), il che significa che nello stato in cui mi trovo, diciamo \(P\), ho già visto \(\alpha\) e posso fare una transizione tramite \(x\);
    \item esiste quindi una transizione da \(P\) a uno stato \(P'\) attraverso l'item \(A \to \alpha x \cdot \beta\); se \(x\) è un terminale, tale transizione rappresenta un'operazione di shift (come abbiamo visto nell'esercizio della sezione scorsa). Questo significa che avrò una transizione etichettata con \(x\) che mi porta da \(P\) a \(P'\);
    \item a questo punto genero il nuovo stato \(P'\) che contiene come kernel l'item \(A \to \alpha x \cdot \beta\), ricordando che poi andrò a includere tra gli item di \(P'\) anche gli item che appartengono a \(closure_0 (\{ A \to \alpha x \cdot \beta \})\), poiché se \(\beta\) è un non-terminale allora mi aspetto di poter trovare in \(P'\) anche tutto ciò che deriva da \(\beta\).
\end{enumerate}
C'è però una nota da aggiungere a questo procedimento: può accadere che quando generiamo un nuovo stato \(P'\) per transizione da uno stato \(P\) ci rendiamo conto che il kernel di questo stato corrisponde al kernel di un altro stato che abbiamo già vistisato, diciamo \(Q\); in questo caso invece che creare un nuovo stato \(P'\), quello che facciamo è collegare (tramite una \(x\)-transizione) \(P\) a \(Q\).

\subsubsection{Esempio costruzione automa caratteristico LR(0)}
\label{subsubsec:esercizio_costruzione_automa_lr0}
Per consolidare la procedura sopra illustrata, andiamo subito a mettere le mani su un esempio di costruzione di un automa caratteristico per il parsing LR(0), in particolare per la seguente grammatica:
\begin{align*}
    S &\to aABe \\
    A &\to Abc \mid b \\
    B &\to d
\end{align*}

\paragraph{Inizio}
Partiamo creando lo stato iniziale, lo stato \(0\):
\begin{equation*}
    S' \to \cdot S
\end{equation*}
Ma c'è di più: nello stato iniziale va inserita anche la \(closure_0(\{ S' \to \cdot S \})\), quindi aggiungo le produzioni di \(S\): \(S \to \cdot aABe\). Dato che non sono presenti altre produzioni con marker prima di caratteri non-terminali, la chiusura dello stato \(0\) è completa.

\paragraph{Svolgimento}
A questo punto ci troviamo nello stato \(0\) ed abbiamo due produzioni, una con il marker prima di \(S\) ed una con il marker prima di \(a\), per cui dobbiamo aggiungere le seguenti transizioni:
\begin{enumerate}
    \item \(\tau (0, S) = 1\)\footnote{Nota: gli stati possono venire indicati con la notazione: \(\tau( \textrm{stato di provenienza}, \textrm{transizione di provenienza})\).}, ovvero una transizione che sta per "sono nello stato \(0\) e leggo \(S\) nell'input buffer";
    \item \(\tau (0, a) = 2\), ovvero una transizione che sta per "sono nello stato \(0\) e leggo \(a\) nell'input buffer".
\end{enumerate}
Questi stati però potrebbero essere già presenti! Non è questo il caso dato che sono i primi stati che troviamo, ma in seguito dovremmo ricordarci di tale controllo. \\

\begin{itemize}
    \item Partiamo con l'analizzare il nuovo stato \(\tau (0, S) =\) 1. \\
    Dobbiamo calcolare il \emph{kernel} dello stato, che si ottiene spostando il marker oltre al carattere che ci ha portati qui:
    \begin{equation*}
        S' \to S \cdot
    \end{equation*}
    Questo è quello che viene definito kernel dello stato; non presenta ulteriori transizioni possibili dato che il marker è arrivato in fondo, quindi passiamo ad analizzare un altro stato. 
    \item Andiamo ad analizzare lo \(\tau (0, a) =\) 2. \\
    Il kernel questa volta è:
    \begin{equation*}
        S \to a \cdot ABe
    \end{equation*}
    Dato che il kernel presenta almeno una produzione con un non-terminale alla destra del marker, dobbiamo aggiungere la chiusura del kernel a questo stato, ovvero:
    \begin{align*}
        A &\to \cdot Abc\\
        A &\to \cdot b
    \end{align*}
    Quindi gli item dello stato 2 sono:
    \begin{align*}
        S &\to a \cdot ABe\\
        A &\to \cdot Abc\\
        A &\to \cdot b
    \end{align*}
    Dallo stato 2 avremo quindi due possibili transizioni, una tramite \(A\) ed una tramite \(b\).
    \item Partiamo ad analizzare \(\tau (2, A) = 3\). \\
    Il kernel di questo stato è composto da due produzioni, dato che da \(2\) si può arrivare in \(3\) tramite due distinte produzioni:
    \begin{align*}
        S &\to aA \cdot Be \\
        A &\to A \cdot bc
    \end{align*}
    Per verificare se questo stato è già stato raggiunto, vado a verificare che non siano presenti stati con gli stessi item; in questo casp non ce ne sono, per cui lo stato \(3\) non è ancora stato effettivamente aggiunto e quindi lo tengo. Calcoliamo la chiusura dello stato, che ci porta ad aggiungere la seguente produzione:
    \begin{equation*}
        B \to \cdot d
    \end{equation*}
    Una volta calcolata la chiusura, mi segno i nuovi stati da visitare.
    \begin{itemize}
        \item \(\tau (3, B) = 5\)
        \item \(\tau (3, b) = 6\)
        \item \(\tau (3, d) = 7\)
    \end{itemize}
    \item Analizziamo ora lo stato \(\tau (2, b) = 4\). \\
    Questo stato ha come kernel:
    \begin{equation*}
        A \to b \cdot
    \end{equation*}
    e non presenta ulteriori possibili sviluppi, quindi passiamo oltre.
    \item Analizziamo lo stato \(\tau (3, B) = 5\). \\
    Il kernel in questo caso è:
    \begin{equation*}
        S \to aAB \cdot e
    \end{equation*}
    questo kernel è già chiuso (la sua chiusura, infatti, è un insieme vuoto) e ci offre come unica transizione possibile \(\tau (5, e) = 8\).
    \item Analizziamo lo stato \(\tau (3, B) = 6\). \\
    Il kernel in questo caso è:
    \begin{equation*}
        A \to Ab \cdot c
    \end{equation*}
    Anche questo kernel è già chiuso; ci offre la transizione allo stato \(\tau (6, c) = 9\).
    \item Analizziamo lo stato \(\tau (3, d) = 7\). \\
    Questo stato ha come kernel:
    \begin{equation*}
        B \to d \cdot 
    \end{equation*}
    Tale kernel è chiuso e non presnta transizioni uscenti.
    \item Non ci rimane che analizzare gli stati \(8\) e \(9\) che presentano rispettivamente i seguenti kernel:
    \begin{align*}
        S &\to aABe \cdot \\
        A &\to Abc \cdot
    \end{align*}
    entrambi i kernel non presentano ulteriori transizioni.
\end{itemize}

\paragraph{Conclusione}
In conclusione, l'automa caratteristico che otteniamo da questo procedimento può essere visualizzato in Fig.\ref{charateristic-automata_cosntruction}
\begin{figure}[H]
    \centering
	\subimport{assets/figures/}{automa_LR_8-3-4.tex}
    \caption{Automa caratteristico LR(0) per la grammatica \ref{subsubsec:esercizio_costruzione_automa_lr0}}
    \label{charateristic-automata_cosntruction}    
\end{figure}

Una volta terminata questa arzigogolata esercitazione possiamo dare un'occhiata all'algoritmo per la costruzione di un automa LR(0) in Alg.\ref{alg:char-automata}. \\
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{lr0-automata_construction_algorithm.jpg}
    \caption{Algoritmo per la costruzione di un automa LR(0)}
    \label{lr0-automata_construction_algorithm}    
\end{figure}
\subimport{assets/pseudocode/}{char-automata.tex}

Ma dove sono finite le etichette rosse che sull'automa dell'esercizio di shift/reduce ci segnalavano le mosse di reduce? Lo scopriremo nella prossima sezione.

\section{Parsing table per bottom-up}
L'algoritmo di shift/reduce compie delle mosse in base a cosa leggiamo sulle due pile (pila del parsing e pila della lettura). L'algoritmo usa una tabella di parsing per muoversi. La tabella ha tante righe quanti gli stati dell'automa caratteristico, ed una colonna per ogni simbolo in \(V \cup \{ \$ \}\). Nota che la tabella dipende dall'automa caratteristico, automi diversi portano a tabelle diverse che portano a tipi di parsing diversi.

Le mosse di shift dipendono direttamente dalla funzione di transizione dell'automa, mentre le mosse di reduce sono più articolate: vanno effettuate solo quando raggiungiamo quei particolari stati etichettati ed implicano poi che dobbiamo cancellare degli elementi dalla pila degli stati e dalla pila di lettura per poi inserire altri caratteri in quest'ultima pila.

Le mosse di reduce dipendono dal contenuto degli stati dell'automa: in un certo stato della tabella di parsing andiamo ad inserire una mossa di riduzione se la produzione \(A \to \beta\) è effettuata in uno stato contenente un reducing item per \(A \to \beta\). A questo punto però ci chiediamo: cosa sono i reducing item?
\begin{itemize}
    \item Nel caso di LR(0)-items avranno forma \(A \to \beta \cdot\), e indicano che siamo arrivati alla fine della derivazione di \(\beta\), per cui il marker \(\cdot\) si trova in fondo alla derivazione;
    \item nel caso LR(1)-items avranno forma \(A \to \beta \cdot, \; \Delta\), e succede quando ho terminato l'analisi della produzione di \(\beta\),  ma con una condizione in più specificata da quel \(\Delta\).
\end{itemize}
Le riduzioni vengono applicae in base alla \emph{lookahead function} \(\mathcal{LA}\), la quale è definita per tutte le coppie \((P,\; A \to \beta)\) tali che \(P\) contiene un reducing item per \(A \to \beta\).

Da menzionare il fatto che la scelta dell'automa e della lookahead function per la costruzione della parsing table sono le caratteristiche che distinguono le varie tecniche di bottomup parsing. Nello specifico abbiamo le seguenti conseguenze:
\begin{itemize}
    \item la classe delle grammatiche analizzabili dipende dalla scelta appena menzionata;
    \item anche la dimensione della parsing table dipende da tale scelta;
    \item la procedura di riempimento della parsing table è invece indipendente;
    \item anche l'algoritmo di parsing, una volta calcolata la parsing table, è sempre uguale.
\end{itemize}

\subsection{Costruire una parsing table per il bottom-up parsing}
Vediamo ora nello specifico come costruire una parsing table. Abbiamo una tabella con delle entries di forma \(M[P, Y]\), dove \(P\) è uno stato e \(Y\) un simbolo del vocabolario \(V\), e dobbiamo riempire ciascuna di queste seconndo le seguenti regole:
\begin{itemize}
    \item se \(Y\) è un terminale e \(\tau (P,Y) = Q\) inserisci la mossa \texttt{shift} Q;
    \item se \(P\) contiene un reducing item per \(A \to \beta\) e \(Y \in \mathcal{LA}(P, A \to \beta)\), inserisci la mossa \texttt{reduce} \(A \to \beta\) (notiamo che è in questa regola che emerge che le reduce dipendono dalla lookahead function);
    \item se \(P\) contiene l'accepting item e \(Y=\$\) inserisci \texttt{accept};
        \begin{itemize}
            \item nel caso degli automi LR(0) l'accepting item è \(\{S' \to S \cdot\}\);
            \item nel caso degli automi LR(1) l'accepting item è \(\{S' \to S \cdot, \; \Delta\}\);
        \end{itemize}
\item se \(Y\) è un terminale o \$ e nessuna delle condizioni precedenti è valida, inserisce \texttt{errore};
    \item se \(Y\) è un non-terminale e \(\tau (P, Y) = Q\) inserisci la mossa \texttt{goto} \(Q\).
\end{itemize}

\subsection{Conflitti}
L'informazione che è contenuta nella tabella di parsing in posizione \(M[P, Y]\) viene dunque utilizzata per stabilire che tipo di operazione effettuare durante il parsing, dipendentemente dallo stato \(P\) in cui ci troviamo e dal simbolo \(Y\)che leggiamo in cima all'input buffer, terminale o non-terminale che esso sia. Nonostante la grandezza della parsing table dipenda dal tipo di algoritmo di parsing bottom-up che andiamo ad applicare (quindi SLR(1), LALR(1), LR(1)) e dalla funzione di lookahead (\(\mathcal{LA}\)), l'algoritmo che viene utilizzato per riempire la tabella rimane quello visto in precedenza.

Visto che abbiamo già avuto modo di discutere di tabelle di parsing anche per il parsing di tipo top-down, risulta naturale metterle a confronto e chiedersi se, anche in questo caso, sia possibile ottenere dei \textbf{conflitti} (ovvero delle entries multiple defined). Questa situazione può verificarsi anche in questo caso in due differenti modalità: parleremo di
\begin{itemize}
    \item \textbf{s/r conflict} (o shift/reduce conflict) nel caso in cui almeno un entry della tabella di parsing contenga sia un'operazione di \texttt{shift} Q (data dal fatto che esiste una \(Y\)-transizione che va dallo stato \(P\), dove ci troviamo attualmente, allo stato \(Q\)), sia un'operazione di \texttt{reduce} \(A \rightarrow \beta\) (poiché \(P\) contiene un reducing item per \(A \rightarrow \beta\) e \(Y \in \mathcal{LA}(P, A \rightarrow \beta)\));
    \item \textbf{r/r conflict} (o reduce/reduce conflict) nel caso in cui almeno un entry della tabella di parsing contenga due operazioni di \texttt{reduce} per produzioni distinte.
\end{itemize}

La prossima domanda sorge spontanea: cosa accade quando abbiamo un conflitto? La conclusione che possiamo trarre è che, se stiamo eseguendo un parsing di un certo tipo (ad esempio SLR(1), LALR(1) o LR(1)) per una grammatica \(\G\) e troviamo un conflitto mentre costruiamo la tabella, allora \(\G\) \emph{non} è una grammatica di quel particolare tipo; ad esempio, se troviamo un conflitto mentre costruiamo una tabella LR(1) per una grammatica \(\G\), allora \(\G\) non è LR(1).
% La prossima domanda sorge spontanea: cosa accade al parsing quando abbiamo un conflitto? Visto che nell'ambito del parsing bottom-up abbiamo già parlato di tre differenti classi di grammatiche, prendiamoci del tempo per analizzare ogni caso singolarmente: se troviamo almeno un conflitto in una tabella di parsing costruita per la grammatica \(\G\) per eseguire un parsing di tipo 
% \begin{itemize}
%     \item \(SLR(1)\), allora \(\G\) \textbf{non} è una grammatica \(SLR(1)\)
%     \item \(LALR(1)\), allora \(\G\) \textbf{non} è una grammatica \(LALR(1)\)
%     \item \(LR(1)\), allora \(\G\) \textbf{non} è una grammatica \(LR(1)\)
% \end{itemize}
\section{Tabelle di Parsing SLR(1)}
Il parsing di tipo \(SLR(1)\) è un parsing poco raffinato, in quanto le tabelle contengono meno informazioni; questo è dovuto a due motivi:
\begin{enumerate}
    \item la loro costruzione ha come base di partenza un automa caratteristico con LR(0)-item;
    \item la funzione di lookahead è pari a \(\mathcal{LA}(P, A \rightarrow \beta) = follow(A)\) per ogni \(A \rightarrow \beta \cdot \in P\).
\end{enumerate}
Come già detto precedentemente, una grammatica \(\G\) è SLR(1) se e solo se la parsing table ottenuta secondo quelle prescrizioni non ha conflitti.

\subsection{Esempio costruzione tabella di parsing SLR(1)}
Dobbiamo costruire la tabella di parsing \(SLR(1)\) per la seguente grammatica \(\G\):
\begin{align*}
    S &\rightarrow aABe \\
    A &\rightarrow Abc \mid b \\
    B &\rightarrow d
\end{align*}
Riconoscerete di certo questa grammatica, l'abbiamo già vista nella costruzione dell'automa caratteristico (si osservi Fig.\ref{fig:charateristic-automata-complete}). Tuttavia, il nostro obiettivo qui è costruire la tabella di parsing, e per farlo utilizzeremo i risultati ottenuti in precedenza: sappiamo infatti che lo stato \(1\) (anche indicato dal colore verde) è quello che contiene l'\textbf{Accepting Item}, mentre invece gli stati contenenti i \textbf{Reducing Item} (segnalati come stati finali) sono invece 4, 7, 8, 9. 
\begin{figure}[H]
    \centering
	\subimport{assets/figures/}{automa_bottom_up_8-3-4.tex}
    \caption{Automa Caratteristico per la Bottom-Up Parsing Table}
    \label{fig:charateristic-automata-complete}    
\end{figure}
In particolare, possiamo dire che i reducing item sono:
\begin{enumerate}
    \item \(\{A \rightarrow b \cdot\}\) per lo stato 4;
    \item \(\{B \rightarrow d \cdot\}\) per lo stato 7;
    \item \(\{S \rightarrow aABe \cdot\}\) per lo stato 8;
    \item \(\{A \rightarrow Abc \cdot\}\) per lo stato 9;
\end{enumerate}
Avendo a disposizione l'automa caratteristico e i dati sopra citati è dunque possibile costruire la tabella di parsing seguendo l'algoritmo per la costruzione di una tabella di parsing per il bottom-up parsing (ricordiamo all'utente distratto che siamo nell'ambito del parsing; parsing e chiudoing). Procediamo quindi in questo modo:
\begin{enumerate}
    \item creiamo una tabella con tante righe quanti sono gli stati nel nostro automa caratteristico (in questo caso da 0 a 9) e con tante colonne quanti sono i terminali (a cui va aggiunto il \$) e i non-terminali presenti all'interno della nostra grammatica (in questo caso abbiamo di conseguenza \emph{a b c d e \$ A B S});
    \item facendo riferimento all'automa caratteristico, se \(Y\) è un terminale e c'è una transizione \(\tau (P, Y) = Q\) (ovvero una transizione che va dallo stato \(P\) allo stato \(Q\) attraversando un arco con etichetta \(Y\)), allora inseriamo un'operazione di \texttt{shift} \(Q\) nella cella \(M[P, Y]\); ad esempio, nel nostro caso inseriremo \(M[0, a] = 2\);
    \item a questo punto è necessario calcolare i follow (e di conseguenza i first, Tab.\ref{tab:first-follow-SLR(1)parsing}), in quanto per poter inserire correttamente le operazioni di \texttt{reduce} abbiamo bisogno di avere queste informazioni per la funzione di lookahead (\(\mathcal{LA}\));
    \begin{table}[H]
        \centering
        \subimport{assets/tables/}{ptbu-follow.tex}
        \caption{LRS(1) Parsing Table - Calcolo Follow per Lookahead}
        \label{tab:first-follow-SLR(1)parsing}
    \end{table}
    \item una volta calcolati i followprocediamo così: per tutti gli stati \(P\) che contengono un redicing item \(A \to \beta\) (ad esempio, \(M[4, b] = A \rightarrow b\cdot\)), andiamo a inserire la mossa di reduce nella casella \(M[P, Y]\), dove \(Y\) è calcolato con la lookahead function come \(Y = LA(P, A \to \beta)\), nel nostro caso \(Y = follow(A)\); 
    \item inseriamo in posizione \(M[1, \$]\) l'operazione di \texttt{Accept}, in quanto 1 contiene un accepting item;
    \item a questo punto dovremmo valorizzare a \texttt{error} tutte quelle celle \(M[P, Y]\) dove \(Y\) è o \$ o un terminale che non ricade nei casi precedenti; tuttavia per una maggiore comprensione a livello visivo, nel nostro esempio queste celle verranno lasciate vuote;
    \item infine, inseriamo un'operazione del tipo \texttt{goto} \(Q\) per tutti quelle celle \(M[P, Y]\) dove \(Y\) è un non-terminale e per cui esiste una transizione \(\tau(P, Y) = Q\) (ad esempio \(M[0, S]\) = \texttt{goto} 1, come è anche osservabile dall'automa caratteristico).
\end{enumerate}
In Tab.\ref{tab:parsing-table-SLR(1)parsing} possiamo trovare il risultato finale dell'algoritmo. Ai fini di una corretta lettura della tabella, specifichiamo qui sotto alcune convenzioni che verranno impiegate per tenere il tutto conciso, come ad esempio:
\begin{itemize}
    \item l'operazione di \texttt{shift} \(Q\) verrà indicata con \(sQ\);
    \item l'operazione di \texttt{reduce} \(A \rightarrow \beta\) con \(rK\), dove \(K\) indica la \(K\)-esima riduzione; questa notazione è molto conveniente e abbassa il grado di entropia della tabella, ma in seguito forniremo anche una notazione estesa per queste riduzioni;
    \item \emph{Acc} indica l'accepting item;
    \item l'operazione di \texttt{Goto} \(Q\) verrà indicata semplicemente da Q.
\end{itemize}
\begin{table}[H]
    \centering
    \subimport{assets/tables/}{ptbu-complete.tex}
    \caption{LRS(1) Parsing Table}
    \label{tab:parsing-table-SLR(1)parsing}
\end{table}
Notiamo che sulla porzione sinistra della tabella troviamo i vari terminali (compreso \$) e, nelle varie entries, avremo mosse di shift, di reduce e delle celle valorizzate a \texttt{error}; a destra di questa, invece, troviamo i non terminali con le relative mosse di \texttt{goto}.
Specifichiamo quindi il significato dei vari \(rK\):
\begin{itemize}
    \item \(r1 = S \to aABe\)
    \item \(r2 = A \to Abc\)
    \item \(r3 = A \to b\)
    \item \(r4 = B \to d\)
\end{itemize}
Dal momento che \(\G\) dà origine a una nuova grammatica senza creare alcun conflitto, possiamo concludere che \(\G\) è SLR(1).

Prima di passare alla prossima sezione è necessario precisare che \emph{solamente} le celle vuote che hanno per colonna un terminale o \$ sarebbero valorizzate a \texttt{error}: infatti, le celle vuote che hanno per colonna un non-terminale rimangono semplicemente vuote, in quanto sono dei casi che non è possibile che si verifichino. 

\subsection{Algoritmo Shift/Reduce}
Presentiamo ora schematicamente l'algoritmo di shift/reduce, che ricordiamo essere quell'algoritmo che usiamo per ottenere il parsing di una parola \(W\) data la tabella di parsing bottom-up di una certa grammatica \(\G\). Ricordiamo inoltre che l'algoritmo di s/r viene uisto indipendentemente dalla classe di parsing considerata.
\begin{itemize}
    \item \textbf{Input}: una stringa \(w\) e la tabella di parsing \(M\) per \(\G = (V, T, S, \P)\);
    \item \textbf{Output}: una derivazione di \(w\) in ordine inverso se \(w \in \mathcal{L}(\G)\) (per trovare i passi di derivazione che ci portano a \(w\) dovremo leggere tutte le reduce che abbiamo applicato in ordine inverso); otterremo invece \texttt{error} se \(w \notin \mathcal{L}(\G)\);
    \item \textbf{Inizializzazione}: Per l'algoritmo è necessario inserire all'interno dell'input buffer la stringa \(w\$\), e inoltre abbiamo bisogno di due strutture dati:
    \begin{itemize}
        \item \(stSt\) (state stack - pila degli stati), che viene inizializzata ponendo in cima lo stato \(P_0\) (ovvero prima chiusura dell'insieme di LR(0)-item);
        \item \(symSt\) (symbol stack - pila dei simboli).
    \end{itemize}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{bottom-up-parsing-algorithm.png}
    \caption{Algoritmo di parsing Bottom-Up}
    \label{fig:bottom-up-parsing-algorithm}    
\end{figure}
\subimport{assets/pseudocode/}{btmup-parsing.tex}
Andiamo subito a mettere in pratica questo algoritmo con un esempio.

\subsubsection{Esercizio shift/reduce 1}
Consideriamo questa parola:
\begin{equation*}
    w = abbcde
\end{equation*}
Andiamo a verificare se appartiene al linguaggio generato dalla grammatica:
\begin{align}
    \label{eq:grammar-ex-sh/re-1}
    \G: S &\to aABe \\
    A &\to Abc \nonumber \\ \notag
    A &\to b \nonumber \\ \notag
    B &\to d  \nonumber \notag
\end{align}
Ne abbiamo già calcolato la tabella di parsing bottom-up in Tab.\ref{tab:parsing-table-SLR(1)parsing}.

\paragraph{Svolgimento}
La configurazione iniziale è la seguente:
\begin{align*}
    w &= abbcde\$ \\
    stSt &= 0 \\
    symSt &= 
\end{align*}
Il primo carattere che leggiamo nel buffer è \(a\), per cui cerchiamo nella tabella del parsing il contenuto della casella [0,\(a\)]; scopriamo che contiene una mossa di shift verso lo stato 2, quindi andiamo ad inserire il nuovo stato di arrivo in \(stSt\) e il carattere letto in \(symSt\):
\begin{align*}
    w &= \underline{a}bbcde\$ \\
    stSt &= 0\;2\\
    symSt &= a
\end{align*}
Ci troviamo ora nello stato 2 e leggiamo il secondo carattere della parola che è \(b\), quindi controlliamo cosa contiene la casella [2,\(b\)] della tabella di parsing. Abbiamo un'altra mossa di shift che ci fa raggiungere lo stato 4 e leggere un altro carattere, e arriviamo quindi ad avere questa situazione:
\begin{align*}
    w &= \underline{ab}bcde\$ \\
    stSt &= 0\;2\;4\\
    symSt &= a\;b
\end{align*}
Ora siamo in 4 e leggiamo \(b\). La cella [4,\(b\)] ci suggerisce di compiere una mossa di reduce, nello specifico \(R: A \to b\); questo significa che compiremo due operazioni:
\begin{itemize}
    \item elimineremo tanti stati da \(stSt\) quanti gli elementi nel body della produzione (ovvero \(1\), dato che \(|b|\ = 1\));
    \item eliminiremo il body stesso da \(symSt\) e, al suo posto, inseriremo il driver della riduzione, in questo caso \(A\).
\end{itemize} 
Le nostre strutture saranno quindi valorizzate così: 
\begin{align*}
    w &= \underline{ab}bcde\$ \\
    stSt &= 0\;2\;\xout{4}\\
    symSt &= a\;A
\end{align*}
Ora però dobbiamo sostituire lo stato 4 con lo stato che è presente nella tabella in posizione [2,\(A\)], ovvero lo stato 3; la situazione si ristabilisce quindi così:
\begin{align*}
    w &= \underline{ab}bcde\$ \\
    stSt &= 0\;2\;3\\
    symSt &= a\;A
\end{align*}
Dobbiamo ancora consumare la \(b\) (le mosse di reduce non consumano i caratteri), quindi ora ci troviamo nella casella [3, \(b\)],e questa ci suggerisce la mossa shift 6; vediamo cosa succede:
\begin{align*}
    w &= \underline{abb}cde\$ \\
    stSt &= 0\;2\;3\;6\\
    symSt &= a\;A\;b
\end{align*}
A questo punto la casella da guardare diventa [6,\(c\)], che ci dice shift 9, ob-la-di, ob-la-da:
\begin{align*}
    w &= \underline{abbc}de\$ \\
    stSt &= 0\;2\;3\;6\;9\\
    symSt &= a\;A\;b\;c
\end{align*}
Plot-twist! proprio mentre ci siamo abituati alla tranquillità delle mosse di shift la casella [9,\(d\)] ci lascia sbigottiti con una mossa di reduce \(R: A \to Abc\), costringendoci ad eliminare ben 3 stati da \(stSt\) e 3 caratteri da \(symSt\) e sostituendoli con una sprezzante \(A\):
\begin{align*}
    w &= \underline{abbc}de\$ \\
    stSt &= 0\;2\;\xout{3\;6\;9}\\
    symSt &= a\;\xout{A\;b\;c}A
\end{align*}
A questo punto siamo costretti a guardare in [2,\(A\)] per capire in quale stato siamo stati sballottati dal \texttt{goto}, e scopriamo che siamo malaugaratamente tornati nello stato 3:
\begin{align*}
    w &= \underline{abbc}de\$ \\
    stSt &= 0\;2\;3\\
    symSt &= a\;A
\end{align*}
Bene, riprendiamo con la lettura: eravamo rimasti a \(d\), per cui ora controlliamo [3,\(d\)], che ci suggerisce shift 7:
\begin{align*}
    w &= \underline{abbcd}e\$ \\
    stSt &= 0\;2\;3\;7\\
    symSt &= a\;A\;d
\end{align*}
Stato 7, il prossimo carattere nel buffer è \(e\) e la casella [7,\(e\)] presenta questa riduzione \(R: B\to d\); ormai la prassi ci è nota (e prestate attenzione al fatto che in [3, \(B\)] troviamo \texttt{goto} 5):
\begin{align*}
    w &= \underline{abbcd}e\$ \\
    stSt &= 0\;2\;3\;\xout{7}\;5\\
    symSt &= a\;A\;\xout{d}\;B
\end{align*}
Dobbiamo ancora leggere la \(e\) e ci troviamo ora nello stato 5, e [5,\(e\)] ci dice shift 8:
\begin{align*}
    w &= \underline{abbcde}\$ \\
    stSt &= 0\;2\;3\;5\;8\\
    symSt &= a\;A\;\;B\;e
\end{align*}
Con 8 come stato e \$ come carattere in lettura ci assicuriamo una bella reduce in forma di \(R: S \to aABe\), che ci fa arrivare a questa situazione (ricordiamoci del \texttt{goto}):
\begin{align*}
    w &= \underline{abbcde}\$ \\
    stSt &= 0\;1\\
    symSt &= 
\end{align*}
Il prossimo passo è descritto dalla casella [1,\$], che ci dice Accept; questo significa che abbiamo terminato la verifica e possiamo dire, con somma pace interiore, che \(w \in \L(\G)\).

Non abbiamo solo trovato una soluzione al quesito iniziale, ma abbiamo anche ricavato i passi di derivazione che ci portano ad ottenere la parola \(w\) stessa; questi. infatti, consistono nelle riduzioni che abbiamo incontrato nel nostro cammino elencate in senso contrario:
\begin{align*}
    S &\to aABe &(aABe)\\
    B &\to d &(aAde)\\
    A &\to Abc &(aAbcde)\\
    A &\to b &(abbcde)
\end{align*}

\paragraph{Il parsing dell'oca}
Ai più attenti non sarà di certo sfuggito che questa stregoneria del parsing è stata fortemente ispirata dal gioco dell'oca. Pensiamoci: è come se, nel nostro parsing dell'oca, l'obiettivo del gioco fosse quello di raggiungere una casella nascosta "dietro al via"\footnote{Abbiamo fiducia nel fatto che i nostri venticinque lettori stiano al gioco e riescano a intuire il senso di quest'espressione un po' criptica; la prima volta che l'ho letta pure a me suonava molto simile a quella storiella che i terrapiattisti tanto amano sui presunti racconti dei presunti diari di Richard Byrd, in cui l'ammiraglio affermerebbe di essersi spinto centinaia di miglia "oltre il Polo Sud". "Dietro al via", "Oltre il Polo", figata, no? \textasciitilde pips}. Abbiamo tre tipi di caselle:
\begin{itemize}
    \item le caselle di shift ci fanno avanzare di una casella;
    \item le caselle di reduce ci fanno indietrggiare di qualche casella;
    \item le celle error ci fanno perdere il gioco.
\end{itemize}
Purtroppo il tabellone di gioco non è un semplice percorso rettilineo, ma è una cosa più brutta che assomiglia un po' a un grafo. In compenso, nella nostra versione il percorso della pedina è \emph{deterministico}: non c'è un lancio dei dadi, c'è una sorta di mappa da seguire, che altro non è se non la parola \(w\) che vogliamo analizzare. E infatti l'obiettivo del nostro gioco è quello di prendere una mappa (ovvero una parola), seguirla e verificare se riusciamo ad arrivare alla casella finale con delle opportune reduce (perché ripetiamolo, la casella finale si trova "dietro all via!"). E quindi, cosa succede quando arriviamo alla casella finale? Succede che abbiamo vinto, perché vuol dire che la mappa che abbiamo seguito è valida, ovvero che la parola \(w\) considerata appartiene alla grammatica \(\G\) che ha generato il tabellone di gioco (ovvero il linguaggio); inoltre, possiamo anche capire esattamente come \(w\) deriva da \(\G\), seguendo il nostro percorso al contrario e annotandoci le mosse di reduce effettuate. Questo era tutto, siamo finalmente riusciti a dispiegare il nostro parsing una volta per tutte! \textasciitilde sam

Ottimo, adesso che abbiamo messo le cose in chiaro possiamo passare a un secondo esercizio, tanto più complesso quanto più completo e divertente del primo.

\subsubsection{Esercizio shift/reduce 2}
La consegna è di costruire la parsing table SLR(1) per la seguente, ormai ben nota, grammatica:
\begin{equation}
    \label{eq:ex2-sh/re-grammar}
    E \to E+E \mid E*E \mid id
\end{equation} 
Per costruire una parsing table abbiamo prima di tutto bisogno di ricavare l'automa a stati finiti determinato da tale grammatica.
\paragraph{Ricaviamo l'automa}
\begin{enumerate}
    \item Inizializziamo lo stato 0; il suo kernel è 
    \begin{equation*}
        E' \to \cdot E
    \end{equation*}
    Di questo kernel devo calcolare la chiusura:
    \begin{align*}
        E &\to \cdot E+E \\
        E &\to \cdot E*E \\
        E &\to \cdot id
    \end{align*}
    Essendo questi gli item per lo stato 0, posso identificare due possibili transizioni (e quindi due possibili nunovi stati): \(\tau(0,E)=1 \textrm{ e } \tau(0,id)=2\).
    \item Analizziamo ora lo stato 2; il suo kernel è 
    \begin{equation*}
        E \to id \cdot    
    \end{equation*}
    Non serve calcolare la chiusura di questo item; inoltre, tale item non comporta transizioni uscenti dallo stato 2, per cui terminiamo qui l'analisi.
    \item Passiamo al vaglio lo stato 1, il cui kernel è popolato da questi item:
    \begin{align*}
        E' &\to E \cdot\\
        E  &\to E \cdot +E \\
        E  &\to E \cdot *E
    \end{align*}
    Tutti gli elementi del kernel sono già chiusi, e ci offrono queste transizioni: \(\tau(1,+)=3 \textrm{ e } \tau(1,*)=4\). Fatto ciò, abbiamo concluso l'analisi dello stato 1.
    \item Passiamo allo stato 3, il cui kernel risulta
    \begin{equation*}
        E \to E+ \cdot E
    \end{equation*}
    Tale kernel presenta però la possibilità di chiusura, per cui aggiungiamo la chiusura agli item dello stato 3, che risultano essere:
    \begin{align*}
        E &\to E+ \cdot E \\
        E &\to\ \cdot E+E \\
        E &\to\ \cdot E*E \\
        E &\to\ \cdot id
    \end{align*}
    Quindi, da qui posso transizionare tramite \(E\) e tramite \(id\). La transizione per \(E\) mi porta in \(\tau(3,E)=5\), la transizione per \(id\) invece è particolare: mi porterebbe in uno stato che ha come kernel \(E \to id.\), ma quello è lo stesso kernel dello stato 2! Quindi \(\tau(3,id)=2\).
    \item Analizziamo lo stato 4, che ha per kernel
    \begin{equation*}
        E \to E*.E 
    \end{equation*}
    Si può effettuare la chiusura, quindi gli item dello stato 4 sono:
    \begin{align*}
        E &\to E*.E \\ 
        E &\to .E+E \\
        E &\to .E*E \\
        E &\to .id
    \end{align*}
    Esistono due possibili transizioni uscenti dallo stato 4: una per \(E\), che genera lo stato \(\tau(4,E)=6\), e una per \(id\), che come per il caso precedente non genera un nuovo stato ma va a finire nello stato 2.
    \item Quando analizzo lo stato 5, trovo che il kernel è
    \begin{align*}
        E &\to E+E. \\
        E &\to E.+E \\
        E &\to E.*E
    \end{align*}
    Questo non necessita di chiusura, e ci fornisce due possibili transizioni:
    \begin{itemize}
        \item quella per \(+\), che però ci porta in uno stato con kernel \({E \to E.+E}\), ovvero nello stato 3, che abbiamo già analizzato;
        \item quella per \(*\), che però ci porta in uno stato con kernel \({E \to E.*E}\), ovvero nello stato 4, che abbiamo già analizzato.
    \end{itemize}
    \item Allo stesso modo, per lo stato 6 il kernel è
    \begin{align*}
        E &\to E*E. \\
        E &\to E.+E \\
        E &\to E.*E
    \end{align*}
    e nemmeno questo necessita di chiusura, e perdipiù, attraverso le due transizioni per \(+\) e per \(*\), ci porta esattamente negli stati 3 e 4.
\end{enumerate}
Una volta terminata l'analisi degli stati e dei relativi item ci preoccupiamo di trovare quali stati contengono l'accepting item e quali stati contengono i reducing item:
\begin{itemize}
    \item l'accepting item nel metodo SLR(1) è l'item \(S' \to S.\) (ovvero \(E' \to E.\)), che nel nostro caso è contenuto nello stato 1;
    \item i reducing item sono quelli con il marker \(\cdot\) al termine di una produzione, nel nostro caso sono \(E \to id \cdot\) (stato 2), \(E \to E+E \cdot\) (stato 5) e \(E \to E*E \cdot\) (stato 6).
\end{itemize}

Per concludere la parte di costruzione dell'automa caratteristico per la nostra grammatica in Eq.\ref{eq:ex2-sh/re-grammar}, inseriamo in Fig.\ref{ex2-sh/re-automata} il disegno di tale sgorbio (scherzo, è carino, sembra un razzetto).
\begin{figure}[H]
    \center
	\subimport{assets/figures/}{automa_caract_8-4.tex}
    \caption{Automa caratteristico per la grammatica \ref{eq:ex2-sh/re-grammar}}
    \label{fig:ex2-sh/re-automata}
\end{figure}

\subsubsection{La risoluzione dei conflitti}
Quindi, partendo dalla grammatica Eq.\ref{eq:ex2-sh/re-grammar} siamo riusciti a ricavarne e l'automa caratteristico SLR(1) in Fig.\ref{fig:ex2-sh/re-automata} e i reducing items, che ricordiamo essere:
\begin{align*}
    2: E &\to id & 5: E &\to E + E \cdot & 6: E &\to E * E \cdot
\end{align*}
Tutto questo è fantastico, ma purtroppo c'è un grosso elefante nella nostra stanza. L'intrinseca ambiguità di Eq.\ref{eq:ex2-sh/re-grammar} ha un effetto collaterale piuttosto sgradevole: la sua tabella di parsing SLR(1) avrà dei conflitti, come possiamo ben vedere in Tab.\ref{tab:ex2-sh/re-table}.
\begin{figure}[H]
    \center
    \includegraphics[width=.6\textwidth]{ex2-sh_re-table.png}
    \caption{tabella di parsing SLR(1) per la grammatica \ref{eq:ex2-sh/re-grammar}}
    \label{tab:ex2-sh/re-table}
\end{figure}
\noindent Quindi, se vogliamo perseguire la nostra ricerca di un parsing deterministico (e lo vogliamo), toccherà rimboccarsi le maniche e andare a capire come possiamo risolvere questi conflitti, non diversamente da quanto già avevamo fatto nel parsing top-down con le entries multiple defined.

I nostri conflitti sono localizzati solo in quattro situzioni, ossiamo quando ci troviamo in uno tra gli stati 5 o 6 e, contemporaneamente, troviamo in lettura un simbolo tra \(+\) o \(\ast\).
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{ex2-sh_re-table_conf.png}
    \caption{Dettaglio di Tab.\ref{tab:ex2-sh/re-grammar_conf}}
    \label{tab:ex2-sh/re-table_conf}
\end{figure}
\noindent Proviamo a capire bene qual è il punto di tutta questa situazione. Se il nostro parser si trova nello stato 5, questo vuol dire che, in testa alla nostra pila dei simboli \(symSt\), avremo necessariamente \(E + E\); possiamo vederlo facilmente, è sufficiente buttare un occhio al nostro automa caratteristico (Fig.\ref{fig:ex2-sh/re-automata}) per renderci conto che lo stato 5 è raggiungibile solo dallo stato \(3\), il quale a sua volta può essere raggiunto o dalla sequenza di stati \((0, 1)\), \((4, 6)\) o \((3, 5)\), in tutti questi casi sulla testa di \(symSt\) si troverà \(E + E\). Il discorso è del tutto analogo se passiamo allo stato 6, dove avremo la testa di \(symSt\) popolata di \(E * E\), poiché 6 è raggiungibile tramite \(\dots, 0, 1, 4, 6\), \(\dots, 4, 6, 4, 6\) oppure \(\dots, 3, 5, 4, 6\).

È chiaro che abbiamo dei conflitti quando siamo nella situazione in cui due simboli legati da un operatore sono stati già letti (\(E + E\) o \(E * E\)) e, subito dopo, leggiamo un terzo simbolo. Questo è dovuto proprio al fatto che la grammatica, nella sua ambiguità, non riesce a esprimere opportunamente né associatività, né precedenza per i due operatori.

E insomma, qual è il segreto per risolvere questi dannati conflitti? Quello che faremo (almeno in questo caso) è, per ogni cella in cui abbiamo un conflitto, scegliere manualmente quale mossa mantenere. Questa scelta pare essere in mano nostra, e nel nostro specifico esempio faremo sì che il parser finisca per conformarsi a quelle che sono le comuni regole di precedenza e associatività per \(+\) e \(*\). 

Possiamo aiutarci se pensiamo ai parse tree della nostra grammatica: gli alberi, infatti, per loro stessa costruzione, veicolano informazioni sull'annidamento dei loro elementi in maniera molto efficace, e l'annidamento può tranquillamente avere una corrispondenza diretta con la parentesizzazione. Di fatto assumendo, come facciamo noi, che i sottoalber vengano risolti prima degli alberi padri, stabiliamo delle regole di precedenza del tutto simili a quelle che si possono ottenere con una parentesizzazione.

Nello specifico, noi siamo interessati a esprimire l'associatività sinistra degli operatori \(+\) e \(*\), nonché la precedenza di \(*\) rispetto a \(+\). Diciamolo in maniera ancora più pragmatica: se guardiamo gli esempi in Fig.\ref{fig:ex2-sh_re-ptree}, vogliamo sempre avere l'albero Fig.\ref{fig:ex2-sh_re-ptree_1} al posto di Fig.\ref{fig:ex2-sh_re-ptree_2}, e vogliamo che la precedenza tra i due operatori sia tale da generare sempre parse tree come Fig.\ref{fig:ex2-sh_re-ptree_3} e Fig.\ref{fig:ex2-sh_re-ptree_4}.
\begin{figure}[H]
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_a.tex}
        \subcaption{Parse tree left-associativo}
        \label{fig:ex2-sh_re-ptree_1}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_b.tex}
        \subcaption{Parse tree left associativo}
        \label{fig:ex2-sh_re-ptree_2}
    \end{minipage}
    
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_c.tex}
        \subcaption{Parse tree per \(E + E * E\)}
        \label{fig:ex2-sh_re-ptree_3}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_d.tex}
        \subcaption{Parse tree per \(E * E + E\)}
        \label{fig:ex2-sh_re-ptree_4}
    \end{minipage}
    \caption{}
    \label{fig:ex2-sh_re-ptree}
\end{figure}
In questa interpretazione possiamo vedere le mosse di reduce come una parentesizzazione, perché stiamo assegnando un padre a un determinato sottoalbero del tipo \(E + E\) o \(E * E\).

A questo punto possiamo andare ad analizzare i quattro conflitti che abbiamo trovato e, per ciascuna cella, scegliere la mossa che rende il parsing conforme alle regole che abbiamo stabilito. Tenendo a mente Tab.\ref{tab:ex2-sh/re-table_conf}, rimbocchiamoci le maniche.
\begin{itemize}
    \item \(M[5, +]\): questa è la situazione in cui la testa di \(symSt\) è popolata di \(E + E\) e in lettura troviamo un altro \(+\); gli operatori sono left-associativi, per cui vogliamo parentesizzare la testa del nostro \(symSt\) e solo dopo leggere il nuovo simbolo; ergo, la mossa che scegliamo è \(r1\).
    \item \(M[6, *]\): vediamo subito questa cella perché presenta una situazione del tutto analoga a quella appena vista;  la testa di \(symSt\) è popolata di \(E * E\) e questa volta l'operatore coinvolto è \(\ast\), che al pari di \(+\) è left-associativo, di conseguenza anche qui scegliamo la reduce \(r2\);
    \item \(M[5, *]\): torniamo adesso a parlare dello stato 5; la testa di \(symSt\) è popolata come prima, ma questa volta il lettura troviamo un simbolo \(\ast\). Questo cambia le carte in tavola, perché noi vogliamo che l'operatore \(\ast\) abbia precedenza più alta di \(+\); per ottenere questo, scegliamo la mossa di shift \(s4\), il che significa che rimandiamo la parentesizzazione della testa e proseguiamo nella lettura.
    \item \(M[6, +]\): qui abbiamo trovato \(E * E\) nella testa di \(symSt\) e ci sta arrivando un \(+\) in lettura, e ormai l'idea dovrebbe essere chiara: coerentemente con le regole di precedenza (\(\ast \prec +\)), vogliamo che la testa di \(symSt\) sia parentesizzata, per cui qui teniamo la reduce \(r2\).
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{ex2-sh_re-table_conf_solved.png}
    \caption{Soluzione dei conflitti di Tab.\ref{tab:ex2-sh/re-grammar_conf}}
    \label{tab:ex2-sh/re-table_conf_solved}
\end{figure}
Fatto! Dopo tutto questo processo abbiamo ottenuto una tabella di parsing senza conflitti, ovvero senza entries multiple defined. Strategie di questo tipo sono abbastanza standard per le grammatiche degli operatori (non solo aritmetici, si pensi alle precedenze dell'algebra booleana); all'atto pratico, chi scrive le grammatiche (che ha studiato LFC e ha capito questi argomenti) si preoccuperà anche di predisporre delle direttive per il parser, sicché quest'ultimo abbia informazioni su quali scelte compiere in caso di conflitti.

Naturalmente, tutto questo è reso necessario dal fatto che la grammatica \(\G\) in Eq.\ref{eq:ex2-sh/re-grammar} è ambigua; se avessimo semplicemente scritto un'altra grammatica \(\G'\) non ambigua, tale che \(\L(\G) = \L(\G')\), saremmo riusciti a ottenere una tabella di parsing senza conflitti da risolvere. Dovessimo percorrere questa strada, sia chiaro che è estremamente importante che la nuova grammatica esprima correttamente le regole di precedenza e associatività che vogliamo. Guardiamo ad esempio:
\begin{align}
    \label{eq:ex2-sh/re-grammar_v2}
    \G': E &\to E + T \mid T \\
    T &\to T * id \mid id \notag
\end{align}
L'ordine con cui ho inserito le produzioni non è causale: abbiamo posizionato \(*\) in un livello di produzione inferiore a \(+\) perché, dal momento che la derivazione è rightmost e andiamo ad attraversare l'albero dal basso verso la radice (bottom-up, no?), ci assicuriamo in questo modo di parentesizzare prima tutte le occorrenze di \(\ast\); non avremmo avuto questa garanzia se avessimo inserito produzioni come, ad esempio, \(E \to T + E\). 

Inoltre, se ad esempio avessimo scritto \(T \to id * T\), avremmo perso la left-associatività per l'operatore \(\ast\) (ma lo stesso sarebbe successo anche per \(+\)). Il modo migliore per convincersi di queste cose (nonché il nostro suggerimento per il lettore), è quello di provare da sé a cambiare la grammatica, provare a tracciare dei parse tree e vedere cosa succede, un po'come abbiamo fatto noi qui sotto in Fig.\ref{fig:ex2-sh_re-altgrm-ptree}.
\begin{figure}[H]
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_a_V2.tex}
        \subcaption{\(T \to T * id\), left-associativa (versione corretta)}
        \label{fig:ex2-sh_re-altgrm-ptree_1}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \subimport{assets/figures/}{parse_tree_left_associative_b_V2.tex}
        \subcaption{Se invece scriviamo \(T \to id * T\) perdiamo la left-associatività}
        \label{fig:ex2-sh_re-altgrm-ptree_2}
    \end{minipage}
    \caption{Esempio di come possiamo alterare Eq.\ref{eq:ex2-sh/re-grammar_v2} e vedere i cambiamenti direttamente dalla struttura dei parse trees}
    \label{fig:ex2-sh_re-altgrm-ptree}
\end{figure}

È bene far notare, inoltre, che questo procedimento ci ha portato ad aggiungere il non-terminale \(T\) alla grammatica. Certo, un non-terminale in più non ha tutto questo grande impatto sull'efficienza degli algoritmi, ma in altre situazioni la scrittura di una grammatica non ambigua che generi un linguaggio equivalente (a quello della grammatica ambigua di partenza, ndr) può rivelarsi davvero complicata e costosa; per questo motivo, spesso si preferisce semplicemente tenere le grammatiche ambigue e gestire i conflitti manuealmente, tramite direttive per il parser: il prezzo da pagare per usare una grammatica non ambigua equivalente sarebbe troppo alto.

\subsubsection{Esercizio sulla risoluzione dei conflitti}
Sia data la seguente grammatica:
\begin{align}
    \label{eq:ex3-slr1-grammarr}
    \G: S &\to aAd \mid bBd \mid aBe \mid bAe \\
    A &\to c \nonumber \\ \notag
    B &\to c \notag
\end{align}
Il nostro obiettivo è costruire una tabella di parsing SLR(1) per la grammatica citata, e per farlo, come prima cosa, andiamo a tracciare l'automa caratteristico.
\paragraph{Ricaviamo l'automa}
\begin{enumerate}
    \item Inizializziamo lo stato 0. Il suo kernel è:
    \begin{equation*}
        S' \to \cdot S
    \end{equation*}
    Di questo kernel devo calcolare la chiusura:
    \begin{align*}
        S &\to \cdot aAd \\
    	S &\to \cdot bBd \\
    	S &\to \cdot aBe \\
    	S &\to \cdot bAe
    \end{align*}
    Essendo questi gli items per lo stato 0, posso identificare tre possibili transizioni (e quindi tre possibili nuovi stati): \(\tau(0,S)=1, \; \tau(0,a)=2 \; \textrm{e} \; \tau(0,b)=3\).
    \item Analizziamo ora lo stato 1; il suo kernel è:
    \begin{equation*}
        S' \to S \cdot    
    \end{equation*}
    Non serve calcolare la chiusura di questo item, in quanto è formata solamente da sé stesso; dobbiamo però evidenziare il fatto che questo è lo stato contenente l'\textbf{Accepting Item}.
    \item Analizziamo dunque lo stato 2, il cui kernel è popolato da questi item:
    \begin{align*}
        S &\to a \cdot Ad \\
        S &\to a \cdot Be
    \end{align*}
    e di cui è necessario calcolare la chiusura:
    \begin{align*}
        A &\to \cdot c \\
        B &\to \cdot c
    \end{align*}
    Anche nel caso dello stato 2 possiamo osservare la presenza di tre transizioni e quindi di tre possibili nuovi stati che sono \(\tau(2,A)=4 \textrm{, } \tau(2,B)=5 \textrm{ e } \tau(2,c)=6\).
    \item Passiamo ora allo stato 3, il cui kernel risulta:
    \begin{align*}
        S &\to b \cdot Bd \\
        S &\to b \cdot Ae
    \end{align*}
    Su questo kernel si presenta la possibilità di effettuare una chiusura, che risulta essere:
    \begin{align*}
        A &\to \cdot c \\
        B &\to \cdot c
    \end{align*}
    In questo caso è necessario prestare un po' più di attenzione, perché sebbene vi siano tre transizioni uscenti dallo stato 3, dovremo creare soltanto 2 nuovi stati: \(\tau(3,B)=7 \textrm{, } \tau(3,A)=8 \textrm{ e } \tau(3,c)=6\). Per convalidare questa affermazione è possibile osservare che il kernel dello stato 6 coinciderà esattamente con quello della transizione \(\tau(3, c)\).
    \item Passiamo allo stato 4, il cui kernel è:
    \begin{equation*}
        S \to aA \cdot d 
    \end{equation*}
   La chiusura di tale stato non porta nuovi elementi, tuttavia è comunque possibile definire la transizione \(\tau(4,d)\) allo stato 9.
    \item Analizzo ora lo stato 5, e il suo kernel è:
    \begin{equation*}
        S \to aB \cdot e
    \end{equation*}
    Tale stato non necessita di chiusura e ci fornisce la transizione \(\tau(5,e)\) verso lo stato 10.
    \item Procedendo come abbiamo fatto finora, il kernel per lo stato 6 è:
    \begin{align*}
        A &\to c \cdot \\
        B &\to c \cdot
    \end{align*}
    la cui chiusura non ci porta nulla di nuovo e possiamo osservare che nello stato 6 sono presenti due reducing item.
\end{enumerate}

Potremmo continuare senza fare ulteriori elucubrazioni, ma non possiamo non porre l'attenzione sullo stato appena analizzato: il fatto che vi possano essere due reducing items all'interno di un singolo stato non è necessariamente motivo di conflitti, in quanto la presenza di conflitti dipende dalla funzione di lookahead che stiamo utilizzando. Nel caso del parsing di tipo SLR(1) sappiamo che è possibile inserire un'operazione di \texttt{reduce} \(A \rightarrow \beta\) in \(M[P, Y]\) nel caso in cui \(P\) sia un stato contenente un reducing item e per tutti quei terminali \(Y \in \mathcal{LA}(P, A \rightarrow \beta) = follow(A)\): in questo caso \(follow(A) = follow(B) = \{d, e\}\)\footnote{Suvvia, un po' di fiducia, questi calcoli li abbiamo fatti davvero.}, il che significa che avremo almeno due celle dove sono presenti dei conflitti nella nostra parsing table finale.

La situazione è ben rappresentata dalla figura \ref{fig:lr0-automata_conflict} che riporta un pezzo dell'automa caratteristico che deriva da questa analisi.

\begin{figure}[H]
    \centering
    \subimport{assets/figures/}{automa_conflict_LR.tex}
    \caption{Dettaglio dell'automa di tipo LR(0) per la grammatica \ref{eq:ex3-slr1-grammarr}, si noti come con una \(c\)-transizione si possa arrivare in 6 sia da 3 che da 2}
    \label{fig:lr0-automata_conflict}
\end{figure}

Tuttavia, in questo caso la colpa non è della grammatica che non è per nulla ambigua (produce solo 4 parole), ma va ricercata piuttosto nel tipo di parsing adoperato, fin troppo grossolano: invece di accorpare tutto all'interno di un singolo stato (riguardiamo lo stato 6), non sarebbe meglio avere due stati differenti? 

Ovviamente stato 2 e lo stato 3 "ricordano" delle informazioni differenti e per poter risolvere questa indecisione è necessario considerare in quale modo si è arrivati all'item di riduzione. Ad esempio, nel caso in cui avessimo letto fino ad ora la parola \(ac\) e ci trovassimo quindi in dubbio su che tipo di operazione di reduce effettuare, basterebbe semplicemente poter osservare quale fosse il prossimo simbolo in lettura: 
\begin{itemize}
    \item effettuiamo \texttt{reduce} \(A \to c\) nel caso in cui il prossimo simbolo sia \(d\) in quanto è possibile derivare da \(S\) solamente una parola che inizi per \(a\) e termini con \(d\), del tipo \(aAd\);
    \item effettuiamo \texttt{reduce} \(B \to c\) nel caso in cui il prossimo simbolo sia \(e\) in quanto è possibile derivare da \(S\) solamente una parola che inizi per \(a\) e termini con \(e\), del tipo \(aBe\).
\end{itemize}

\subsection{LR(1)-items}
Per poter ottenere quanto detto andremo a utilizzare gli LR(1)-items, che hanno questa forma:
\begin{equation}
    \label{lr0}
    [A \to \alpha \cdot B \beta, \Delta]
\end{equation}
Viene più semplice immaginare che un LR(1)-item sia una tupla composta da due elementi:
\begin{enumerate}
    \item il primo è un item di tipo LR(0) \((A \to \alpha \cdot B \beta)\);
    \item il secondo è un insieme di caratteri \((\Delta)\).
\end{enumerate} 
\(\Delta\) è detto \textbf{lookahead-set}.

Quando andiamo a calcolare la chiusura di un item di tipo LR(1) utilizziamo la \(closure_1(item)\) e non la \(closure_0(item)\), che funziona esattamente come la \(closure_0\) ma in più ci permette di aggiornare l'insieme \(\Delta\), il lookahead set. Questo insieme ci torna poi utile quando dobbiamo inserire mosse di riduzione in qualche stato dell'automa caratteristico LR(1): il lookahead set specifica quali caratteri ci dobbiamo aspettare di vedere in lettura quando stiamo per utilizzare una mossa di reduce.

Ma specifichiamo ora meglio come è strutturata l'operazione di \(closure_1(item)\):
\begin{itemize}
    \item la prima parte della chiusura è in tutto e per tutto uguale alla \(closure_0(item)\), quindi quando si calcola una \(closure_1(item)\) la prima cosa che si fa è appunto calcolare la \(closure_0(item)\), utilizzando la parte LR(0) dell'item LR(1) che stiamo chiudendo;
    \item la seconda parte pervede di aggiornare l'insieme di lookahead, che viene aggiunto alla \(closure_0(item)\) appena calcolata.  
\end{itemize}
Di fatto con questo metodo di calcolo l'informazione contenuta nell'insieme \(\Delta\) di un LR(1)-item viene tramandata agli item che ne derivano grazie alla seconda parte della procedura \(closure_1\), così se si segue una derivazione lungo tutto l'albero delle produzioni guardando a \(\Delta\) si ha sempre sotto controllo quali simboli ci si aspetta di leggere in un certo stato quando si deve effettuare una mossa di riduzione.

\subsection{Chiusure di insiemi di LR(1)-item}
Come facciamo a calcolare la chiusura degli insiemi di LR(1)-items? Il metodo di calcolo che ci viene presentato è un classico calcolo risolvibile grazie al teorema del punto fisso:
\begin{definition}
    \label{def:lr1-closure}
    Sia \(P\) un insieme di LR(1)-item, la \(closure_1(P)\) identifica il più piccolo insieme di item, con il più piccolo lookahead-set, che soddisfa la seguente equazione:
    \begin{align*}
        closure_1(P) = P \; \cup \; &\{[B \rightarrow \cdot \gamma, \Gamma] : [A \rightarrow \alpha \cdot B \beta, \Delta] \in closure_1(P) \; \land \\ 
        & B \rightarrow \gamma \in \P' \; \land \\
        & first(\beta \Delta) \subseteq \Gamma\}
    \end{align*}  
    dove \(first(\beta \Delta) = \cup_{d \in \Delta} first(\beta d)\) e ricordiamo che \(\P'\) è ricavato dall'insieme delle produzioni \(\P\) aggiungendo la produzione \(S' \to S\).
\end{definition}
Un altro modo per descrivere questa procedura è quello di rappresentarla in linguaggio algoritmico, questa seconda possibilità è mostrata in Alg.\ref{alg:lr1-closure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{alg_closure-lr1.png}
    \caption{Algoritmo per il calcolo della chiusura di uno stato LR(1)}
    \label{alg:closure-lr1}
\end{figure}
\subimport{assets/pseudocode/}{lr1-closure.tex}
In questo algoritmo si deve fare conto che \(B \to \cdot \gamma \notin prj(P)\) significa che non è ancora presente in \(closure_1(P)\) una produzione \(B \to \cdot \gamma\).
Di fatto quello che succede all'interno del \texttt{foreach} centrale è che se l'elemento è nuovo vado ad aggiungerlo con il suo \(\Delta\), se l'elemento è già presente vado a vedere se è il caso di aggiungere al \(\Delta\) di questa qualche elemento.

La definizione appena illustrata potrebbe sermbrare di difficile comprensione, ma sarà tutto più chiaro dopo qualche esercizio esplicativo.

\subsubsection{Esesercizio di calcolo di \(closure_1(P)\)}
Facendo riferimento alla grammatica dell'esercizio precedente (riportata qui sotto), calcolare tutti gli stati dell'automa caratteristico LR(1).
\begin{align}
    \label{eq:ex3-slr1-grammarr}
    \G: S &\to aAd \mid bBd \mid aBe \mid bAe \\
    A &\to c \nonumber \\ \notag
    B &\to c \notag
\end{align}
Partiamo con l'inizializzazione dello stato \(0\); esso conterrà la \(closure_1(\{[S' \to \cdot S, \{\$\}]\})\); questa inizializzazione sottolinea il fatto che, che una volta che avremo raggiunto il particolare stato in cui avremo l'accepting item \(S' \to S\cdot \), vorremo vedere il \$ che viene utilizzato come terminatore per definire una parola appartenente al linguaggio. Quindi, che si fa ora?

Secondo la Def.\ref{def:lr1-closure}, la \(closure_1(P)\) va inizializzata con \(P\) stesso, ovvero \([S' \to \cdot S, \{\$\}]\).
In seguito dobbiamo andare a prendere all'interno della \(closure_1(P)\) (che è parziale, non ancora completa) tutti gli elementi in forma \([A \rightarrow \alpha \cdot B \beta, \Delta]\) e dovremo aggiungere a \(closure_1(P)\) tutti quegli item che soddisfano la forma \([B \rightarrow \cdot \gamma, \Gamma]\); dove \(\Gamma\) è un insieme calcolato come descritto dall'algoritmo; niente paura, in seguito sarà tutto più chiaro.

Nel nostro caso l'unico item in \(closure_1(P)\) è \([S' \to \cdot S, \{\$\}]\), che corrisponde alla forma desiderata (\([A \to a \cdot B\beta, \Delta]\)), quindi possiamo assumere \(\Delta = \{\$\} \textrm{ e } \beta = \varepsilon\); di conseguenza, \(first(\varepsilon \$) = first(\$) = \{\$\} \subseteq \Gamma\). Gli item che otteniamo da \(closure_1(\{[S' \to \cdot S, \{\$\}]\})\) saranno dunque:
\begin{align*}
    [&S' \to \cdot S, \{\$\}] \\
    [&S \to \cdot aAd, \{\$\}] \\
	[&S \to \cdot bBd, \{\$\}] \\
	[&S \to \cdot aBe, \{\$\}] \\
	[&S \to \cdot bAe, \{\$\}]
\end{align*}
Quanto abbiamo appena ottenuto è la chiusura dell'insieme che costituisce lo stato 0 del nostro automa caratteristico di tipo LR(1).
Come già detto prima la prima parte di questi item è esattamente corrispondente all'item LR(0) che avremmo trovato utilizzando SLR parsing, la seconda parte (ovvero il \(\Delta\)) è l'unica novità introdotta dalla procedura LR(1).

È da notare anche che la componente del lookahead-set è usata solo in caso di riduzioni, e non incide sulle transizioni, per cui il procedimento rimarrà sotto quel punto di vista inalterato rispetto al metodo SLR. Di conseguenza, possiamo affermare la presenza, di tre transizioni che portano a tre stati differenti: \(\tau(0,S)=1 \textrm{, } \tau(0,a)=2 \textrm{ e } \tau(0,b)=3\). A questo punto è possibile procedere come abbiamo sempre fatto, prestando attenzione al calcolo della \(closure_1(S)\):
\begin{enumerate}
    \item Proseguiamo osservando lo stato \(\tau(0, S)=\)1. Il suo kernel è:
    \begin{equation*}
        [S' \to S \cdot, \{\$\}]
    \end{equation*}
    dobbiamo dunque calcolare \(closure_1(\{[S' \to S \cdot, \{\$\}]\})\), per cui sappiamo che \(B = \varepsilon\) e dunque non possiamo fare altro se non appuntarci che lo stato 1 contiene l'Accepting Item.
    \item Analizziamo dunque lo stato \(\tau(0, a)=\)2, il cui kernel è popolato da questi item:
    \begin{align*}
        [S &\to a \cdot Ad, \{\$\}] \\
        [S &\to a \cdot Be, \{\$\}]
    \end{align*}
    entrambi soddisfano la forma \([A \rightarrow \alpha \cdot B \beta, \Delta]\).
    Nel primo caso abbiamo le seguenti corrispondenze: \(A=S\), \(\alpha = a\), \(B = A\), \(\beta = d\) e \(\Delta=\$\); andiamo a cercare nella nostra grammatica se sono presenti derivazioni in forma \(B \to \gamma\), troviamo \(A \to c\), quindi grazie alla formula del calcolo di \(closure_1(P)\) sappiamo che dobbiamo aggiungere agli item dello stato 2 un item formato così: \([A \to \cdot c, {\Gamma}]\), dove \(\Gamma = first(\beta\Delta) = first(d\$) = \{d\}\); in definitiva, aggiungiamo l'item \(A \to \cdot c, \{d\}\).

    Abbiamo detto che anche la seconda produzione (\([S \to a \cdot Be, \{\$\}]\)) soddisfa la forma \([A \rightarrow \alpha \cdot B \beta, \Delta]\), quindi con lo stesso procedimento appena adottato possiamo trovare la chiusura di \([S \to a \cdot Be, \{\$\}]\): in questo caso le corrispondenze sono: \(A=S\), \(\alpha=a\), \(B=B\), \(\beta=e\) e \(\Delta=\$\), la produzione da analizzare è \(B \to c\) e l'item che ne ricaviamo è \([B \to \cdot c, {\Gamma}]\), dove \(\Gamma = first(\beta\Delta) = first(e\$) = \{e\}\).
    
    In sostanza, lo stato 2 contiene questi LR(1) items:
    \begin{align*}
        [S &\to a \cdot Ad, \{\$\}] \\
        [S &\to a \cdot Be, \{\$\}] \\
        [A &\to \cdot c, \{d\}] \\
        [B &\to \cdot c, \{e\}]
    \end{align*}
    Come succedeva nell'esempio con il parsing LR(0), nello stato 2 possiamo osservare la presenza di tre transizioni e quindi di tre possibili nuovi stati che sono \(\tau(2,A)=4 \textrm{, } \tau(2,B)=5 \textrm{ e } \tau(2,c)=6\).
    \item[...]
    \item[6.] Ovviamente la parte interessante dell'esercizio è verificare se siamo riusciti a risolvere il conflitto che occorreva nel parsing SLR(0). In modo pressoché analogo a quanto accadeva, abbiamo che il kernel per lo stato 6 è :
    \begin{align*}
        [A &\to c \cdot, \{d\}] \\
        [B &\to c \cdot, \{e\}]
    \end{align*}
    Anche questa volta nello stato sono presenti due item di riduzione, ma non aviene il conflitto che si verificava ncon il parsing di tipo LR(0) perché in questo caso la riduzione \(A \to c\) si applica solo se nell'input buffer si sta leggendo \(d\), mentre la riduzione \(B \to c\) si applica solo nel caso in cui nell'input \emph{baffer} si sta leggendo \(e\). Questo implica che nello stato 6, grazie al parsing LR(1), è stata eliminata l'ambiguità del conflitto reduce-reduce.
\end{enumerate}

\subsubsection{Facciamone un altro, dai}
\label{ex:closure-lr1}
Sia data la seguente grammatica:
\begin{align}
    \G: S &\to L=R \mid R \\
    L &\to *R \mid id \nonumber \\ \notag
    R &\to L \notag
\end{align}
Inizializziamo lo stato 0 ponendo come suo kernel:
\begin{equation*}
    [S' \to \cdot S, \{\$\}]
\end{equation*}
Calcoliamo \(closure_1(0)\). Questo item soddisfa la forma \([A \rightarrow \alpha \cdot B \beta, \Delta]\), con le seguenti corrispondenze: \(B = S \texttt{, } \beta = \varepsilon \texttt{, } \Delta = \{\$\}\) (e quindi \(\Gamma = first(\beta \Delta) = first(\varepsilon \$) = \{\$\}\)).
Le due produzioni di \(B\) (ovvero di \(S\)) in questo caso sono \(S \to L=R \mid R\), quindi aggiungo i due item LR(1) seguenti:
\begin{align}
    \label{prod:production-L}
    [S &\to \cdot L = R, \{\$\}] \\
    [S &\to \cdot R, \{\$\}] \notag
\end{align}
A questo punto la chiusura non è completa in quando abbiamo aggiunto due elementi che possono essere presi in considerazione nella definizione ricorsiva della chiusura, ovvero soddisfano anch'essi la famosa forma \([A \rightarrow \alpha \cdot B \beta, \Delta]\).
Analizziamo dunque \([S \to \cdot L = R, \{\$\}]\), le sue corrispondenze sono: \(B = L \texttt{, } \beta = \; \; =R \texttt{, } \Delta = \{\$\}\) (e quindi \(\Gamma = first(\beta \Delta) = first(=R\$) = \{=\}\)); le produzioni di \(L\) sono \(L \to *R \mid id\), quindi arriviamo a generare i seguenti LR(1)-items:
\begin{align*}
    [L &\to \cdot *R, \{=\}] \\
    [L &\to \cdot id, \{=\}]
\end{align*}
che non presentano altre possibili espansioni (grazie al cielo).

Ci rimane ora da analizzare \([S \to \cdot R, \{\$\}]\), che soddisfa la nostra formosa forma e presenta le seguenti corrispondenze: \(B = R \texttt{, } \beta = \{\varepsilon\} \texttt{, } \Delta = \{\$\}\) (e quindi \(\Gamma = first(\beta \Delta) = first(\varepsilon \$) = \{\$\}\)), inoltre le produzioni di \(R\) sono \(R \to L\), quindi otteniamo il seguente LR(1)-items:
\begin{align*}
    [R &\to \cdot L, \{\$\}]
\end{align*}
questo item presenta il marker davanti a un non-terminale; può generare nuovi item per la chiusura che stiamo calcolando?
Nonostante la produzione con driver \(L\) sia già stata analizzata (vedi \ref{prod:production-L}), è necessario eseguire nuovamente la sua chiusura in quanto ha un lookahed-set differente (nel caso precedente \(\beta= \; \;  =R\), mentre ora \(\beta=\varepsilon\)), il che ci porta a ripetere la chiusura per l'item \([R \to \cdot L, \{\$\}]\), ricavando i seguenti LR(1)-items:
\begin{align*}
    [L &\to \cdot *R, \{\$\}] \\
    [L &\to \cdot id, \{\$\}]
\end{align*}
Visto che però avevamo già inserito due LR(1)-item con lo stesso LR(0)-item (la parte \(L \to \cdot qualcosa\)) possiamo semplicemente accrescere i lookahead-set corrispondenti. Il risultato finale per la chiusura degli item dello stato iniziale è:
\begin{align*}
    [&S'\to \cdot S, \{\$\}] \\
    [&S \to \cdot L = R, \{\$\}] \\
    [&S \to \cdot R, \{\$\}] \\
    [&L \to \cdot *R, \{\$, =\}] \\
    [&L \to \cdot id, \{\$, =\}] \\
    [&R \to \cdot L, \{\$\}]
\end{align*}
Bene, ora abbiamo fatto un po' di allenamento con il calcolo degli stati, ma dobbiamo ricordare che non è questo il nostro obiettivo finale, noi vogliamo ricavare l'automa caratteristico!

\subsection{Costruire un automa LR(1)}
Partiamo con lo spiegare nel modo più semplice come si fa a costruire un automa LR(1):
\begin{enumerate}
    \item costruiamo lo stato di partenza con l'item \([S' \to \cdot S, {\$}]\);
    \item ricorsiavamente aggiungiamo gli stati che si possono raggiungere dagli stati presenti nell'automa;
    \item aggiungamo le transizioni che ci permettono di passare da uno stato all'altro.
\end{enumerate}
E come facciamo a capire quali stati sono collegati ad un certo stato \(P\)?

\noindent Se uno stato \(P\) contiene un item nella forma \([A \to \alpha \cdot Y \beta, \Delta]\) allora esiste una transizione da \(P\) ad uno stato \(Q\) che contiene l'item \([A \to \alpha Y \cdot \beta , \Delta]\); inoltre, siccome \(Q\) contiene \([A \to \alpha Y \cdot \beta , \Delta]\), esso contiene anche tutti gli item in \(closure_1([A \to \alpha Y \cdot \beta , \Delta])\).

Proviamo ora a chiarire tale procedura tramite la sua scrittura in forma algoritmica, si veda \ref{alg:lr1-automata}.

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{alg-construct-lr1-automata.png}
%     \caption{Algoritmo per la costruzione di un automa LR(1)}
%     \label{alg:construct-lr1-automata}
% \end{figure}
\subimport{assets/pseudocode/}{lr1-automata.tex}
Indovinate un po' con quale grammatica stiamo per andare a osservare cosa si ottiene con questa procedura? Oh sì! proprio lei:
\begin{align}
    \label{eq:ex3-slr1-grammarrrrrr-again-and-again}
    \G: S &\to aAd \mid bBd \mid aBe \mid bAe \\
    A &\to c \nonumber \\ \notag
    B &\to c \notag
\end{align}
Ora chiederemo al lettore di fare uno sforzo mnemonico poderoso e di ricordare che in passato, quando abbiamo provato a costruire l'automa caratteristico di tipo SLR per questa grammatica ci siamo trovati ad avere la situazione rappresentata in figura \ref{fig:lr0-automata_conflict}, che riprtiamo qui sotto per comodità.
\begin{figure}[H]
    \centering
    \subimport{assets/figures/}{automa_conflict_LR.tex}
    \caption{Automa di tipo LR(0), si noti il conflitto sullo stato 6}
    \label{fig:lr0-automata_conflict}
\end{figure}
Abbiamo già calcolato quali sono gli stati LR(1) per la grammatica in questione in un esercizio precedente (vedi \ref{ex:closure-lr1}), ed applicando l'algoritmo per la costruzione degli automi LR(1) arriviamo ad ottenere il seguente automa (parziale):
\begin{figure}[H]
    \centering
    \subimport{assets/figures/}{automa_conflict_solution_LR.tex}
    \caption{Automa di tipo LR(1), si noti che il conflitto sullo stato 6 è stato eliminato}
    \label{fig:lr1-automata_no-conflict}
\end{figure}
Ma siamo sicuri di aver effettivamente risolto il problema? per verificarlo dobbiamo costruire la parsing table!

La costruzione della parsing table per il parsing di tipo LR(1) è semplice da ottenere perché si crea esattamente come la parsing table per l'SLR parsing con queste uniche due differenze:
\begin{itemize}
    \item l'automa caratteristico è di tipo LR(1);
    \item la lookahead function utilizzata è la seguente: per ogni \([A \to \beta \cdot , \Delta] \in P\), \(\mathcal{LA}(P, A \to \beta ) = \Delta\).
\end{itemize}
Naturalmente una grammatica \(\mathcal{G}\) è di tipo LR(1) se la sua tabella di parsing LR(1) non presenta conflitti.

Sbirciamo quindi cosa sarebbe successo costruendo la parsing table LR(1) per la nostra grammatica preferita.

Innanzitutto, l'automa caratteristico LR(1) si presenta come in Fig.\ref{fig:lr1_automata-complete}
\begin{figure}[H]
    \centering
    \subimport{assets/figures/}{automa_LR_complete_solution.tex}
    \caption{Automa di tipo LR(1) completo}
    \label{fig:lr1_automata-complete}
\end{figure}
Negli stati 6 e 9, che ci avrebbero causato problemi con la costruzione di tipo LR(0), ora troviamo i seguenti item:
\begin{itemize}
    \item Stato 6
    \begin{align*}
        A &\to c \cdot , {d} \\
        B &\to c \cdot , {e}    
    \end{align*}
    \item Stato 9
    \begin{align*}
        A &\to c \cdot , {e} \\
        B &\to c \cdot , {d}    
    \end{align*}
\end{itemize}
 Quindi, seguendo le regole di compilazione della tabella di parsing LR(1) avremmo:
 \begin{itemize}
     \item nella casella M[6,d] la riduzione \(A \to c \cdot , {d}\);
     \item nella casella M[6,e] la riduzione \(B \to c \cdot , {e}\);
     \item nella casella M[9,e] la riduzione \(A \to c \cdot , {e}\);
     \item nella casella M[9,d] la riduzione \(B \to c \cdot , {d}\);
 \end{itemize}
Festa! Festa! non ci sono più conflitti!
È però notevole il fatto che il numero di stati sia aumentato parecchio: questo è quello che succede quando si va ad utilizzare tecniche di parsing più precise e raffinate.

\subsection{Esercizi: costruzione di parsing table LR(1)}
Mettiamoci ora di buona lena ad esercitarci con la costruzione di tabelle di parsing di tipo LR(1).

\subsubsection{Esercizio 1}
La prima grammatica che andiamo ad analizzare è la seguente:
\begin{align}
    \label{gr:pointers-grammar}
    S &\to L = R \mid R \\
    L &\to *R \mid id \nonumber \\ \notag
    R &\to L \notag
\end{align}
Questa è una grammatica che ci parla di puntatori, ma capiremo in seguito cos significa ciò, per ora concentriamoci sulla sua risoluzione.

È il momento di campiare Pokémon, vai DaPips, scelgo te!

\end{document}
